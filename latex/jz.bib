@inproceedings{Romero2014,
abstract = {While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network.},
archivePrefix = {arXiv},
arxivId = {1412.6550},
author = {Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
booktitle = {International Conference on Learning Representations},
eprint = {1412.6550},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1412.6550v4.pdf:pdf},
title = {{FitNets: Hints for Thin Deep Nets}},
url = {http://arxiv.org/abs/1412.6550},
year = {2014}
}
@inproceedings{Springenberg2015,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
booktitle = {International Conference on Learning and Recognition},
eprint = {1412.6806},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1412.6806.pdf:pdf},
isbn = {9781600066634},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {1--14},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2015}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural net- works and in particular for Deep Learning may seem to involve many bells and whistles, called hyper- parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back- propagated gradient and gradient-based optimiza- tion. It also discusses how to deal with the fact that more interesting results can be obtained when allow- ing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observedwith deeper architectures. 1},
archivePrefix = {arXiv},
arxivId = {1206.5533},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8-26},
eprint = {1206.5533},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1206.5533v2.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {437--478},
pmid = {25497547},
title = {{Practical recommendations for gradient-based training of deep architectures}},
volume = {7700 LECTU},
year = {2012}
}
@article{Szegedy2015a,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error and 17.3{\%} top-1 error.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1512.00567.pdf:pdf},
isbn = {9781617796029},
issn = {08866236},
journal = {arXiv preprint},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2015}
}
@article{Gordo2016,
abstract = {We propose a novel approach for instance-level image retrieval. It produces a global and compact fixed-length representation for each image by aggregating many region-wise descriptors. In contrast to previous works employing pre-trained deep networks as a black box to produce features, our method leverages a deep architecture trained for the specific task of image retrieval. Our contribution is twofold: (i) we introduce a ranking framework to learn convolution and projection weights that are used to build the region features; and (ii) we employ a region proposal network to learn which regions should be pooled to form the final global descriptor. We show that using clean training data is key to the success of our approach. To that aim, we leverage a large scale but noisy landmark dataset and develop an automatic cleaning approach. The proposed architecture produces a global image representation in a single forward pass. Our approach significantly outperforms previous approaches based on global descriptors on standard datasets. It even surpasses most prior works based on costly local descriptor indexing and spatial verification. We intend to release our pre-trained model.},
archivePrefix = {arXiv},
arxivId = {1604.01325},
author = {Gordo, Albert and Almazan, Jon and Revaud, Jerome and Larlus, Diane},
eprint = {1604.01325},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1604.01325v2.pdf:pdf},
journal = {Proc. ECCV},
keywords = {Himalaya,deep learning,instance-level retrieval},
mendeley-tags = {Himalaya},
pages = {1--21},
title = {{Deep Image Retrieval: Learning Global Representations for Image Search}},
url = {http://arxiv.org/abs/1604.01325},
year = {2016}
}
@article{Fan2009,
abstract = {Dimensionality reduction is a very important tool in data mining. Intrinsic dimension of data sets is a key parameter for dimensionality reduction. However, finding the correct intrinsic dimension is a challenging task. In this paper, a new intrinsic dimension estimation method is presented. The estimator is derived by finding the exponential relationship between the radius of an incising ball and the number of samples included in the ball. The method is compared with the previous dimension estimation methods. Experiments have been conducted on synthetic and high dimensional image data sets and on data sets of the Santa Fe time series competition, and the results show that the new method is accurate and robust. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fan, Mingyu and Qiao, Hong and Zhang, Bo},
doi = {10.1016/j.patcog.2008.09.016},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1505.01257.pdf:pdf},
isbn = {978-3-319-24946-9},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Data mining,Intrinsic dimension estimation,Manifold learning,Nonlinear dimensionality reduction},
number = {5},
pages = {780--787},
pmid = {14983609},
title = {{Intrinsic dimension estimation of manifolds by incising balls}},
volume = {42},
year = {2009}
}
@article{Torralba,
abstract = {Datasets are an integral part of contemporary object recognition research. They have been the chief reason for the considerable progress in the field, not just as source of large amounts of training data, but also as means of measuring and comparing performance of competing algo-rithms. At the same time, datasets have often been blamed for narrowing the focus of object recognition research, re-ducing it to a single benchmark performance number. In-deed, some datasets, that started out as data capture efforts aimed at representing the visual world, have become closed worlds unto themselves (e.g. the Corel world, the Caltech-101 world, the PASCAL VOC world). With the focus on beating the latest benchmark numbers on the latest dataset, have we perhaps lost sight of the original purpose? The goal of this paper is to take stock of the current state of recognition datasets. We present a comparison study us-ing a set of popular datasets, evaluated based on a number of criteria including: relative data bias, cross-dataset gen-eralization, effects of closed-world assumption, and sample value. The experimental results, some rather surprising, suggest directions that can improve dataset collection as well as algorithm evaluation protocols. But more broadly, the hope is to stimulate discussion in the community regard-ing this very important, but largely neglected issue.},
author = {Torralba, Antonio and Efros, Alexei A},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/datasets.pdf:pdf},
title = {{Unbiased Look at Dataset Bias}}
}
@article{Zeiler2011,
abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.},
author = {Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
doi = {10.1109/ICCV.2011.6126474},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/zeilertaylorfergus{\_}iccv2011.pdf:pdf},
isbn = {9781457711015},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2018--2025},
title = {{Adaptive deconvolutional networks for mid and high level feature learning}},
year = {2011}
}
@article{Kowalski2009,
author = {Kowalski, M and Torr, B},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/30.pdf:pdf},
journal = {Signal Processing},
keywords = {Christopher},
mendeley-tags = {Christopher},
title = {{Structured Sparsity : from Mixed Norms to Structured Shrinkage}},
year = {2009}
}
@article{Sablayrolles2016,
abstract = {Hashing is the problem of producing compact representations such that one can efficiently solve a task, such as image classification or retrieval, based on these short codes. When hashing is supervised, the codes are trained on a labelled dataset. This paper first shows that the evaluation protocols used in the literature are not satisfactory. In particular, we show that a trivial solution that encodes the output of a classifier significantly outperforms existing supervised or semi-supervised methods, while using much shorter codes. We then propose two alternative protocols for supervised hashing: one based on retrieval on a disjoint set of classes, and another based on transfer learning to new classes. We provide two baseline methods for image-related tasks to assess the performance of (semi-)supervised hashing: without coding and with unsupervised codes. These baselines give a lower- and upper-bound on the performance of a supervised hashing scheme.},
archivePrefix = {arXiv},
arxivId = {1609.06753},
author = {Sablayrolles, Alexandre and Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}} and Usunier, Nicolas},
eprint = {1609.06753},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1609.06753v1.pdf:pdf},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{How should we evaluate supervised hashing?}},
url = {http://arxiv.org/abs/1609.06753},
year = {2016}
}
@article{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which further makes training easy and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10/100, and a 200-layer ResNet on ImageNet.},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1603.05027},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1603.05027v2.pdf:pdf},
journal = {arXiv preprint},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {1--15},
title = {{Identity Mappings in Deep Residual Networks}},
url = {http://arxiv.org/abs/1603.05027},
year = {2016}
}
@article{Zagoruyko2016,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR-10, CIFAR-100 and SVHN.},
archivePrefix = {arXiv},
arxivId = {1605.07146},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
eprint = {1605.07146},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1605.07146v1.pdf:pdf},
journal = {Arxiv},
keywords = {Christopher},
mendeley-tags = {Christopher},
title = {{Wide Residual Networks}},
url = {http://arxiv.org/abs/1605.07146},
year = {2016}
}
@article{Bach2010,
abstract = {Sparse methods for supervised learning aim at finding good linear predictors from as few variables as possible, i.e., with small cardinality of their supports. This combinatorial selection problem is often turned into a convex optimization problem by replacing the cardinality function by its convex envelope (tightest convex lower bound), in this case the L1-norm. In this paper, we investigate more general set-functions than the cardinality, that may incorporate prior knowledge or structural constraints which are common in many applications: namely, we show that for nondecreasing submodular set-functions, the corresponding convex envelope can be obtained from its $\backslash$lova extension, a common tool in submodular analysis. This defines a family of polyhedral norms, for which we provide generic algorithmic tools (subgradients and proximal operators) and theoretical results (conditions for support recovery or high-dimensional inference). By selecting specific submodular functions, we can give a new interpretation to known norms, such as those based on rank-statistics or grouped norms with potentially overlapping groups; we also define new norms, in particular ones that can be used as non-factorial priors for supervised learning.},
archivePrefix = {arXiv},
arxivId = {1008.4220},
author = {Bach, Francis},
eprint = {1008.4220},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach - 2010 - Structured sparsity-inducing norms through submodular functions.pdf:pdf},
isbn = {9781617823800},
journal = {Nips},
pages = {1--9},
title = {{Structured sparsity-inducing norms through submodular functions}},
url = {http://arxiv.org/abs/1008.4220},
year = {2010}
}
@article{Huang2009,
abstract = {This paper investigates a new learning formulation called structured sparsity, which is a natural extension of the standard sparsity concept in statistical learning and compressive sensing. By allowing arbitrary structures$\backslash$non the feature set, this concept generalizes the group sparsity idea that has become popular in recent years. A general theory is developed for learning with$\backslash$nstructured sparsity, based on the notion of coding complexity associated with the structure. It is shown that if the coding complexity of the target signal is small, then one can achieve improved performance by using coding complexity regularization methods, which generalize the standard sparse regularization. Moreover, a structured greedy algorithm is proposed to efficiently solve the$\backslash$nstructured sparsity problem. It is shown that the greedy algorithm approximately solves the coding complexity optimization problem under appropriate conditions. Experiments are included to demonstrate the advantage$\backslash$nof structured sparsity over standard sparsity on some real applications.},
archivePrefix = {arXiv},
arxivId = {0903.3002v2},
author = {Huang, Junzhou and Zhang, Tong and Metaxas, Dimitris},
doi = {10.1145/1553374.1553429},
eprint = {0903.3002v2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Zhang, Metaxas - 2009 - Learning with Structured Sparsity.pdf:pdf},
isbn = {978-1-60558-516-1},
issn = {1532-4435},
journal = {Computer},
keywords = {Methodology,Statistics},
pages = {1--30},
title = {{Learning with Structured Sparsity}},
url = {http://arxiv.org/abs/0903.3002v2},
volume = {12},
year = {2009}
}
@article{Jenatton2011a,
abstract = {We consider the empirical risk minimization problem for linear supervised learning, with regularization by structured sparsity-inducing norms. These are defined as sums of Euclidean norms on certain subsets of variables, extending the usual {\$}\backslashell{\_}1{\$}-norm and the group {\$}\backslashell{\_}1{\$}-norm by allowing the subsets to overlap. This leads to a specific set of allowed nonzero patterns for the solutions of such problems. We first explore the relationship between the groups defining the norm and the resulting nonzero patterns, providing both forward and backward algorithms to go back and forth from groups to patterns. This allows the design of norms adapted to specific prior knowledge expressed in terms of nonzero patterns. We also present an efficient active set algorithm, and analyze the consistency of variable selection for least-squares linear regression in low and high-dimensional settings.},
archivePrefix = {arXiv},
arxivId = {0904.3523},
author = {Jenatton, Rodolphe and Audibert, Jean-Yves and Bach, Francis},
doi = {arXiv:0904.3523},
eprint = {0904.3523},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jenatton, Audibert, Bach - 2011 - Structured Variable Selection with Sparsity-Inducing Norms.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {J. Mach. Learn. Res. (JMLR)},
keywords = {active set algorithm,consistency,convex optimization,sparsity,variable selection},
pages = {2777--2824},
title = {{Structured Variable Selection with Sparsity-Inducing Norms}},
volume = {12},
year = {2011}
}
@article{Bach2012a,
abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. While naturally cast as a combinatorial optimization problem, variable or feature selection admits a convex relaxation through the regularization by the {\$}\backslashell{\_}1{\$}-norm. In this paper, we consider situations where we are not only interested in sparsity, but where some structural prior knowledge is available as well. We show that the {\$}\backslashell{\_}1{\$}-norm can then be extended to structured norms built on either disjoint or overlapping groups of variables, leading to a flexible framework that can deal with various structures. We present applications to unsupervised learning, for structured sparse principal component analysis and hierarchical dictionary learning, and to supervised learning in the context of non-linear variable selection.},
archivePrefix = {arXiv},
arxivId = {1109.2397},
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
doi = {10.1214/12-STS394},
eprint = {1109.2397},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2012 - Structured sparsity through convex optimization(3).pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Convex optimization,Sparsity,and phrases,convex optimization,sparsity},
number = {4},
pages = {450--468},
title = {{Structured sparsity through convex optimization}},
url = {http://projecteuclid.org/euclid.ss/1356098550$\backslash$nhttp://arxiv.org/abs/1109.2397},
volume = {27},
year = {2012}
}
@article{Bach2012,
abstract = {Sparse estimation methods are aimed at using or obtaining parsimonious representations of data or models. While naturally cast as a combinatorial optimization problem, variable or feature selection admits a convex relaxation through the regularization by the {\$}\backslashell{\_}1{\$}-norm. In this paper, we consider situations where we are not only interested in sparsity, but where some structural prior knowledge is available as well. We show that the {\$}\backslashell{\_}1{\$}-norm can then be extended to structured norms built on either disjoint or overlapping groups of variables, leading to a flexible framework that can deal with various structures. We present applications to unsupervised learning, for structured sparse principal component analysis and hierarchical dictionary learning, and to supervised learning in the context of non-linear variable selection.},
archivePrefix = {arXiv},
arxivId = {1109.2397},
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
doi = {10.1214/12-STS394},
eprint = {1109.2397},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2012 - Structured sparsity through convex optimization.pdf:pdf;:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach et al. - 2012 - Structured sparsity through convex optimization(2).pdf:pdf},
isbn = {0883-4237},
issn = {0883-4237},
journal = {Statistical Science},
keywords = {Convex optimization,Sparsity,and phrases,convex optimization,sparsity},
number = {4},
pages = {450--468},
title = {{Structured sparsity through convex optimization}},
url = {http://projecteuclid.org/euclid.ss/1356098550$\backslash$nhttp://arxiv.org/abs/1109.2397},
volume = {27},
year = {2012}
}
@article{Yuan2006,
abstract = {We consider the problem of selecting grouped variables (factors) for accurate pre-diction in regression. Such a problem arises naturally in many practical situations with the multi-factor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose effi-cient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
author = {Yuan, Ming and Lin, Yi},
doi = {10.1111/j.1467-9868.2005.00532.x},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/yuanlin07.pdf:pdf},
isbn = {1369-7412},
issn = {13697412},
journal = {J. R. Statist. Soc. B},
keywords = {analysis of variance,lasso,least angle regression,non-negative garrotte,piecewise linear solution path},
number = {1},
pages = {49--67},
pmid = {11161800},
title = {{Model selection and estimation in regression with grouped varibles}},
volume = {68},
year = {2006}
}
@article{Bach2011,
archivePrefix = {arXiv},
arxivId = {1108.0775v2},
author = {Bach, Francis},
doi = {10.1561/2200000015},
eprint = {1108.0775v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1108.0775.pdf:pdf},
isbn = {9780262016469},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--106},
title = {{Optimization with Sparsity-Inducing Penalties}},
url = {http://www.nowpublishers.com/article/Details/MAL-015},
volume = {4},
year = {2011}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We ﬁrst observe the inﬂuence of the non-linear activations functions. We ﬁnd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we ﬁnd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We ﬁnd that a new non-linearity that saturates less can often be beneﬁcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difﬁcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/glorot10a.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Courbariaux2015,
abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and power-hungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
archivePrefix = {arXiv},
arxivId = {1511.00363},
author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
eprint = {1511.00363},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/5647-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{BinaryConnect: Training Deep Neural Networks with binary weights during propagations}},
url = {http://arxiv.org/abs/1511.00363},
year = {2015}
}
@article{Scardapane2016,
abstract = {—In this paper, we consider the joint task of simul-taneously optimizing (i) the weights of a deep neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are generally dealt with separately, we present a simple regularized formulation allowing to solve all three of them in parallel, using standard optimization routines. Specifically, we extend the group Lasso penalty (originated in the linear regression literature) in order to impose group-level sparsity on the network's connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We perform an extensive experimental evaluation, by comparing with classical weight decay and Lasso penalties. We show that a sparse version of the group Lasso penalty is able to achieve competitive performances, while at the same time resulting in extremely compact networks with a smaller number of input features. We evaluate both on a toy dataset for handwritten digit recognition, and on multiple realistic large-scale classification problems.},
archivePrefix = {arXiv},
arxivId = {1607.00485},
author = {Scardapane, Simone and Comminiello, Danilo and Hussain, Amir and Uncini, Aurelio},
eprint = {1607.00485},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1607.00485.pdf:pdf},
keywords = {Christopher,Fea-ture selection,Group sparsity,Index Terms—Deep networks,Pruning},
mendeley-tags = {Christopher},
pages = {1--10},
title = {{Group Sparse Regularization for Deep Neural Networks}},
year = {2016}
}
@article{Lin2016,
author = {Lin, Kevin and Lu, Jiwen and Chen, Chu-song and Zhou, Jie},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2016{\_}cvpr{\_}deepbit{\_}learning{\_}compact{\_}binary{\_}descriptors{\_}with{\_}unsupervised{\_}deep{\_}neural{\_}networks.pdf:pdf},
title = {{Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks}},
year = {2016}
}
@article{Liub,
author = {Liu, Haomiao and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2016{\_}cvpr{\_}Liu{\_}Deep{\_}Supervised{\_}Hashing{\_}for{\_}fast{\_}image{\_}retrieval.pdf:pdf},
pages = {2064--2072},
title = {{Deep Supervised Hashing for Fast Image Retrieval}}
}
@inproceedings{Arandjelovic2015,
abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture obtains a large improvement in performance over non-learnt image representations as well as significantly outperforms off-the-shelf CNN descriptors on two challenging place recognition benchmarks.},
archivePrefix = {arXiv},
arxivId = {1511.07247},
author = {Arandjelovi{\'{c}}, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2016.572},
eprint = {1511.07247},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1511.07247v1.pdf:pdf},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{NetVLAD: CNN architecture for weakly supervised place recognition}},
url = {http://arxiv.org/abs/1511.07247},
year = {2015}
}
@article{Hadsell2006,
abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
author = {Hadsell, Raia and Chopra, Sumit and LeCun, Yann},
doi = {10.1109/CVPR.2006.100},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/hadsell-chopra-lecun-06.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
pages = {1735--1742},
title = {{Dimensionality reduction by learning an invariant mapping}},
volume = {2},
year = {2006}
}
@article{Liong,
author = {Liong, Venice Erin and Lu, Jiwen and Wang, Gang and Moulin, Pierre and Zhou, Jie},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2015{\_}cvpr{\_}Liong{\_}Deep{\_}Hashing{\_}for{\_}compact{\_}binary{\_}codes{\_}learning.pdf:pdf},
isbn = {0100110010},
title = {{Deep Hashing for Compact Binary Codes Learning}}
}
@article{Lin2015b,
author = {Lin, Kevin and Yang, Huei-fang and Hsiao, Jen-hao and Chen, Chu-song},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2015{\_}cvprw{\_}Deep Learning of Binary Hash Codes for Fast Image Retrieval.pdf:pdf},
title = {{Deep Learning of Binary Hash Codes for Fast Image Retrieval Large-scale Image Search Query}},
year = {2015}
}
@article{Lai2015,
abstract = {Similarity-preserving hashing is a widely-used method for nearest neighbour search in large-scale image retrieval tasks. For most existing hashing methods, an image is first encoded as a vector of hand-engineering visual fea- tures, followed by another separate projection or quantiza- tion step that generates binary codes. However, such visual feature vectors may not be optimally compatible with the coding process, thus producing sub-optimal hashing codes. In this paper, we propose a deep architecture for supervised hashing, in which images are mapped into binary codes via carefully designed deep neural networks. The pipeline of the proposed deep architecture consists of three building blocks: 1) a sub-network with a stack of convolution lay- ers to produce the effective intermediate image features; 2) a divide-and-encode module to divide the intermediate im- age features into multiple branches, each encoded into one hash bit; and 3) a triplet ranking loss designed to character- ize that one image is more similar to the second image than to the third one. Extensive evaluations on several bench- mark image datasets show that the proposed simultaneous feature learning and hash coding pipeline brings substan- tial improvements over other state-of-the-art supervised or unsupervised hashing methods. 1.},
archivePrefix = {arXiv},
arxivId = {1504.0341},
author = {Lai, Hanjiang and Pan, Yan and Liu, Ye and Yan, Shuicheng},
doi = {10.1109/CVPR.2015.7298947},
eprint = {1504.0341},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Lai{\_}Simultaneous{\_}Feature{\_}Learning{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
pages = {3270--3278},
title = {{Simultaneous feature learning and hash coding with deep neural networks}},
volume = {07-12-June},
year = {2015}
}
@article{Karpathy2015,
abstract = {Convolutional neural networks (CNNs) have been extensively applied for image recognition problems giving state-of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image information across a video over longer time periods than previously attempted. We propose two methods capable of handling full length videos. The first method explores various convolutional temporal feature pooling architectures, examining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improvements over previously published results on the Sports 1 million dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 72.8{\%}).},
archivePrefix = {arXiv},
arxivId = {1412.2306},
author = {Karpathy, Andrej and Li, Fei Fei},
doi = {10.1109/CVPR.2015.7298932},
eprint = {1412.2306},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cvpr2015.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {3128--3137},
title = {{Deep visual-semantic alignments for generating image descriptions}},
volume = {07-12-June},
year = {2015}
}
@article{Kokkinos2016,
archivePrefix = {arXiv},
arxivId = {1609.02132},
author = {Kokkinos, Iasonas},
eprint = {1609.02132},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1609.02132v1.pdf:pdf},
title = {{UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory}},
year = {2016}
}
@article{Liuc,
author = {Liu, Haomiao and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
doi = {10.1109/CVPR.2016.227},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Liu{\_}Deep{\_}Supervised{\_}Hashing{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
pages = {2064--2072},
title = {{Deep Supervised Hashing for Fast Image Retrieval}}
}
@article{Srinivas2015,
abstract = {Deep neural networks with millions of parameters are at the heart of many state of the art machine learning models today. However, recent works have shown that models with much smaller number of parameters can also perform just as well. In this work, we introduce the problem of architecture-learning, i.e; learning the architecture of a neural network along with weights. We introduce a new trainable parameter called tri-state ReLU, which helps in eliminating unnecessary neurons. We also propose a smooth regularizer which encourages the total number of neurons after elimination to be small. The resulting objective is differentiable and simple to optimize. We experimentally validate our method on both small and large networks, and show that it can learn models with a considerably small number of parameters without affecting prediction accuracy.},
archivePrefix = {arXiv},
arxivId = {1511.05497},
author = {Srinivas, Suraj and Babu, R. Venkatesh},
eprint = {1511.05497},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1511.05497v2.pdf:pdf},
keywords = {Christopher},
mendeley-tags = {Christopher},
number = {Section 2},
pages = {1--13},
title = {{Learning the Architecture of Deep Neural Networks}},
url = {http://arxiv.org/abs/1511.05497},
year = {2015}
}
@article{Feng2015,
abstract = {In this work, we develop a novel method for automati-cally learning aspects of the structure of a deep model in order to improve its performance, especially when labeled training data are scarce. We propose a new convolutional neural network model with the Indian Buffet Process (IBP) prior, termed ibpCNN. The ibpCNN automatically adapts its structure to provided training data, achieves an optimal balance among model complexity, data fidelity and training loss, and thus offers better generalization performance. The proposed ibpCNN captures complex data distribu-tion in an unsupervised generative way. Therefore, ibpCNN can exploit unlabeled data – which can be collected at low cost – to learn its structure. After determining the structure, ibpCNN further learns its parameters according to speci-fied tasks, in an end-to-end fashion, and produces discrimi-native yet compact representations. We evaluate the performance of ibpCNN, on fully-and semi-supervised image classification tasks; ibpCNN sur-passes standard CNN models on benchmark datasets with much smaller size and higher efficiency.},
author = {Feng, Jiashi and Darrell, Trevor},
doi = {10.1109/ICCV.2015.315},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng, Darrell - 2015 - Learning The Structure of Deep Convolutional Networks.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {2749--2757},
title = {{Learning The Structure of Deep Convolutional Networks}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Feng{\_}Learning{\_}The{\_}Structure{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@inproceedings{Ren2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v2},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Neural Information Processing Systems},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf:pdf},
issn = {01689002},
keywords = {CNN SOTA,Himalaya,Praveen},
mendeley-tags = {CNN SOTA,Himalaya,Praveen},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}
@article{Oquab2015,
author = {Oquab, Maxime},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Oquab15.pdf:pdf},
keywords = {Praveen},
mendeley-tags = {Praveen},
number = {iii},
title = {{Is object localization for free ? – Weakly-supervised learning with convolutional neural networks To cite this version : Is object localization for free ? – Weakly-supervised learning with convolutional neural networks}},
year = {2015}
}
@article{Lin2015a,
author = {Lin, Kevin and Yang, Huei-fang and Hsiao, Jen-hao and Chen, Chu-song},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cvprw15.pdf:pdf},
isbn = {9781467367592},
issn = {9781467367592},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
pages = {27--35},
title = {{Deep Learning of Binary Hash Codes for Fast Image Retrieval}},
year = {2015}
}
@article{Cvpr2016,
author = {Cvpr, Anonymous and Id, Paper},
doi = {10.1109/CVPR.2016.133},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Lin{\_}Learning{\_}Compact{\_}Binary{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks}},
year = {2016}
}
@article{Liong2015,
abstract = {Deep hashing을 통해 간단한 binary code를 만들어서 large scale image search를 하는 방법 소개},
author = {Liong, Venice Erin and Lu, Jiwen and Wang, Gang and Moulin, Pierre and Zhou, Jie},
doi = {10.1109/CVPR.2015.7298862},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Liong{\_}Deep{\_}Hashing{\_}for{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
pages = {2475--2483},
title = {{Deep hashing for compact binary codes learning}},
volume = {07-12-June},
year = {2015}
}
@article{Benosman2011,
author = {Benosman, R and Ieng, S.-H. and Rogister, P and Posch, C},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Benosman2011.pdf:pdf},
journal = {{\{}IEEE{\}} Trans. Neural Netw.},
number = {11},
pages = {1723--1734},
title = {{Asynchronous Event-Based {\{}H{\}}ebbian Epipolar Geometry}},
volume = {22},
year = {2011}
}
@article{Larsson2016,
abstract = {We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a single expansion rule generates an extremely deep network whose structural layout is precisely a truncated fractal. Such a network contains interacting subpaths of different lengths, but does not include any pass-through connections: every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. This property stands in stark contrast to the current approach of explicitly structuring very deep networks so that training is a residual learning problem. Our experiments demonstrate that residual representation is not fundamental to the success of extremely deep convolutional neural networks. A fractal design achieves an error rate of 22.85{\%} on CIFAR-100, matching the state-of-the-art held by residual networks. Fractal networks exhibit intriguing properties beyond their high performance. They can be regarded as a computationally efficient implicit union of subnetworks of every depth. We explore consequences for training, touching upon connection with student-teacher behavior, and, most importantly, demonstrating the ability to extract high-performance fixed-depth subnetworks. To facilitate this latter task, we develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. With such regularization, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.},
archivePrefix = {arXiv},
arxivId = {1605.07648},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
eprint = {1605.07648},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1605.07648v1.pdf:pdf},
keywords = {Christopher,Himalaya,Praveen},
mendeley-tags = {Christopher,Himalaya,Praveen},
pages = {1--9},
title = {{FractalNet: Ultra-Deep Neural Networks without Residuals}},
url = {http://arxiv.org/abs/1605.07648},
year = {2016}
}
@article{Lagorce2016,
author = {Lagorce, Xavier and Orchard, Garrick and Gallupi, Francesco and Shi, Bertram E. and Benosman, Ryad},
doi = {10.1109/TPAMI.2016.2574707},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/spike{\_}context{\_}reduced.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {1--1},
title = {{HOTS: A Hierarchy Of event-based Time-Surfaces for pattern recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7508476},
year = {2016}
}
@article{Bengio2003,
author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas Le},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2810-the-curse-of-highly-variable-functions-for-local-kernel-machines.pdf:pdf},
title = {{The Curse of Highly Variable Functions for Local Kernel Machines}},
year = {2003}
}
@article{Cho2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1869v2},
author = {Cho, Kyunghyun and Pascanu, Razvan and Bengio, Yoshua},
eprint = {arXiv:1402.1869v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1402.1869.pdf:pdf},
keywords = {deep learning,input space partition,maxout,neural network,rectifier},
title = {{Deep Neural Networks}},
year = {2014}
}
@article{Maaten2008,
author = {Maaten, Laurens Van Der and Hinton, Geoffrey},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/tsne.pdf:pdf},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {1--25},
title = {{Visualizing Data using t-SNE}},
volume = {1},
year = {2008}
}
@article{Liua,
author = {Liu, Haomiao and Wang, Ruiping and Shan, Shiguang and Chen, Xilin},
doi = {10.1109/CVPR.2016.227},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Liu{\_}Deep{\_}Supervised{\_}Hashing{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
keywords = {Christopher,Himalaya},
mendeley-tags = {Christopher,Himalaya},
pages = {2064--2072},
title = {{Deep Supervised Hashing for Fast Image Retrieval}}
}
@article{Shen2015,
author = {Shen, Fumin},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Shen{\_}Supervised{\_}Discrete{\_}Hashing{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{Supervised Discrete Hashing University of Adelaide ; and Australian Centre for Robotic Vision}},
year = {2015}
}
@article{Liu2014,
abstract = {Hashing has emerged as a popular technique for fast nearest neighbor search in gi- gantic databases. In particular, learning based hashing has received considerable attention due to its appealing storage and search efficiency. However, the perfor- mance of most unsupervised learning based hashing methods deteriorates rapidly as the hash code length increases. We argue that the degraded performance is due to inferior optimization procedures used to achieve discrete binary codes. This paper presents a graph-based unsupervised hashing model to preserve the neigh- borhood structure of massive data in a discrete code space. We cast the graph hashing problem into a discrete optimization framework which directly learns the binary codes. A tractable alternating maximization algorithm is then proposed to explicitly deal with the discrete constraints, yielding high-quality codes to well capture the local neighborhoods. Extensive experiments performed on four large datasets with up to one million samples show that our discrete optimization based graph hashing method obtains superior search accuracy over state-of-the-art un- supervised hashing methods, especially for longer codes.},
author = {Liu, Wei and Mu, Cun and Kumar, Sanjiv},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/43145.pdf:pdf},
issn = {10495258},
journal = {Nips},
pages = {1--9},
title = {{Discrete Graph Hashing}},
year = {2014}
}
@article{Foroosh2015,
abstract = {Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90{\%} of parameters, with a drop of accuracy that is less than 1{\%} on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.},
author = {Foroosh, Hassan and Tappen, Marshall and Penksy, Marianna},
doi = {10.1109/CVPR.2015.7298681},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Liu{\_}Sparse{\_}Convolutional{\_}Neural{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1063-6919},
journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {Accuracy,Christopher,Convolutional codes,Kernel,Matrix decomposition,Neural networks,Redundancy,SCNN model,Sparse matrices,cascade model,matrix decomposition,matrix multiplication,neural nets,object detection,object detection problem,sparse convolutional neural networks,sparse decomposition,sparse fully connected layers,sparse matrix multiplication algorithm},
mendeley-tags = {Christopher},
pages = {806--814},
title = {{Sparse Convolutional Neural Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298681},
year = {2015}
}
@article{Mathieu2014,
abstract = {Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a signiﬁcant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5851v5},
author = {Mathieu, Michael and Henaff, Mikael and LeCun, Y},
eprint = {arXiv:1312.5851v5},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1312.5851.pdf:pdf},
journal = {International Conference on Learning Representations (ICLR2014)},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {1--9},
title = {{Fast Training of Convolutional Networks through FFTs}},
url = {http://arxiv.org/abs/1312.5851},
year = {2014}
}
@article{Rusk2015,
abstract = {NATURE METHODS | VOL.13 NO.1 | JANUARY 2016 | 35 METHODS TO WATCH | SPECIAL FEATURE and high computational costs are being tackled. Researchers in academic settings as well as in startup companies such as Deep Genomics, launched July 22, 2015, by some of the authors of DeepBind, will increasingly apply deep learning to genome analysis and precision medicine. The goal is to predict the effect of genetic variants— both naturally occurring and introduced by genome editing—on a cell's regulatory landscape and how this in turn affects dis-ease development. Nicole Rusk ❯❯Deep learning New computational tools learn complex motifs from large sequence data sets. A powerful form of machine learning that enables computers to solve perceptual problems such as image and speech rec-ognition is increasingly making an entry into the biological sciences. These deep-learning methods, such as deep artificial neural networks, use multiple processing layers to discover patterns and structure in very large data sets. Each layer learns a concept from the data that subsequent lay-ers build on; the higher the level, the more abstract the concepts that are learned. Deep learning does not depend on prior data processing and automatically extracts features. To use a simple example, a deep neural network tasked with interpreting shapes would learn to recognize simple edges in the first layer and then add recog-nition of the more complex shapes com-posed of those edges in subsequent lay-ers. There is no hard and fast rule for how many layers are needed to constitute deep learning, but most experts agree that more than two are required. Recent examples show the power of deep learning to derive regulatory fea-tures in genomes from DNA sequence alone: DeepSEA (Nat. Methods 12, 931– 934, 2015) uses genomic sequence as input, trains on chromatin profiles from large consortia such as ENCODE and the Epigenomics Roadmap, and predicts the effect of single-nucleotide variants on reg-ulatory regions such as DNase hypersen-sitive sites, transcription factor–binding sites and histone marks. Basset (bioRxiv, doi:10.1101/028399, 2015) uses similar deep neural networks to predict the effect of single-nucleotide polymorphisms on chromatin accessibility. DeepBind (Nat. Biotechnol. 33, 831–838, 2015) finds protein-binding sites on RNA and DNA and predicts the effects of mutations. Deep learning will be invaluable in the context of big data, as it extracts high-level information from very large volumes of data. As it gains traction in genome analy-sis, initial challenges such as overfitting due to rare dependencies in the training data},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {Rusk, Nicole},
doi = {10.1038/nmeth.3707},
eprint = {arXiv:1312.6184v5},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/nature14539.pdf:pdf},
isbn = {9780521835688},
issn = {1548-7091},
journal = {Nature Methods},
number = {1},
pages = {35--35},
pmid = {10463930},
title = {{Deep learning}},
url = {http://www.nature.com/doifinder/10.1038/nature14539$\backslash$nhttp://www.nature.com/doifinder/10.1038/nmeth.3707},
volume = {13},
year = {2015}
}
@article{Jaderberg2014,
abstract = {The focus of this paper is speeding up the evaluation of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learn- ing tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition, showing a possible 2.5× speedup with no loss in accuracy, and 4.5× speedup with less than 1{\%} drop in accuracy, still achieving state-of-the-art on standard benchmarks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.3866v1},
author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, A},
doi = {10.5244/C.28.88},
eprint = {arXiv:1405.3866v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/jaderberg14speeding.pdf:pdf},
isbn = {1-901725-52-9},
journal = {arXiv preprint arXiv:1405.3866},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {7},
title = {{Speeding up Convolutional Neural Networks with Low Rank Expansions}},
url = {http://arxiv.org/abs/1405.3866},
year = {2014}
}
@inproceedings{deng2009imagenet,
author = {{Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei}, Li},
booktitle = {Computer Vision and Pattern Recognition},
title = {{Imagenet: A large-scale hierarchical image database}},
year = {2009}
}
@article{Letters2011,
author = {Letters, Photonics Technology},
doi = {10.1109/JPHOT.2015.XXXXXXX},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Memory assisted seismic signal compression.pdf:pdf},
isbn = {8153244698},
pages = {26--39},
title = {{r Fo Re vi ew On r Re vi On ly}},
year = {2011}
}
@article{Srinivas2015a,
abstract = {Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85$\backslash${\%} of the total parameters in an MNIST-trained network, and about 35$\backslash${\%} for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.},
archivePrefix = {arXiv},
arxivId = {1507.06149},
author = {Srinivas, Suraj and Babu, R. Venkatesh},
eprint = {1507.06149},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1507.06149v1.pdf:pdf},
journal = {arXiv},
pages = {1--12},
title = {{Data-free parameter pruning for Deep Neural Networks}},
url = {http://arxiv.org/abs/1507.06149},
year = {2015}
}
@article{Herranz,
author = {Herranz, Luis and Jiang, Shuqiang and Li, Xiangyang},
doi = {10.1109/CVPR.2016.68},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Herranz{\_}Scene{\_}Recognition{\_}With{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
title = {{Scene recognition with CNNs : objects , scales and dataset bias}}
}
@article{Wei2016,
author = {Wei, Zijun and Hoai, Minh},
doi = {10.1109/CVPR.2016.326},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Wei{\_}Region{\_}Ranking{\_}SVM{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
journal = {Computer Vision and Pattern Recognition (CVPR),},
pages = {2987--2996},
title = {{Region Ranking SVM for Image Classification}},
year = {2016}
}
@article{Dai2016,
abstract = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6{\%} mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart. Code will be made publicly available.},
archivePrefix = {arXiv},
arxivId = {1605.06409},
author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
eprint = {1605.06409},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1605.06409v2.pdf:pdf},
title = {{R-FCN: Object Detection via Region-based Fully Convolutional Networks}},
url = {http://arxiv.org/abs/1605.06409},
year = {2016}
}
@article{Liu2015,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of bounding box priors over different aspect ratios and scales per feature map location. At prediction time, the network generates confidences that each prior corresponds to objects of interest and produces adjustments to the prior to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that requires object proposals, such as R-CNN and MultiBox, because it completely discards the proposal generation step and encapsulates all the computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on ILSVRC DET and PASCAL VOC dataset confirm that SSD has comparable performance with methods that utilize an additional object proposal step and yet is 100-1000x faster. Compared to other single stage methods, SSD has similar or better performance, while providing a unified framework for both training and inference.},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott},
eprint = {1512.02325},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1512.02325v2.pdf:pdf},
journal = {Arxiv},
keywords = {convolutional neural network,real-time object detection},
pages = {1--15},
title = {{SSD: Single Shot MultiBox Detector}},
url = {http://arxiv.org/abs/1512.02325},
year = {2015}
}
@article{Springenberg2013,
abstract = {We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).},
archivePrefix = {arXiv},
arxivId = {1312.6116},
author = {Springenberg, Jost Tobias Jt and Riedmiller, Martin},
eprint = {1312.6116},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1312.6116v2.pdf:pdf},
isbn = {9781479961399},
journal = {arXiv: 1312.6116},
pages = {1--10},
title = {{Improving Deep Neural Networks with Probabilistic Maxout Units}},
url = {http://arxiv.org/abs/1312.6116$\backslash$nhttp://arxiv.org/pdf/1312.6116},
year = {2013}
}
@article{Radford2015,
abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
archivePrefix = {arXiv},
arxivId = {1511.06434},
author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
doi = {10.1051/0004-6361/201527329},
eprint = {1511.06434},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1511.06434.pdf:pdf},
journal = {arXiv},
pages = {1--15},
title = {{Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1511.06434},
year = {2015}
}
@article{Wang2015,
abstract = {Is strong supervision necessary for learning a good visual representation? Do we really need millions of semantically-labeled images to train a ConvNet? In this paper, we present a simple yet surprisingly powerful approach for unsupervised learning of ConvNets. Specifically, we use hundreds of thousands of unlabeled videos from the web to learn visual representations. Our key idea is that we track millions of patches in these videos. Visual tracking provides the key supervision. That is, two patches connected by a track should have similar visual representation in deep feature space since they probably belong to same object or object part. We design a Siamese-triplet network with a ranking loss function to train this ConvNet representation. Without using a single image from ImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train an ensemble of unsupervised networks that achieves 52{\%} mAP (no bounding box regression). This performance comes tantalizingly close to its ImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4{\%}. We also show that our unsupervised network can perform competitive in other tasks such as surface-normal estimation.},
archivePrefix = {arXiv},
arxivId = {1505.00687v1},
author = {Wang, Xiaolong and Gupta, Abhinav},
doi = {10.1109/ICCV.2015.320},
eprint = {1505.00687v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Wang{\_}Unsupervised{\_}Learning{\_}of{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
journal = {Iccv},
pages = {2794--2802},
title = {{Unsupervised Learning of Visual Representations Using Videos}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Wang{\_}Unsupervised{\_}Learning{\_}of{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@article{Cvpr2016a,
abstract = {Deep neural networks have proved very successful in domains where large training sets are available, but when the number of training samples is small, their performance suffers from overfitting. Prior methods of reducing over- fitting such as weight decay, Dropout and DropConnect are data-independent. This paper proposes a new method, GraphConnect, that is data-dependent, and is motivated by the observation that data of interest lie close to a manifold. The new method encourages the relationships between the learned decisions to resemble a graph representing the manifold structure. Essentially GraphConnect is designed to learn attributes that are present in data samples in contrast to weight decay, Dropout and DropConnect which are simply designed to make it more difficult to fit to random error or noise. Empirical Rademacher complexity is used to connect the generalization error of the neural network to spectral properties of the graph learned from the input data. This framework is used to show that GraphConnect is superior to weight decay. Experimental results on several benchmark datasets validate the theoretical analysis, and show that when the number of training samples is small, GraphConnect is able to significantly improve performance over weight decay.},
archivePrefix = {arXiv},
arxivId = {1512.06757},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {1512.06757},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/GraphConnect{\_}LearnFeaturesWithGraphRegu.pdf:pdf},
journal = {arXiv},
keywords = {Deep Learning,Neural Network,Optimization,Regularization},
number = {June},
title = {{GraphConnect : A Regularization Framework for Neural Networks}},
url = {http://arxiv.org/pdf/1512.06757v1.pdf},
year = {2016}
}
@article{Eccvd,
author = {Eccv, Anonymous and Networks, Neural and Cnns, The},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/kulkarni{\_}ECCV.pdf:pdf},
title = {{Parts learning}}
}
@article{Zhu2014,
abstract = {We address the visual categorization problem and present a method that utilizes weakly labeled data from other visual domains as the auxiliary source data for enhancing the original learning system. The proposed method aims to expand the intra-class diversity of original training data through the collaboration with the source data. In order to bring the original target domain data and the auxiliary source domain data into the same feature space, we introduce a weakly-supervised cross-domain dictionary learning method, which learns a reconstructive, discriminative and domain-adaptive dictionary pair and the corresponding classifier parameters without using any prior information. Such a method operates at a high level, and it can be applied to different cross-domain applications. To build up the auxiliary domain data, we manually collect images from Web pages, and select human actions of specific categories from a different dataset. The proposed method is evaluated for human action recognition, image classification and event recognition tasks on the UCF YouTube dataset, the Caltech101/256 datasets and the Kodak dataset, respectively, achieving outstanding results. {\textcopyright} 2014 Springer Science+Business Media New York.},
author = {Zhu, Fan and Shao, Ling},
doi = {10.1007/s11263-014-0703-y},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/xdomain{\_}dictionary{\_}learning.pdf:pdf},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Event recognition,Human action recognition,Image classification,Transfer learning,Visual categorization,Weakly-supervised dictionary learning},
number = {1-2},
pages = {42--59},
title = {{Weakly-supervised cross-domain dictionary learning for visual recognition}},
volume = {109},
year = {2014}
}
@article{Kulkarni2015a,
author = {Kulkarni, Praveen and Zepeda, Joaquin and Jurie, Fr{\'{e}}d{\'{e}}ric and Perez, Patrick and Chevallier, Louis},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/max{\_}margin{\_}single{\_}layer{\_}adaptation.pdf:pdf},
journal = {BigVision Workshop, Computer Vision and Pattern Recognition},
pages = {2--3},
title = {{Max-Margin, Single-Layer Adaptation of Transferred Image Features}},
year = {2015}
}
@inproceedings{Kulkarnia,
author = {Kulkarni, Praveen and Zepeda, Joaquin and Jurie, Frederic and P{\'{e}}rez, Patrick and Chevallier, Louis},
booktitle = {British Machine Vision Conference},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/bmvc{\_}review.pdf:pdf},
title = {{Learning the Structure of Deep Architectures via L1 Penalization}},
year = {2015}
}
@article{Blancs,
author = {Blancs, Champs and Cesson, S},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/icip16.pdf:pdf},
pages = {1--5},
title = {{SUPERVISED LEARNING OF LOW-RANK TRANSFORMS FOR IMAGE RETRIEVAL ¸ Bilen , Joaquin Zepeda and Patrick P ´ Technicolor}}
}
@article{Zepedab,
author = {Zepeda, Joaquin and Patrick, P},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/eusipco16.pdf:pdf},
title = {{The CNN News Footage Datasets : Enabling Supervision in Image Retrieval}}
}
@article{Changpinyo2016,
abstract = {Given semantic descriptions of object classes, zero- shot learning aims to accurately recognize objects of the unseen classes, from which no examples are available at the training stage, by associating them to the seen classes, from which labeled examples are provided. We propose to tackle this problem from the perspective of manifold learning. Our main idea is to align the seman- tic space that is derived from external information to the model space that concerns itself with recognizing visual features. To this end, we introduce a set of “phantom” object classes whose coordinates live in both the seman- tic space and the model space. Serving as bases in a dictionary, they can be optimized from labeled data such that the synthesized real object classifiers achieve opti- mal discriminative performance. We demonstrate supe- rior accuracy of our approach over the state of the art on four benchmark datasets for zero-shot learning, in- cluding the full ImageNet Fall 2011 dataset with more than 20,000 unseen classes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.00550v1},
author = {Changpinyo, Soravit and Chao, Wei-lun and Gong, Boqing and Sha, Fei},
eprint = {arXiv:1603.00550v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Changpinyo{\_}Synthesized{\_}Classifiers{\_}for{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
journal = {arXiv},
title = {{Synthesized Classifiers for Zero-Shot Learning}},
year = {2016}
}
@article{Heber,
author = {Heber, Stefan},
doi = {10.1109/CVPR.2016.407},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Heber{\_}Convolutional{\_}Networks{\_}for{\_}CVPR{\_}2016{\_}paper.pdf:pdf},
pages = {3746--3754},
title = {{Convolutional Networks for Shape from Light Field}}
}
@article{Gupta2015a,
abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05192v1},
author = {Gupta, Abhinav and Efros, Alexei a},
doi = {10.1109/ICCV.2015.167},
eprint = {arXiv:1505.05192v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Doersch{\_}Unsupervised{\_}Visual{\_}Representation{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {arXiv preprint},
pages = {1422--1430},
pmid = {903},
title = {{Unsupervised Visual Representation Learning by Context Prediction}},
year = {2015}
}
@article{Rennes2011,
author = {Rennes, Inria and Atlantique, Bretagne},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/0000793.pdf:pdf},
isbn = {9781457705397},
pages = {793--796},
title = {{IMAGE COMPRESSION USING THE ITERATION-TUNED AND ALIGNED DICTIONARY Joaquin Zepeda , Christine Guillemot , Ewa Kijak}},
year = {2011}
}
@article{Isola2015,
abstract = {We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.},
archivePrefix = {arXiv},
arxivId = {1511.06811},
author = {Isola, Phillip and Zoran, Daniel and Krishnan, Dilip and Adelson, Edward H.},
eprint = {1511.06811},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1511.06811.pdf:pdf},
journal = {Iclr},
pages = {1--11},
title = {{Learning visual groups from co-occurrences in space and time}},
url = {http://arxiv.org/abs/1511.06811},
year = {2015}
}
@article{Estabridis2012,
abstract = {This paper proposes an adaptive face recognition algorithm to jointly classify and learn from unlabeled data. It presents an efficient design that specifically addresses the case when only a single sample per person is available for training. A dictionary composed of regional descriptors serves as the basis for the recognition system while providing a flexible framework to augment or update dictionary atoms. The algorithm is based on l1 minimization techniques and the decision to update the dictionary is made in an unsupervised mode via non-parametric Bayes. The dictionary learning is done via reverse-OMP to select atoms that are orthogonal or near orthogonal to the current dictionary elements. The proposed algorithm was tested with two face databases showing the capability to handle illumination, scale, and some moderate pose and expression variations. Classification results as high as 96{\%} were obtained with the Georgia Tech database and 94{\%} correct classification rates for the Multi-PIE database for the frontal-view scenarios. {\textcopyright} 2012 IEEE.},
author = {Estabridis, Katia},
doi = {10.1109/THS.2012.6459862},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/06459862.pdf:pdf},
isbn = {9781467327084},
journal = {2012 IEEE International Conference on Technologies for Homeland Security, HST 2012},
keywords = {face recognition,l1 minimization,single sample per person,unsupervised learning},
pages = {280--285},
title = {{Face recognition and learning via adaptive dictionaries}},
year = {2012}
}
@article{Isola2015a,
abstract = {We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.},
archivePrefix = {arXiv},
arxivId = {1511.06811},
author = {Isola, Phillip and Zoran, Daniel and Krishnan, Dilip and Adelson, Edward H.},
eprint = {1511.06811},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1511.06811.pdf:pdf},
journal = {Iclr},
pages = {1--11},
title = {{Learning visual groups from co-occurrences in space and time}},
url = {http://arxiv.org/abs/1511.06811},
year = {2015}
}
@article{Kr,
author = {Kr, Philipp and Berkeley, U C},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Krahenbuhl{\_}Learning{\_}to{\_}Propose{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
title = {{Learning to Propose Objects}}
}
@article{Isola2015b,
abstract = {We propose a self-supervised framework that learns to group visual entities based on their rate of co-occurrence in space and time. To model statistical dependencies between the entities, we set up a simple binary classification problem in which the goal is to predict if two visual primitives occur in the same spatial or temporal context. We apply this framework to three domains: learning patch affinities from spatial adjacency in images, learning frame affinities from temporal adjacency in videos, and learning photo affinities from geospatial proximity in image collections. We demonstrate that in each case the learned affinities uncover meaningful semantic groupings. From patch affinities we generate object proposals that are competitive with state-of-the-art supervised methods. From frame affinities we generate movie scene segmentations that correlate well with DVD chapter structure. Finally, from geospatial affinities we learn groups that relate well to semantic place categories.},
archivePrefix = {arXiv},
arxivId = {1511.06811},
author = {Isola, Phillip and Zoran, Daniel and Krishnan, Dilip and Adelson, Edward H.},
eprint = {1511.06811},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1511.06811.pdf:pdf},
journal = {Iclr},
pages = {1--11},
title = {{Learning visual groups from co-occurrences in space and time}},
url = {http://arxiv.org/abs/1511.06811},
year = {2015}
}
@article{Golts2015,
abstract = {In this paper we present a new approach of incorporating kernels into dictionary learning. The kernel K-SVD algorithm (KKSVD), which has been introduced recently, shows an improvement in classification performance, with relation to its linear counterpart K-SVD. However, this algorithm requires the storage and handling of a very large kernel matrix, which leads to high computational cost, while also limiting its use to setups with small number of training examples. We address these problems by combining two ideas: first we approximate the kernel matrix using a cleverly sampled subset of its columns using the Nystr$\backslash$"{\{}o{\}}m method; secondly, as we wish to avoid using this matrix altogether, we decompose it by SVD to form new "virtual samples," on which any linear dictionary learning can be employed. Our method, termed "Linearized Kernel Dictionary Learning" (LKDL) can be seamlessly applied as a pre-processing stage on top of any efficient off-the-shelf dictionary learning scheme, effectively "kernelizing" it. We demonstrate the effectiveness of our method on several tasks of both supervised and unsupervised classification and show the efficiency of the proposed scheme, its easy integration and performance boosting properties.},
archivePrefix = {arXiv},
arxivId = {1509.05634},
author = {Golts, Alona and Elad, Michael},
doi = {10.1109/JSTSP.2016.2555241},
eprint = {1509.05634},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/07454703.pdf:pdf},
issn = {19324553},
number = {320649},
pages = {1--13},
title = {{Linearized Kernel Dictionary Learning}},
url = {http://arxiv.org/abs/1509.05634},
volume = {10},
year = {2015}
}
@article{Chen2015,
abstract = {Culinary systems, the practice of preparing a refined combination of ingredients that is palatable as well as socially acceptable, are examples of complex dynamical systems. They evolve over time and are affected by a large number of factors. Modeling the dynamic nature of evolution of regional cuisines may provide us a quantitative basis and exhibit underlying processes that have driven them into the present day status. This is especially important given that the potential culinary space is practically infinite because of possible number of ingredient combinations as recipes. Such studies also provide a means to compare and contrast cuisines and to unearth their therapeutic value. Herein we provide rigorous analysis of modeling eight diverse Indian regional cuisines, while also highlighting their uniqueness, and a comparison among those models at the level of flavor compounds which opens up molecular level studies associating them especially with non-communicable diseases such as diabetes.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.01554v1},
author = {Chen, Xinlei and Gupta, Abhinav},
doi = {10.1109/ICCV.2015.168},
eprint = {arXiv:1505.01554v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Chen{\_}Webly{\_}Supervised{\_}Learning{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {978-1-4673-8391-2},
journal = {Iccv},
pages = {1431--1439},
title = {{Webly Supervised Learning of Convolutional Networks}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Chen{\_}Webly{\_}Supervised{\_}Learning{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@article{Pathak2016,
abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
archivePrefix = {arXiv},
arxivId = {1604.07379},
author = {Pathak, Deepak and Donahue, Jeff and Efros, Alexei A},
eprint = {1604.07379},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1604.07379v1.pdf:pdf},
journal = {Cvpr 2016},
title = {{Context Encoders : Feature Learning by Inpainting}},
year = {2016}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
eprint = {arXiv:1406.2661v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/5423-generative-adversarial-nets.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
pages = {2672--2680},
title = {{Generative Adversarial Nets}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Zhang2015a,
abstract = {We study the problem of stochastic optimization for deep learning in the paral-lel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameter vectors they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between lo-cal workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the theoretical analysis of the syn-chronous variant in the quadratic case and prove it achieves the highest possible asymptotic rate of convergence for the center variable. We additionally propose the momentum-based version of the algorithm that can be applied in both syn-chronous and asynchronous settings. An asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6651v4},
author = {Zhang, Sixin and Choromanska, Anna and LeCun, Yann},
eprint = {arXiv:1412.6651v4},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/5761-deep-learning-with-elastic-averaging-sgd.pdf:pdf},
issn = {10495258},
journal = {To appear: ICLRws-2015},
number = {2012},
pages = {1--21},
title = {{Deep learning with Elastic Averaging SGD}},
url = {http://arxiv.org/abs/1412.6651},
year = {2015}
}
@article{Dosovitskiy2014a,
abstract = {We train generative 'up-convolutional' neural networks which are able to generate images of objects given object style, viewpoint, and color. We train the networks on rendered 3D models of chairs, tables, and cars. Our experiments show that the networks do not merely learn all images by heart, but rather find a meaningful representation of 3D models allowing them to assess the similarity of different models, interpolate between given views to generate the missing ones, extrapolate views, and invent new objects not present in the training set by recombining training instances, or even two different object classes. Moreover, we show that such generative networks can be used to find correspondences between different objects from the dataset, outperforming existing approaches on this task.},
archivePrefix = {arXiv},
arxivId = {1411.5928},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Tatarchenko, Maxim and Brox, Thomas},
doi = {10.1109/CVPR.2015.7298761},
eprint = {1411.5928},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1411.5928.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {arXiv preprint arXiv:1411.5928},
pages = {1--14},
pmid = {25246403},
title = {{Learning to Generate Chairs, Tables and Cars with Convolutional Networks}},
year = {2014}
}
@article{Goodfellow2013,
author = {Goodfellow, Ian J and Warde-farley, David and Courville, Aaron},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/goodfellow13.pdf:pdf},
journal = {Journal of Machine Learning Research},
title = {{Maxout Networks}},
year = {2013}
}
@article{Gulcehre2013,
abstract = {In this paper we propose and investigate a novel nonlinear unit, called {\$}L{\_}p{\$} unit, for deep neural networks. The proposed {\$}L{\_}p{\$} unit receives signals from several projections of a subset of units in the layer below and computes a normalized {\$}L{\_}p{\$} norm. We notice two interesting interpretations of the {\$}L{\_}p{\$} unit. First, the proposed unit can be understood as a generalization of a number of conventional pooling operators such as average, root-mean-square and max pooling widely used in, for instance, convolutional neural networks (CNN), HMAX models and neocognitrons. Furthermore, the {\$}L{\_}p{\$} unit is, to a certain degree, similar to the recently proposed maxout unit (Goodfellow et al., 2013) which achieved the state-of-the-art object recognition results on a number of benchmark datasets. Secondly, we provide a geometrical interpretation of the activation function based on which we argue that the {\$}L{\_}p{\$} unit is more efficient at representing complex, nonlinear separating boundaries. Each {\$}L{\_}p{\$} unit defines a superelliptic boundary, with its exact shape defined by the order {\$}p{\$}. We claim that this makes it possible to model arbitrarily shaped, curved boundaries more efficiently by combining a few {\$}L{\_}p{\$} units of different orders. This insight justifies the need for learning different orders for each unit in the model. We empirically evaluate the proposed {\$}L{\_}p{\$} units on a number of datasets and show that multilayer perceptrons (MLP) consisting of the {\$}L{\_}p{\$} units achieve the state-of-the-art results on a number of benchmark datasets. Furthermore, we evaluate the proposed {\$}L{\_}p{\$} unit on the recently proposed deep recurrent neural networks (RNN).},
archivePrefix = {arXiv},
arxivId = {1311.1780},
author = {Gulcehre, Caglar and Cho, Kyunghyun and Pascanu, Razvan and Bengio, Yoshua},
eprint = {1311.1780},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1311.1780v7.pdf:pdf},
keywords = {deep learning,l p unit,multilayer perceptron},
title = {{Learned-Norm Pooling for Deep Feedforward and Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1311.1780},
year = {2013}
}
@article{Goodfellow2013a,
author = {Goodfellow, Ian J and Warde-farley, David and Courville, Aaron},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/goodfellow13.pdf:pdf},
title = {{Maxout Networks}},
year = {2013}
}
@article{Wang2016,
abstract = {The explosive growth in big data has attracted much attention in designing efficient indexing and search methods recently. In many critical applications such as large-scale search and pattern matching, finding the nearest neighbors to a query is a fundamental research problem. However, the straightforward solution using exhaustive comparison is infeasible due to the prohibitive computational complexity and memory requirement. In response, Approximate Nearest Neighbor (ANN) search based on hashing techniques has become popular due to its promising performance in both efficiency and accuracy. Prior randomized hashing methods, e.g., Locality-Sensitive Hashing (LSH), explore data-independent hash functions with random projections or permutations. Although having elegant theoretic guarantees on the search quality in certain metric spaces, performance of randomized hashing has been shown insufficient in many real-world applications. As a remedy, new approaches incorporating data-driven learning methods in development of advanced hash functions have emerged. Such learning to hash methods exploit information such as data distributions or class labels when optimizing the hash codes or functions. Importantly, the learned hash codes are able to preserve the proximity of neighboring data in the original feature spaces in the hash code spaces. The goal of this paper is to provide readers with systematic understanding of insights, pros and cons of the emerging techniques. We provide a comprehensive survey of the learning to hash framework and representative techniques of various types, including unsupervised, semi-supervised, and supervised. In addition, we also summarize recent hashing approaches utilizing the deep learning models. Finally, we discuss the future direction and trends of research in this area.},
archivePrefix = {arXiv},
arxivId = {1509.05472},
author = {Wang, Jun and Liu, Wei and Kumar, Sanjiv and Chang, Shih Fu},
doi = {10.1109/JPROC.2015.2487976},
eprint = {1509.05472},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1509.05472.pdf:pdf},
isbn = {0018-9219 VO  - 104},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Artificial neural networks,Big data,Binary codes,Indexing,Semantics,Tagging,Twitter},
number = {1},
pages = {34--57},
title = {{Learning to hash for indexing big data - A survey}},
volume = {104},
year = {2016}
}
@article{Eccvb,
author = {Eccv, Anonymous},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/kulkarni{\_}ECCV{\_}SPLEAP.pdf:pdf},
number = {i},
title = {{SPLeaP : Soft Pooling of Learned Parts for Image Classification}},
volume = {004}
}
@article{Girshick2015a,
abstract = {Deformable part models (DPMs) and convolutional neu-ral networks (CNNs) are two widely used tools for vi-sual recognition. They are typically viewed as distinct ap-proaches: DPMs are graphical models (Markov random fields), while CNNs are " black-box " non-linear classifiers. In this paper, we show that a DPM can be formulated as a CNN, thus providing a synthesis of the two ideas. Our con-struction involves unrolling the DPM inference algorithm and mapping each step to an equivalent CNN layer. From this perspective, it is natural to replace the standard im-age features used in DPMs with a learned feature extractor. We call the resulting model a DeepPyramid DPM and ex-perimentally validate it on PASCAL VOC object detection. We find that DeepPyramid DPMs significantly outperform DPMs based on histograms of oriented gradients features (HOG) and slightly outperforms a comparable version of the recently introduced R-CNN detection system, while run-ning significantly faster.},
author = {Girshick, Ross and Iandola, Forrest and Darrell, Trevor and Malik, Jitendra},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/dpdpm.pdf:pdf},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Deformable Part Models are Convolutional Neural Networks}},
year = {2015}
}
@article{Ohn-Bar2014,
author = {Ohn-Bar, Eshed and Trivedi, Mohan Manubhai},
doi = {10.1109/CVPRW.2014.32},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/subcat.pdf:pdf},
isbn = {978-1-4799-4308-1},
issn = {21607516},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops},
pages = {179--184},
title = {{Fast and Robust Object Detection Using Visual Subcategories}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}workshops{\_}2014/W03/html/Ohn-Bar{\_}Fast{\_}and{\_}Robust{\_}2014{\_}CVPR{\_}paper.html$\backslash$nhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909977},
year = {2014}
}
@article{Student2014,
author = {Student, Susan and Prof, Petra and Collaborator, Colin},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/bmvc{\_}review (1).pdf:pdf},
pages = {10--13},
title = {{Author Guidelines for the British Machine Vision Conference}},
year = {2014}
}
@inproceedings{He2015,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Arxiv.Org},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1512.03385v1.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
keywords = {Christopher,Deep vision,Himalaya,Praveen,deep learning,denoising auto-encoder,image denoising},
mendeley-tags = {Christopher,Deep vision,Himalaya,Praveen},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
volume = {7},
year = {2015}
}
@article{Wangd,
author = {Wang, Xiaojuan and Zhang, Ting and Jinhui, Guo-jun Qi and Jingdong, Tang},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/pubs-CVPR16-SQ.pdf:pdf},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{Supervised Quantization for Similarity Search}}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1412.6980.pdf:pdf},
journal = {arXiv:1412.6980 [cs]},
keywords = {sgd},
mendeley-tags = {sgd},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980$\backslash$nhttp://www.arxiv.org/pdf/1412.6980.pdf},
year = {2014}
}
@article{Burgos2015a,
author = {Burgos, Xavier and Zepeda, Joaquin and {Le Clerc}, Fran{\c{c}}ois and P{\'{e}}rez, Patrick},
doi = {10.1109/ICCVW.2015.117},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/iccv15{\_}final.pdf:pdf},
isbn = {9780769557205},
journal = {International Conference on Computer Vision},
pages = {37--45},
title = {{Pose and expression-coherent face recovery in the wild}},
year = {2015}
}
@article{Shuman2012,
abstract = {In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogues to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting, and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions.},
archivePrefix = {arXiv},
arxivId = {1211.0053},
author = {Shuman, David I. and Narang, Sunil K. and Frossard, Pascal and Ortega, Antonio and Vandergheynst, Pierre},
doi = {10.1109/MSP.2012.2235192},
eprint = {1211.0053},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1211.0053v2.pdf:pdf},
journal = {arXiv:1211.0053},
title = {{Signal Processing on Graphs: Extending High-Dimensional Data Analysis to Networks and Other Irregular Data Domains}},
url = {http://arxiv.org/abs/1211.0053$\backslash$nhttp://www.arxiv.org/pdf/1211.0053.pdf},
year = {2012}
}
@article{Tang2013,
abstract = {Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide vari-ety of tasks such as speech recognition, im-age classification, natural language process-ing, and bioinformatics. For classification tasks, most of these “deep learning” models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the soft-max layer with a linear support vector ma-chine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neu-ral nets and SVMs in prior art, our results using L2-SVMs show that by simply replac-ing softmax with linear SVMs gives signifi-cant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Rep-resentation LearningWorkshop's face expres-sion recognition challenge.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.0239v2},
author = {Tang, Yichuan},
eprint = {arXiv:1306.0239v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/dlsvm.pdf:pdf},
journal = {Deeplearning.Net},
title = {{Deep Learning using Linear Support Vector Machines}},
url = {http://deeplearning.net/wp-content/uploads/2013/03/dlsvm.pdf},
year = {2013}
}
@article{Hariharan2015,
abstract = {Recognition algorithms based on convolutional networks (CNNs) typically use the output of the last layer as a fea- ture representation. However, the information in this layer may be too coarse spatially to allow precise localization. On the contrary, earlier layers may be precise in localiza- tion but will not capture semantics. To get the best of both worlds, we define the hypercolumn at a pixel as the vector of activations of all CNN units above that pixel. Using hy- percolumns as pixel descriptors, we show results on three fine-grained localization tasks: simultaneous detection and segmentation [22], where we improve state-of-the-art from 49.7 mean APr [22] to 60.0, keypoint localization, where we get a 3.3 point boost over [20], and part labeling, where we show a 6.6 point gain over a strong baseline. 1.},
archivePrefix = {arXiv},
arxivId = {1411.5752},
author = {Hariharan, Bharath and Arbel, Pablo and Girshick, Ross},
doi = {10.1109/CVPR.2015.7298642},
eprint = {1411.5752},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/hypercolumn.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
pages = {447--456},
title = {{Hypercolumns for Object Segmentation and Fine-grained Localization}},
year = {2015}
}
@article{Babenko2015a,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.07493v1},
author = {Babenko, Artem and Lempitsky, Victor},
doi = {10.1109/ICCV.2015.150},
eprint = {arXiv:1510.07493v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Babenko{\_}Aggregating{\_}Local{\_}Deep{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
isbn = {978-1-4673-8391-2},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1269--1277},
title = {{Aggregating Local Deep Features for Image Retrieval}},
url = {http://www.cv-foundation.org/openaccess/content{\_}iccv{\_}2015/html/Babenko{\_}Aggregating{\_}Local{\_}Deep{\_}ICCV{\_}2015{\_}paper.html},
year = {2015}
}
@inproceedings{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. A particularly significant one is data augmentation, which achieves a boost in performance in shallow methods analogous to that observed with CNN-based methods. Finally, we are planning to provide the configurations and code that achieve the state-of-the-art performance on the PASCAL VOC Classification challenge, along with alternative configurations trading-off performance, computation speed and compactness.},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
booktitle = {British Machine Vision Conference},
eprint = {1405.3531},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatfield et al. - Unknown - Return of the Devil in the Details Delving Deep into Convolutional Nets.pdf:pdf;:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/chatfield14.pdf:pdf},
pages = {1--11},
title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
url = {http://arxiv.org/abs/1405.3531},
year = {2014}
}
@article{Eccvc,
author = {Eccv, Anonymous},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/eccvSub.pdf:pdf},
title = {{Kernel Square-Loss Exemplar Machines For Image Retrieval}}
}
@article{Parizi2012,
abstract = {We propose a new latent variable model for scene recognition. Our approach represents a scene as a collection of region models ({\&}{\#}x201C;parts{\&}{\#}x201D;) arranged in a reconfigurable pattern. We partition an image into a predefined set of regions and use a latent variable to specify which region model is assigned to each image region. In our current implementation we use a bag of words representation to capture the appearance of an image region. The resulting method generalizes a spatial bag of words approach that relies on a fixed model for the bag of words in each image region. Our models can be trained using both generative and discriminative methods. In the generative setting we use the Expectation-Maximization (EM) algorithm to estimate model parameters from a collection of images with category labels. In the discriminative setting we use a latent structural SVM (LSSVM). We note that LSSVMs can be very sensitive to initialization and demonstrate that generative training with EM provides a good initialization for discriminative training with LSSVM.},
author = {Parizi, Sobhan Naderi and Oberlin, John G. and Felzenszwalb, Pedro F.},
doi = {10.1109/CVPR.2012.6248001},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/parizi{\_}et{\_}al{\_}{\_}2012{\_}{\_}reconfigurable{\_}{\_}cvpr.pdf:pdf},
isbn = {9781467312264},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2775--2782},
title = {{Reconfigurable models for scene recognition}},
year = {2012}
}
@article{Kobayashi2015,
abstract = {SVM의 기본 정의에 two novel viewpoint를 추가하여 일반 classification task의 성능을 향상시키는 방법 설명 하나는 SVM과 관련된 class를 수집한 것 과 또 다른 하나는 least-square로 reconstruction한 것임. Novel view point parameter 정규화로 SVM을 오직 하나의 정확한 intuition으로 축소시키고, 다른 parameter들을 tuning시킴 SVM분류를 위해 기하학적 해석 뿐만 아니라, 벡터 계수를 연관시켜 라그랑지 승수로 구성},
author = {Kobayashi, Takumi},
doi = {10.1109/CVPR.2015.7298893},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Kobayashi{\_}Three{\_}Viewpoints{\_}Toward{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467369640},
journal = {Cvpr},
pages = {2765--2773},
title = {{Three Viewpoints Toward Exemplar SVM}},
year = {2015}
}
@article{Planck2006a,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. Nevertheless, on the first glance spectral clustering looks a bit mysterious, and it is not obvious to see why it works at all and what it really does. This article is a tutorial introduction to spectral clustering. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:0711.0189v1},
author = {Planck, Max and Luxburg, Ulrike Von},
doi = {10.1007/s11222-007-9033-z},
eprint = {arXiv:0711.0189v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/0711.0189.pdf:pdf},
isbn = {0960-3174},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {graph laplacian,spectral clustering},
number = {March},
pages = {395--416},
pmid = {19784854},
title = {{A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering}},
url = {http://www.springerlink.com/index/10.1007/s11222-007-9033-z},
volume = {17},
year = {2006}
}
@article{Nova2013,
abstract = {In this work we present a review of the state of the art of Learning Vector Quantization (LVQ) classifiers. A taxonomy is proposed which integrates the most relevant LVQ approaches to date. The main concepts associated with modern LVQ approaches are defined. A comparison is made among eleven LVQ classifiers using one real-world and two artificial datasets.},
archivePrefix = {arXiv},
arxivId = {1509.07093},
author = {Nova, David and Est??vez, Pablo A.},
doi = {10.1007/s00521-013-1535-3},
eprint = {1509.07093},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1509.07093v1.pdf:pdf},
issn = {09410643},
journal = {Neural Computing and Applications},
keywords = {Learning vector quantization,Likelihood ratio maximization,Margin maximization,Neural networks,Supervised learning},
pages = {1--14},
title = {{A review of learning vector quantization classifiers}},
year = {2013}
}
@article{Kohonen2003,
author = {Kohonen, Teuvo},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/02c2.pdf:pdf},
journal = {The Handbook of Brain Theory and Neural Networks},
pages = {631--635},
title = {{Learning Vector Quantization}},
year = {2003}
}
@article{Sha2007,
author = {Sha, Fei and Lin, Yuanqing and Saul, Lawrence K and Lee, Daniel D},
doi = {http://dx.doi.org/10.1162/neco.2007.19.8.2004},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/svm{\_}nips02.pdf:pdf},
issn = {0899-7667},
number = {8},
pages = {2004--2031},
title = {{Multiplicative Updates for Nonnegative Quadratic Programming}},
volume = {19},
year = {2007}
}
@inproceedings{Engan1999,
author = {Engan, K and Aase, S O and {Hakon Husoy}, J},
booktitle = {Acoustics, Speech, and Signal Processing, 1999. Proceedings., 1999 IEEE International Conference on},
doi = {10.1109/ICASSP.1999.760624},
issn = {1520-6149},
keywords = {Algorithm design and analysis,Approximation algorithms,Design optimization,ECG signal,Electrocardiography,Iterative algorithms,MSE reduction,Matching pursuit algorithms,Pursuit algorithms,Signal design,Speech,Testing,electrocardiogram,electrocardiography,experiments,frame design,iterative design algorithm,iterative methods,matching pursuits,mean square error methods,mean squared error,medical signal processing,method of optimal directions,optimal directions,optimisation,optimized frames,signal vectors,speech processing,speech signal,training set,vector selection algorithms},
pages = {2443--2446 vol.5},
title = {{Method of Optimal Directions for Frame Design}},
volume = {5},
year = {1999}
}
@article{Wang2012a,
abstract = {Trata la caracterizaci{\'{o}}n de im{\'{a}}genes desde un punto de vista jer{\'{a}}rquico usando sparse coding. Usa una formulaci{\'{o}}n a la cual le a{\~{n}}aden una restricci{\'{o}}n jer{\'{a}}rquica para obtener un descriptor. La imagen se particiona en un conjunto de c{\'{e}}lula/ regiones usando una grilla fija. Luego se procesan cada regi{\'{o}}n usando su formulaci{\'{o}}n. Las descripciones de todas las regiones se concatenan para construir un {\'{u}}nico descriptor para la imagen.},
author = {Wang, Jiang and Yuan, Junsong and Chen, Zhuoyuan and Wu, Ying},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/wang12a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning - Asian conf. on Machine Learning},
pages = {491--505},
title = {{Spatial Locality-Aware Sparse Coding and Dictionary Learning}},
volume = {1},
year = {2012}
}
@article{Everingham2014,
abstract = {The Pascal Visual Object Classes (VOC) challenge consists of two components: (i) a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii) an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008–2012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. Mark},
author = {Everingham, Mark and Eslami, S. M Ali and {Van Gool}, Luc and Williams, Christopher K I and Winn, John and Zisserman, Andrew},
doi = {10.1007/s11263-014-0733-5},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/everingham15.pdf:pdf},
isbn = {0920-5691},
issn = {15731405},
journal = {International Journal of Computer Vision},
keywords = {Benchmark,Database,Object detection,Object recognition,Segmentation},
number = {1},
pages = {98--136},
pmid = {20713396},
title = {{The Pascal Visual Object Classes Challenge: A Retrospective}},
volume = {111},
year = {2014}
}
@article{Wang2013,
abstract = {Sparse coding approximates the data sample as a sparse linear combination of some basic codewords and uses the sparse codes as new presentations. In this paper, we investigate learning discriminative sparse codes by sparse coding in a semi-supervised manner, where only a few training samples are labeled. By using the manifold structure spanned by the data set of both labeled and unlabeled samples and the constraints provided by the labels of the labeled samples, we learn the variable class labels for all the samples. Furthermore, to improve the discriminative ability of the learned sparse codes, we assume that the class labels could be predicted from the sparse codes directly using a linear classifier. By solving the codebook, sparse codes, class labels and classifier parameters simultaneously in a unified objective function, we develop a semi-supervised sparse coding algorithm. Experiments on two real-world pattern recognition problems demonstrate the advantage of the proposed methods over supervised sparse coding methods on partially labeled data sets.},
archivePrefix = {arXiv},
arxivId = {1311.6834},
author = {Wang, Jim Jing-Yan and Gao, Xin},
doi = {10.1109/IJCNN.2014.6889449},
eprint = {1311.6834},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1311.6834.pdf:pdf},
title = {{Semi-Supervised Sparse Coding}},
url = {http://arxiv.org/abs/1311.6834 http://dx.doi.org/10.1109/IJCNN.2014.6889449},
year = {2013}
}
@article{Taylor1986,
author = {Taylor, A E and Lay, D C},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/math3263m.pdf:pdf},
title = {{Introduction to Functional Analysis}},
year = {1986}
}
@inproceedings{Lim2014,
author = {Lim, D and Lanckriet, Gert},
booktitle = {International Conference on Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/lim14.pdf:pdf},
isbn = {9781634393973},
keywords = {efficient learning of mahalanobis,metrics for ranking},
pages = {1980--1988},
title = {{Efficient Learning of Mahalanobis Metrics for Ranking}},
url = {http://jmlr.org/proceedings/papers/v32/lim14.html},
volume = {32},
year = {2014}
}
@article{Ge2013a,
author = {Ge, Tiezheng and Ke, Qifa and Sun, Jian},
doi = {10.5244/C.27.132},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/BMVC13{\_}SparseCoded.pdf:pdf},
isbn = {1-901725-49-9},
journal = {Procedings of the British Machine Vision Conference 2013},
pages = {132.1--132.11},
title = {{Sparse-Coded Features for Image Retrieval}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0132/index.html},
year = {2013}
}
@article{Wanner2011,
abstract = {In this paper, we present a novel method for the generation of Epipolar-Image (EPI) representations of 4D Light Fields (LF) from raw data captured by a single lens “Focused Plenoptic Camera”. Compared to other LF representations which are usually used in the context of computational photography with Plenoptic Cameras, the EPI representation is more suitable for image analysis tasks - providing direct access to scene geometry and reflectance properties. The generation of EPIs requires a set of “all in focus” (full depth of field) images from different views of a scene. Hence, the main contribution of this paper is a novel algorithm for the rendering of such images from a single raw image captured with a Focused Plenoptic Camera. The main advantage of the proposed approach over existing full depth of field methods is that is able to cope with non-Lambertian reflectance in the scene.},
author = {Wanner, Sven and Fehr, Janis and J{\"{a}}hne, Bernd},
doi = {10.1007/978-3-642-24028-7_9},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Wanner{\_}ISVC{\_}11.pdf:pdf},
isbn = {9783642240270},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {90--101},
title = {{Generating EPI representations of 4D light fields with a single lens focused plenoptic camera}},
volume = {6938 LNCS},
year = {2011}
}
@inproceedings{Perronnin2015,
author = {Perronnin, Florent and Larlus, Diane},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2015.7298998},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Perronnin{\_}Fisher{\_}Vectors{\_}Meet{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {978-1-4673-6964-0},
pages = {3743--3752},
title = {{Fisher vectors meet Neural Networks: A hybrid classification architecture}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7298998},
year = {2015}
}
@article{Hu2013,
author = {Hu, Chenhui and Cheng, Lin and Sepulcre, Jorge},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/graphrecon{\_}isbi{\_}reprocessed{\_}1.pdf:pdf},
journal = {{\ldots} Imaging (ISBI), 2013 {\ldots}},
title = {{A graph theoretical regression model for brain connectivity learning of Alzheimer'S disease}},
url = {http://lu.seas.harvard.edu/files/yuelu/files/graphrecon{\_}isbi{\_}reprocessed{\_}1.pdf$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6556550},
year = {2013}
}
@article{Liu,
author = {Liu, Xianming and Cheung, Gene and Wu, Xiaolin and Zhao, Debin},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/icip2015{\_}nii.pdf:pdf},
isbn = {9781479983391},
pages = {2--6},
title = {{INTER-BLOCK CONSISTENT SOFT DECODING OF JPEG IMAGES WITH SPARSITY AND GRAPH-SIGNAL SMOOTHNESS PRIORS School of Computer Science and Technology , Harbin Institute of Technology , China National Institute of Informatics , Japan}}
}
@article{Bi2003,
abstract = {We describe a methodology for performing variable ranking and selection using support vector machines (SVMs). The method constructs a series of sparse linear SVMs to generate linear models that can generalize well, and uses a subset of nonzero weighted variables found by the linear models to produce a final nonlinear model. The method exploits the fact that a linear SVM (no kernels) with l1-norm regularization inherently performs variable selection as a side-effect of minimizing capacity of the SVM model. The distribution of the linear model weights provides a mechanism for ranking and interpreting the effects of variables. Starplots are used to visualize the magnitude and variance of the weights for each variable. We illustrate the effectiveness of the methodology on synthetic data, benchmark problems, and challenging regression problems in drug design. This method can dramatically reduce the number of variables and outperforms SVMs trained using all attributes and using the attributes selected according to correlation coefficients. The visualization of the resulting models is useful for understanding the role of underlying variables.},
author = {Bi, Jinbo and Bennett, Kristin P and Embrechts, Mark and Breneman, Curt M and Song, Minghu},
doi = {10.1162/153244303322753643},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/bi03a.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {bootstrap aggregation,dimensionality reduction,model visualization,pattern search,regression,support vector machines,variable selection},
pages = {1229--1243},
title = {{Dimensionality Reduction via Sparse Support Vector Machines}},
url = {http://www.crossref.org/jmlr{\_}DOI.html},
volume = {3},
year = {2003}
}
@article{Shivaswamy2008,
author = {Shivaswamy, P.K. and Jebara, Tony},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/nips08.pdf:pdf},
isbn = {9781605609492},
journal = {Advances in Neural Information Processing Systems},
pages = {1--8},
title = {{Relative margin machines}},
url = {http://www1.cs.columbia.edu/{~}pks2103/pubs/paper{\_}relative.pdf},
volume = {21},
year = {2008}
}
@article{Gupta2011,
abstract = {Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition tool for multivariate data. Non-negative bases allow strictly additive combinations which have been shown to be part-based as well as relatively sparse. We pursue a discriminative decomposition by coupling NMF objective with a maximum margin classifier, specifically a support vector machine (SVM). Conversely, we propose anNMFbased regularizer for SVM. We formulate the joint update equations and propose a new method which identifies the decomposition as well as the classification parameters. We present classification results on synthetic as well as real datasets.},
author = {Gupta, Mithun Das and Xiao, Jing and Jose, San},
doi = {10.1109/CVPR.2011.5995492},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/NMF{\_}SVM{\_}CVPR{\_}11.pdf:pdf},
isbn = {978-1-4577-0394-2},
journal = {Computer Vision and Pattern Recognition (CVPR)},
pages = {2841--2848},
title = {{Non-negative Matrix Factorization as a Feature Selection Tool for Maximum Margin Classifiers}},
year = {2011}
}
@article{Cotter2013,
abstract = {We show how to train SVMs with an opti- mal guarantee on the number of support vec- tors (up to constants), and with sample com- plexity and training runtime bounds match- ing the best known for kernel SVM optimiza- tion (i.e. without any additional asymptotic cost beyond standard SVM training). Our method is simple to implement and works well in practice.},
author = {Cotter, Andrew and Shalev-Shwartz, Shai and Srebro, Nathan},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cotter13.pdf:pdf},
journal = {Proceedings of the 30th international conference on machine learning - ICML '13},
pages = {266--274},
title = {{Learning optimally sparse support vector machines}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/ICML2013{\_}cotter13},
year = {2013}
}
@article{Skretting2010,
author = {Skretting, K and Engan, K},
doi = {10.1109/TSP.2010.2040671},
issn = {1053-587X},
journal = {Signal Processing, IEEE Transactions on},
keywords = {adaptive filters;learning (artificial intelligence},
month = {apr},
number = {4},
pages = {2121--2130},
title = {{Recursive Least Squares Dictionary Learning Algorithm}},
volume = {58},
year = {2010}
}
@article{Heo2014,
abstract = {Many binary code embedding techniques have been proposed for large-scale approximate nearest neighbor search in computer vision. Recently, product quantization that encodes the cluster index in each subspace has been shown to provide impressive accuracy for nearest neighbor search. In this paper, we explore a simple question: is it best to use all the bit budget for encoding a cluster index in each subspace? We have found that as data points are located farther away from the centers of their clusters, the error of estimated distances among those points becomes larger. To address this issue, we propose a novel encoding scheme that distributes the available bit budget to encoding both the cluster index and the quantized distance between a point and its cluster center. We also propose two different distance metrics tailored to our encoding scheme. We have tested our method against the-state-of-the-art techniques on several well-known benchmarks, and found that our method consistently improves the accuracy over other tested methods. This result is achieved mainly because our method accurately estimates distances between two data points with the new binary codes and distance metric.},
author = {Heo, Jae Pil and Lin, Zhe and Yoon, Sung Eui},
doi = {10.1109/CVPR.2014.274},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/CVPR14{\_}DPQ.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
keywords = {Large-scale search,binary code,image retrieval},
pages = {2139--2146},
title = {{Distance encoded product quantization}},
year = {2014}
}
@article{Wu2015,
abstract = {3D shape is a crucial but heavily underutilized cue in today's computer vision systems, mostly due to the lack of a good generic shape representation. With the recent availability of inexpensive 2.5D depth sensors (e.g. Microsoft Kinect), it is becoming increasingly important to have a powerful 3D shape representation in the loop. Apart from category recognition, recovering full 3D shapes from viewbased 2.5D depth maps is also a critical part of visual understanding. To this end, we propose to represent a geometric 3D shape as a probability distribution of binary variables on a 3D voxel grid, using a Convolutional Deep Belief Network. Our model, 3D ShapeNets, learns the distribution of complex 3D shapes across different object categories and arbitrary poses from raw CAD data, and discovers hierarchical compositional part representations automatically. It naturally supports joint object recognition and shape completion from 2.5D depth maps, and it enables active object recognition through view planning. To train our 3D deep learning model, we construct ModelNet – a large-scale 3D CAD model dataset. Extensive experiments show that our 3D deep representation enables significant performance improvement over the-state-of-the-arts in a variety of tasks.},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.5670v3},
author = {Wu, Zhirong and Song, Shuran},
doi = {10.1109/CVPR.2015.7298801},
eprint = {arXiv:1406.5670v3},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cvpr2015{\_}wu.pdf:pdf},
isbn = {9781467369640},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR2015)},
pages = {1--9},
title = {{3D ShapeNets : A Deep Representation for Volumetric Shapes}},
year = {2015}
}
@article{Chatfield2015a,
abstract = {We investigate the gains in precision and speed, that can be obtained by using Convolutional Networks (ConvNets) for on-the-fly retrieval - where classifiers are learnt at run time for a textual query from downloaded images, and used to rank large image or video datasets. We make three contributions: (i) we present an evaluation of state-of-the-art image representations for object category retrieval over standard benchmark datasets containing 1M+ images; (ii) we show that ConvNets can be used to obtain features which are incredibly performant, and yet much lower dimensional than previous state-of-the-art image representations, and that their dimensionality can be reduced further without loss in performance by compression using product quantization or binarization. Consequently, features with the state-of-the-art performance on large-scale datasets of millions of images can fit in the memory of even a commodity GPU card; (iii) we show that an SVM classifier can be learnt within a ConvNet framework on a GPU in parallel with downloading the new training images, allowing for a continuous refinement of the model as more images become available, and simultaneous training and ranking. The outcome is an on-the-fly system that significantly outperforms its predecessors in terms of: precision of retrieval, memory requirements, and speed facilitating accurate on-the-fly learning and ranking in under a second on a single GPU.},
archivePrefix = {arXiv},
arxivId = {1407.4764},
author = {Chatfield, Ken and Simonyan, Karen and Zisserman, Andrew},
doi = {10.1007/978-3-319-16865-4_9},
eprint = {1407.4764},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/chatfield14a.pdf:pdf},
isbn = {9783319168647},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
pages = {129--145},
title = {{Efficient On-The-fly category retrieval using ConvNets and GPUs}},
volume = {9003},
year = {2015}
}
@article{Chatfield2015,
abstract = {The objective of this work is to visually search large-scale video datasets for semantic entities specified by a text query. The paradigm we explore is constructing visual models for such semantic entities on-the-fly, i.e. at run time, by using an image search engine to source visual training data for the text query. The approach combines fast and accurate learning and retrieval, and enables videos to be returned within seconds of specifying a query. We describe three classes of queries, each with its associated visual search method: object instances (using a bag of visual words approach for matching); object categories (using a discriminative classifier for ranking key frames); and faces (using a discriminative classifier for ranking face tracks). We discuss the features suitable for each class of query, for example Fisher vectors or features derived from convolutional neural networks (CNNs), and how these choices impact on the trade-off between three important performance measures for a real-time system of this kind, namely: (1) accuracy, (2) memory footprint, and (3) speed. We also discuss and compare a number of important implementation issues, such as how to remove ‘outliers' in the downloaded images efficiently, and how to best obtain a single descriptor for a face track. We also sketch the architecture of the real-time on-the-fly system. Quantitative results are given on a number of large-scale image and video benchmarks (e.g. TRECVID INS, MIRFLICKR-1M), and we further demonstrate the performance and real-world applicability of our methods over a dataset sourced from 10,000 h of unedited footage from BBC News, comprising 5M+ key frames.},
author = {Chatfield, Ken and Arandjelovi{\'{c}}, Relja and Parkhi, Omkar and Zisserman, Andrew},
doi = {10.1007/s13735-015-0077-0},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/chatfield15.pdf:pdf},
issn = {2192-6611},
journal = {International Journal of Multimedia Information Retrieval},
keywords = {Himalaya,convolutional neural networks,face retrieval,object category retrieval and,object instance retrieval,on-the-fly,recognition},
mendeley-tags = {Himalaya},
number = {2},
pages = {75--93},
title = {{On-the-fly learning for visual search of large-scale image and video datasets}},
url = {http://link.springer.com/10.1007/s13735-015-0077-0$\backslash$nhttp://link.springer.com/article/10.1007/s13735-015-0077-0},
volume = {4},
year = {2015}
}
@article{Babenko2015,
author = {Babenko, Artem and Lempitsky, Victor},
doi = {10.1109/TPAMI.2014.2361319},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cvpr2012.pdf:pdf},
isbn = {9781467312288},
issn = {0162-8828},
number = {6},
pages = {1247--1260},
pmid = {26357346},
title = {{The Inverted Multi-Index}},
volume = {37},
year = {2015}
}
@article{McHenry2005,
abstract = { This paper addresses the problem of finding glass objects in images. Visual cues obtained by combining the systematic distortions in background texture occurring at the boundaries of transparent objects with the strong highlights typical of glass surfaces are used to train a hierarchy of classifiers, identify glass edges, and find consistent support regions for these edges. Qualitative and quantitative experiments involving a number of different classifiers and real images are presented.},
author = {McHenry, Kenton and Ponce, Jean and Forsyth, David},
doi = {10.1109/CVPR.2005.161},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cvpr05b.pdf:pdf},
isbn = {0769523722},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {1},
pages = {973--979},
title = {{Finding glass}},
volume = {2},
year = {2005}
}
@article{Boyd2010,
abstract = {printed},
author = {Boyd, Stephen},
doi = {10.1561/2200000016},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/admm{\_}distr{\_}stats.pdf:pdf},
isbn = {1935823719358},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {1},
pages = {1--122},
title = {{Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers}},
volume = {3},
year = {2010}
}
@article{Maeno2013,
abstract = {Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance of such objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light field image as an input and model the distortion of the light field caused by the refractive property of a transparent object. We propose a new feature, called the light field distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.},
author = {Maeno, Kazuki and Nagahara, Hajime and Shimada, Atsushi and Taniguchi, Rin Ichiro},
doi = {10.1109/CVPR.2013.359},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/CVIU{\_}Xu.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {Image feature,Light field,Object recognition,Transparent object},
pages = {2786--2793},
publisher = {Elsevier Ltd.},
title = {{Light field distortion feature for transparent object recognition}},
url = {http://dx.doi.org/10.1016/j.cviu.2015.02.009},
volume = {139},
year = {2013}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/liszt-franz-Campanella-etude-no-3-497.pdf:pdf},
title = {{liszt-franz-Campanella-etude-no-3-497.pdf}}
}
@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3–7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1508.06576v2.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {eural algorithm of artistic,style},
pages = {3--7},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}
@article{Eccva,
author = {Eccv, Anonymous and Networks, Neural and Cnns, The},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/kulkarni{\_}ECCV.pdf:pdf},
title = {{Parts learning}}
}
@article{Maeno2013a,
abstract = {Current object-recognition algorithms use local features, such as scale-invariant feature transform (SIFT) and speeded-up robust features (SURF), for visually learning to recognize objects. These approaches though cannot apply to transparent objects made of glass or plastic, as such objects take on the visual features of background objects, and the appearance of such objects dramatically varies with changes in scene background. Indeed, in transmitting light, transparent objects have the unique characteristic of distorting the background by refraction. In this paper, we use a single-shot light field image as an input and model the distortion of the light field caused by the refractive property of a transparent object. We propose a new feature, called the light field distortion (LFD) feature, for identifying a transparent object. The proposal incorporates this LFD feature into the bag-of-features approach for recognizing transparent objects. We evaluated its performance in laboratory and real settings.},
author = {Maeno, Kazuki and Nagahara, Hajime and Shimada, Atsushi and Taniguchi, Rin Ichiro},
doi = {10.1109/CVPR.2013.359},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Maeno{\_}Light{\_}Field{\_}Distortion{\_}2013{\_}CVPR{\_}paper.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {Image feature,Light field,Object recognition,Transparent object},
pages = {2786--2793},
title = {{Light field distortion feature for transparent object recognition}},
year = {2013}
}
@article{Lobel2013,
abstract = {Currently, Bag-of-Visual-Words (BoVW) and part-based methods are the $\backslash$nmost popular approaches for visual recognition. In both cases, a mid-level $\backslash$nrepresentation is built on top of low-level image descriptors and top-level $\backslash$nclassifiers use this mid-level representation to achieve visual recognition. $\backslash$nWhile in current part-based approaches, mid- and top-level representations are $\backslash$nusually jointly trained, this is not the usual case for BoVW schemes. A main $\backslash$nreason for this is the complex data association problem related to the usual $\backslash$nlarge dictionary size needed by BoVW approaches. As a further observation, $\backslash$ntypical solutions based on BoVW and part-based representations are usually $\backslash$nlimited to extensions of binary classification schemes, a strategy that ignores $\backslash$nrelevant correlations among classes. In this work we propose a novel $\backslash$nhierarchical approach to visual recognition based on a BoVW scheme that jointly $\backslash$nlearns suitable mid- and top-level representations. Furthermore, using a $\backslash$nmax-margin learning framework, the proposed approach directly handles the $\backslash$nmulticlass case at both levels of abstraction. We test our proposed method using $\backslash$nseveral popular benchmark datasets. As our main result, we demonstrate that, by $\backslash$ncoupling learning of mid- and top-level representations, the proposed approach $\backslash$nfosters sharing of discriminative visual words among target classes, being able $\backslash$nto achieve state-of-the-art recognition performance using far less visual words $\backslash$nthan previous approaches.},
author = {Lobel, Hans and Vidal, Rene and Soto, Alvaro},
doi = {10.1109/ICCV.2013.213},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Lobel{\_}Hierarchical{\_}Joint{\_}Max-Margin{\_}2013{\_}ICCV{\_}paper.pdf:pdf},
isbn = {9781479928392},
issn = {1550-5499},
journal = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {1697--1704},
title = {{Hierarchical joint max-margin learning of mid and top level representations for visual recognition}},
year = {2013}
}
@article{Rowe2015,
author = {Rowe, Angela},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/pk{\_}parts.pdf:pdf},
title = {{Mid- ­ ‐ level clouds}},
year = {2015}
}
@article{Gregor2015,
abstract = {This paper introduces the Deep Recurrent Atten- tive Writer (DRAW) neural network architecture for image generation. DRAWnetworks combine a novel spatial attention mechanism that mimics the foveation of the human eye, with a sequential variational auto-encoding framework that allows for the iterative construction of complex images. The system substantially improves on the state of the art for generative models on MNIST, and, when trained on the Street View House Numbers dataset, it generates images that cannot be distin- guished from real data with the naked eye. 1.},
archivePrefix = {arXiv},
arxivId = {1502.04623},
author = {Gregor, K and Danihelka, I and Graves, A and Wierstra, D},
eprint = {1502.04623},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1502.04623.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {11},
title = {{DRAW: A Recurrent Neural Network For Image Generation}},
url = {http://arxiv.org/abs/1502.04623},
year = {2015}
}
@article{Zhu2015,
abstract = {object segmentation을 활용하여 정확한 object detection이 되는 방법 소개 Markov random field에 기반하여 추론하고, CNN을 통해 object appearance에 대한 가설에 score를 매겨서 score에 기반해 segment하여 object segment에 대해 정확히 pooling이 가능},
author = {Zhu, Yukun and Urtasun, Raquel and Salakhutdinov, Ruslan and Fidler, Sanja},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Zhu{\_}segDeepM{\_}Exploiting{\_}Segmentation{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
journal = {Cvpr},
title = {{segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection}},
year = {2015}
}
@article{Hosang2015,
abstract = {Current top performing object detectors employ detection proposals to guide the search for objects, thereby avoiding exhaustive sliding window search across images. Despite the popularity and widespread use of detection proposals, it is unclear which trade-offs are made when using them during object detection. We provide an in-depth analysis of twelve proposal methods along with four baselines regarding proposal repeatability, ground truth annotation recall on PASCAL and ImageNet, and impact on DPM and R-CNN detection performance. Our analysis shows that for object detection improving proposal localisation accuracy is as important as improving recall. We introduce a novel metric, the average recall (AR), which rewards both high recall and good localisation and correlates surprisingly well with detector performance. Our findings show common strengths and weaknesses of existing methods, and provide insights and metrics for selecting and tuning proposal methods.},
archivePrefix = {arXiv},
arxivId = {1502.05082},
author = {Hosang, Jan and Benenson, Rodrigo and Doll{\'{a}}r, Piotr and Schiele, Bernt},
doi = {10.1109/TPAMI.2015.2465908},
eprint = {1502.05082},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1502.05082.pdf:pdf},
issn = {0162-8828},
journal = {Arxiv},
pages = {2014},
title = {{What makes for effective detection proposals?}},
url = {http://arxiv.org/abs/1502.05082},
year = {2015}
}
@article{Wangc,
author = {Wang, Ting-chun and Berkeley, U C and Efros, Alexei A and Berkeley, U C and Ramamoorthi, Ravi},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/lfocclusion.pdf:pdf},
title = {{Occlusion-aware Depth Estimation Using Light-field Cameras}}
}
@article{Xu2015,
abstract = {The segmentation of transparent objects can be very useful in computer vision applications. However, because they borrow texture from their background and have a similar appearance to their surroundings, transparent objects are not handled well by regular image segmentation methods. We propose a method that overcomes these problems using the consistency and distortion properties of a light-field image. Graph-cut optimization is applied for the pixel labeling problem. The light-field linearity is used to estimate the likelihood of a pixel belonging to the transparent object or Lambertian background, and the occlusion detector is used to find the occlusion boundary. We acquire a light field dataset for the transparent object, and use this dataset to evaluate our method. The results demonstrate that the proposed method successfully segments transparent objects from the background.},
archivePrefix = {arXiv},
arxivId = {1511.06853},
author = {Xu, Yichao and Nagahara, Hajime and Shimada, Atsushi and Taniguchi, Rin-ichiro},
doi = {10.1109/ICCV.2015.393},
eprint = {1511.06853},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Xu{\_}TransCut{\_}Transparent{\_}Object{\_}ICCV{\_}2015{\_}paper.pdf:pdf},
title = {{TransCut: Transparent Object Segmentation from a Light-Field Image}},
url = {http://arxiv.org/abs/1511.06853},
year = {2015}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {1405.0312},
author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Doll{\'{a}}r, Piotr},
eprint = {1405.0312},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1405.0312v3.pdf:pdf},
pages = {1--15},
title = {{Microsoft COCO: Common Objects in Context}},
url = {http://arxiv.org/abs/1405.0312},
year = {2014}
}
@article{Yu,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.00278v1},
author = {Yu, Licheng and Park, Eunbyung and Berg, Alexander C and Berg, Tamara L},
doi = {10.1109/ICCV.2015.283},
eprint = {arXiv:1506.00278v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1506.00278v1.pdf:pdf},
title = {{Visual Madlibs : Fill in the blank Image Generation and Question Answering}}
}
@article{Russakovsky2012,
author = {Russakovsky, Olga and Fei--Fei, Li},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/eccv10workshop-Attributes.pdf:pdf},
journal = {Trends and Topics in Computer Vision},
pages = {1--14},
title = {{Attribute Learning in Large--Scale Datasets}},
volume = {6553},
year = {2012}
}
@article{Donoho2015,
author = {Donoho, David},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/50YearsDataScience.pdf:pdf},
pages = {1--41},
title = {{50 years of data collection UNHCR expce}},
year = {2015}
}
@article{Large2012,
author = {Large, Imagenet and Visual, Scale and Challenge, Recognition},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/master.pdf:pdf},
number = {1},
title = {{Maximum Margin Linear Classifiers in Unions of Subspaces}},
year = {2012}
}
@article{Mousavi,
archivePrefix = {arXiv},
arxivId = {arXiv:1508.04065v1},
author = {Mousavi, Ali and Patel, Ankit B. and Baraniuk, Richard G.},
eprint = {arXiv:1508.04065v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1508.04065.pdf:pdf},
title = {{A deep learning approach to structured signal recovery}},
url = {http://arxiv.org/pdf/1508.04065.pdf}
}
@article{Marx,
author = {Marx, Karl},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/FocusedPlenoptic.pdf:pdf},
isbn = {9781424445332},
journal = {Camera},
title = {{The Focused Plenoptic Camera Plenoptic Camera , Adelson 1992}}
}
@article{Malinowski2015,
abstract = {We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation of this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (question). Our result doubles the performance of the previous best result on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. Further annotations were collected to study human consensus, which is related to the ambiguities inherent in this challenging task.},
archivePrefix = {arXiv},
arxivId = {1505.01121},
author = {Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
doi = {10.1109/ICCV.2015.9},
eprint = {1505.01121},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/8391a001.pdf:pdf},
title = {{Ask Your Neurons: A Neural-based Approach to Answering Questions about Images}},
url = {http://arxiv.org/abs/1505.01121},
year = {2015}
}
@article{Girshick2015,
abstract = {This paper proposes Fast R-CNN, a clean and fast framework for object detection. Compared to traditional R-CNN, and its accelerated version SPPnet, Fast R-CNN trains networks using a multi-task loss in a single training stage. The multi-task loss simplifies learning and improves detection accuracy. Unlike SPPnet, all network layers can be updated during fine-tuning. We show that this difference has practical ramifications for very deep networks, such as VGG16, where mAP suffers when only the fully-connected layers are updated. Compared to "slow" R-CNN, Fast R-CNN is 9x faster at training VGG16 for detection, 213x faster at test-time, and achieves a significantly higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains VGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is implemented in Python and C++ and is available under the open-source MIT License at https://github.com/rbgirshick/fast-rcnn},
archivePrefix = {arXiv},
arxivId = {1504.08083},
author = {Girshick, Ross},
doi = {10.1109/ICCV.2015.169},
eprint = {1504.08083},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/8391b440.pdf:pdf},
journal = {Arxiv},
keywords = {CNN SOTA,Deep vision,Praveen},
mendeley-tags = {CNN SOTA,Deep vision,Praveen},
title = {{Fast R-CNN}},
url = {http://arxiv.org/abs/1504.08083},
year = {2015}
}
@article{Meng2015,
author = {Meng, Lingfei and Innovations, Ricoh and Lu, Liyang and Bedard, Noah and Berkner, Kathrin and Innovations, Ricoh},
doi = {10.1109/ICCV.2015.392},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/8391d433.pdf:pdf},
pages = {3433--3441},
title = {{Single-shot Specular Surface Reconstruction with Gonio-plenoptic Imaging}},
year = {2015}
}
@article{Palatucci2009,
abstract = {We consider the problem of zero-shot learning, where the goal is to$\backslash$nlearn a clas- sifier f : X �?Y that must predict novel values of$\backslash$nY that were omitted from the training set. To achieve this, we define$\backslash$nthe notion of a semantic output code classifier (SOC) which utilizes$\backslash$na knowledge base of semantic properties of Y to extrapolate to novel$\backslash$nclasses. We provide a formalism for this type of classifier and study$\backslash$nits theoretical properties in a PAC framework, showing conditions$\backslash$nun- der which the classifier can accurately predict novel classes.$\backslash$nAs a case study, we build a SOC classifier for a neural decoding$\backslash$ntask and show that it can often predict words that people are thinking$\backslash$nabout from functional magnetic resonance images (fMRI) of their neural$\backslash$nactivity, even without training examples for those words.},
author = {Palatucci, Mark and Hinton, Geoffrey E and Pomerleau, Dean and Mitchell, Tom M},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/zero-shot-learning.pdf:pdf},
isbn = {9781615679119},
issn = {{\textless}null{\textgreater}},
journal = {Neural Information Processing Systems},
keywords = {Machine learning,zero-shot learning},
pages = {1--9},
title = {{Zero-Shot Learning with Semantic Output Codes}},
year = {2009}
}
@article{Ioffe2015,
abstract = {{Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch{\}}. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1502.03167.pdf:pdf},
journal = {Arxiv},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@article{Gupta2015,
abstract = {This work explores the use of spatial context as a source of free and plentiful supervisory signal for training a rich visual representation. Given only a large, unlabeled image collection, we extract random pairs of patches from each image in the collection and train a discriminative model to predict their relative position within the image. We argue that doing well on this task will require the model to learn about the layout of visual objects and object parts. We demonstrate that the feature representation learned using this within-image context prediction task is indeed able to capture visual similarity across images. For example, this representation allows us to perform unsupervised visual discovery of objects like cats, people, and even birds from the Pascal VOC 2011 detection dataset. Furthermore, we show that the learned features, when used as pre-training for the R-CNN object detection pipeline, provide a significant boost over random initialization on Pascal object detection, resulting in state-of-the-art performance among algorithms which use only Pascal-provided training set annotations.},
archivePrefix = {arXiv},
arxivId = {arXiv:1505.05192v1},
author = {Gupta, Abhinav and Efros, Alexei a},
doi = {10.1109/ICCV.2015.167},
eprint = {arXiv:1505.05192v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/8391b422.pdf:pdf},
journal = {arXiv preprint},
title = {{Unsupervised Visual Representation Learning by Context Prediction}},
year = {2015}
}
@article{Sutskever2013,
author = {Sutskever, I and Martens, J},
doi = {10.1109/ICASSP.2013.6639346},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/sutskever13.pdf:pdf},
isbn = {978-1-4799-0356-6},
journal = {{\ldots}  on Machine Learning  {\ldots}},
number = {2010},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/icml2013{\_}sutskever13},
year = {2013}
}
@article{Yao2015,
author = {Yao, Ting and Mei, Tao and Ngo, Chong-wah},
doi = {10.1109/ICCV.2015.12},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/8391a028.pdf:pdf},
journal = {IEEE International Conference on Computer Vision},
title = {{Learning Query and Image Similarities with Ranking Canonical Correlation Analysis}},
year = {2015}
}
@article{Blancs2014,
author = {Blancs, Champs},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/icip{\_}demosaicking{\_}last2.pdf:pdf},
isbn = {9781479957514},
pages = {5482--5486},
title = {{DISPARITY-GUIDED DEMOSAICKING OF LIGHT FIELD IMAGES Mozhdeh Seifi , Neus Sabater , Valter Drazic and Patrick Perez}},
year = {2014}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm3.pdf:pdf},
number = {5},
pages = {1--5},
title = {{Kernel Square-Loss Examplar Machines Kernelized version}},
year = {2015}
}
@article{Sabater2015,
author = {Sabater, Neus and Mozhdeh, Seifi and Valter, Drazic and Sandri, Gustavo and P{\'{e}}rez, Patrick},
doi = {10.1007/978-3-319-16181-5},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/eccv{\_}workshop{\_}cameraready.pdf:pdf},
isbn = {9783319161815},
journal = {Computer Vision - ECCV 2014 Workshops},
keywords = {densification,edgels,edges,epipolar geometry,feature extraction,lucas-kanade,tracking},
pages = {548--560},
title = {{Accurate Disparity Estimation for Plenoptic Images}},
volume = {8926},
year = {2015}
}
@article{Ng2006,
abstract = {This dissertation introduces a new approach to everyday photography, which solves the longstanding problems related to focusing images accurately. The root of these problems ismissing information. It turns out that conventional photographs tell us rather little about the light passing through the lens. In particular, they do not record the amount of light traveling along individual rays that contribute to the image. They tell us only the sum total of light rays striking each point in the image. To make an analogy with a music-recording studio, taking a conventional photograph is like recording all themusicians playing together, rather than recording each instrument on a separate audio track. In this dissertation, we will go after themissing information. With micron-scale changes to its optics and sensor, we can enhance a conventional camera so that it measures the light along each individual ray flowing into the image sensor. In other words, the enhanced camera samples the total geometric distribution of light passing through the lens in a single exposure. The price we will pay is collecting much more data than a regular photograph. However, I hope to convince you that the price is a very fair one for a solution to a problem as pervasive and long-lived as photographic focus. In photography, as in recordingmusic, it iswise practice to save asmuch of the source data as you can. Of course simply recording the light rays in the camera is not a complete solution to the focus problem. The other ingredient is computation.The idea is to re-sort the recorded light rays to where they should ideally have terminated, to simulate the flow of rays through the virtual optics of an idealized camera into the pixels of an idealized output photograph.},
author = {Ng, Ren},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/renng-thesis.pdf:pdf},
isbn = {978-0-542-70779-7},
pages = {1--203},
title = {{Digital light field photography}},
url = {https://www.lytro.com/renng-thesis.pdf$\backslash$nhttp://testcis.cis.rit.edu/{~}cnspci/references/dip/light{\_}field{\_}photography/ng2006.pdf},
year = {2006}
}
@article{Arandjelovic2014,
abstract = {The goal of this work is a data structure to support approximate nearest neighbor search on very large scale sets of vector descriptors. The criteria we wish to optimize are: (i) that the memory footprint of the representation should be very small (so that it fits into main memory); and (ii) that the approximation of the original vectors should be accurate. We introduce a novel encoding method, named a Set Compression Tree (SCT), that satisfies these criteria. It is able to accurately compress 1 million descriptors using only a few bits per descriptor. The large compression rate is achieved by not compressing on a per-descriptor basis, but instead by compressing the set of descriptors jointly. We describe the encoding, decoding and use for nearest neighbor search, all of which are quite straightforward to implement. The method, tested on standard benchmarks (SIFT1M and 80 Million Tiny Images), achieves superior performance to a number of state-of-the-art approaches, including Product Quantization, Locality Sensitive Hashing, Spectral Hashing, and Iterative Quantization. For example, SCT has a lower error using 5 bits than any of the other approaches, even when they use 16 or more bits per descriptor. We also include a comparison of all the above methods on the standard benchmarks.},
author = {Arandjelovi{\'{c}}, R. and Zisserman, a.},
doi = {Doi 10.1109/Tpami.2014.2339821},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/arandjelovic14b.pdf:pdf},
isbn = {0162-8828},
journal = {Tpami},
number = {12},
pages = {2396--2406},
title = {{Extremely low bit-rate nearest neighbor search using a Set Compression Tree}},
volume = {36},
year = {2014}
}
@article{Medioni1996,
author = {Medioni, G},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1996352.pdf:pdf},
journal = {Proceeding of IAPR Workshop on Machine Vision Applications},
pages = {352--360},
title = {{Using computer vision in real applications: Two success stories}},
year = {1996}
}
@article{Arandjelovic,
author = {Arandjelovic, R and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/arandjelovic13a.pdf:pdf},
journal = {Robots.Ox.Ac.Uk},
title = {{Extremely low bit-rate nearest neighbor search using a Set Compression Tree}},
url = {http://www.robots.ox.ac.uk:5000/{~}vgg/publications/2013/arandjelovic13a/arandjelovic13a.pdf}
}
@inproceedings{Li2015,
author = {Li, Yao and Hengel, Anton Van Den},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Li{\_}Mid-Level{\_}Deep{\_}Pattern{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781467369640},
title = {{Mid-level Deep Pattern Mining}},
year = {2015}
}
@inproceedings{Kong2012,
abstract = {This paper propose a novel method to explicitly learn a common pattern pool (the commonality) and class-specific dictionaries (the particularity) for classification.},
author = {Kong, Shu and Wang, Donghui},
booktitle = {European Conference on Computer Vision},
doi = {10.1007/978-3-642-33718-5_14},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/draft{\_}ECCV12{\_}particularity.pdf:pdf},
isbn = {9783642337178},
issn = {03029743},
keywords = {Classification,Commonality,Dictionary Learning,Particularity},
title = {{A dictionary learning approach for classification: Separating the particularity and the commonality}},
url = {http://dx.doi.org/10.1007/978-3-642-33718-5{\_}14},
year = {2012}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - displacement ranks of matrices.pdf:pdf},
title = {displacement ranks of matrices}
}
@article{Sindhwani2015,
abstract = {We consider the task of building compact deep learning pipelines suitable for deployment on storage and power constrained mobile devices. We propose a unified framework to learn a broad family of structured parameter matrices that are characterized by the notion of low displacement rank. Our structured transforms admit fast function and gradient evaluation, and span a rich range of parameter sharing configurations whose statistical modeling capacity can be explicitly tuned along a continuum from structured to unstructured. Experimental results show that these transforms can significantly accelerate inference and forward/backward passes during training, and offer superior accuracy-compactness-speed tradeoffs in comparison to a number of existing techniques. In keyword spotting applications in mobile speech recognition, our methods are much more effective than standard linear low-rank bottleneck layers and nearly retain the performance of state of the art models, while providing more than 3.5-fold compression.},
archivePrefix = {arXiv},
arxivId = {1510.01722},
author = {Sindhwani, Vikas and Sainath, Tara N. and Kumar, Sanjiv},
eprint = {1510.01722},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1510.01722v1.pdf:pdf},
pages = {1--9},
title = {{Structured Transforms for Small-Footprint Deep Learning}},
url = {http://arxiv.org/abs/1510.01722},
year = {2015}
}
@article{Finley2008,
abstract = {The k-means clustering algorithm is one of the most widely used, effective, and best understood clustering methods. However, successful use of k-means requires a carefully chosen distance measure that reflects the properties of the clustering task. Since designing this distance measure by hand is often difficult, we provide methods for training k-means using supervised data. Given training data in the form of sets of items with their desired partitioning, we provide a structural SVM method that learns a distance measure so that k-means produces the desired clusterings 1. We propose two variants of the methods one based on a spectral relaxation and one based on the traditional k-means algorithm that are both computationally efficient. For each variant, we provide a theoretical characterization of its accuracy in solving the training problem. We also provide an empirical clustering quality and runtime analysis of these learning methods on varied high-dimensional datasets. 1.},
author = {Finley, Thomas and Joachims, Thorsten},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/supervised{\_}kmeans-08.pdf:pdf},
journal = {Learning},
keywords = {clus-,support vector machines,svm,training algorithms},
number = {1813-11621},
title = {{Supervised k-Means Clustering}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.168.1061},
year = {2008}
}
@article{Jiang2011,
author = {Jiang, Z and Lin, Z and Davis, Ls},
doi = {10.1109/TPAMI.2013.88},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/CVPR2011{\_}LCKSVD{\_}final.pdf:pdf},
isbn = {9781457703942},
issn = {01628828},
journal = {Computer Vision and Pattern  {\ldots}},
pmid = {24051726},
title = {{Learning a discriminative dictionary for sparse coding via label consistent K-SVD}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5995354},
year = {2011}
}
@article{Boiman2008,
author = {Boiman, Oren and Shechtman, Eli and Irani, Michal},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/258.pdf:pdf},
isbn = {9781424422432},
journal = {Ieee},
number = {i},
title = {{In Defense of Nearest-Neighbor Based Image Classi fi cation}},
year = {2008}
}
@article{Ding2004,
author = {Ding, Chris and He, Xiaofeng},
doi = {10.1145/967900.968021},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/10.1.1.77.4522.pdf:pdf},
isbn = {1581138121},
journal = {Proceedings of the 2004 ACM Symposium on Applied Computing},
pages = {584--589},
title = {{K-nearest-neighbor consistency in data clustering: Incorporating local information into global optimization}},
year = {2004}
}
@article{Parikh2014,
abstract = {Thismonograph is about a class of optimization algorithms called prox- imal algorithms.Much like Newton's method is a standard tool for solv- ing unconstrained smooth optimization problems of modest size, proxi- mal algorithms can be viewed as an analogous tool for nonsmooth, con- strained, large-scale, or distributed versions of these problems. They are very generally applicable, but are especially well-suited to problems of substantial recent interest involving large or high-dimensional datasets. Proximal methods sit at a higher level of abstraction than classical al- gorithms like Newton's method: the base operation is evaluating the proximal operator of a function, which itself involves solving a small convex optimization problem. These subproblems, which generalize the problem of projecting a point onto a convex set, often admit closed- form solutions or can be solved very quickly with standard or simple specialized methods. Here, we discuss the many different interpreta- tions of proximal operators and algorithms, describe their connections to many other topics in optimization and applied mathematics, survey some popular algorithms, and provide a large number of examples of proximal operators that commonly arise in practice.},
author = {Parikh, Neal and Boyd, Stephen},
doi = {10.1561/2400000003},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/prox{\_}algs.pdf:pdf},
issn = {2167-3888},
journal = {Foundations and Trends in Optimization},
number = {3},
pages = {123--231},
title = {{Proximal Algorithms}},
volume = {1},
year = {2014}
}
@article{Tomasev2011,
author = {Tomasev, N and Brehar, Raluca and Mladenic, Dunja and Nedevschi, Sergiu},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/imageHubness.pdf:pdf},
isbn = {1457714795},
journal = {Intelligent Computer Communication and Processing (ICCP), 2011 IEEE International Conference on},
pages = {367--374},
title = {{The influence of hubness on nearest-neighbor methods in object recognition}},
year = {2011}
}
@article{Tomasev2011a,
abstract = {High-dimensional data arise naturally in many domains, and have regularly presented a great challenge for traditional data-mining techniques, both in terms of effectiveness and efficiency. Clustering becomes difficult due to the increasing sparsity of such data, as well as the increasing difficulty in distinguishing distances between data points. In this paper we take a novel perspective on the problem of clustering high-dimensional data. Instead of attempting to avoid the curse of dimensionality by observing a lower-dimensional feature subspace, we embrace dimensionality by taking advantage of some inherently high-dimensional phenomena. More specifically, we show that hubness, i.e., the tendency of high-dimensional data to contain points (hubs) that frequently occur in k-nearest neighbor lists of other points, can be successfully exploited in clustering. We validate our hypothesis by proposing several hubness-based clustering algorithms and testing them on high-dimensional data. Experimental results demonstrate good performance of our algorithms in multiple settings, particularly in the presence of large quantities of noise.},
author = {Toma{\v{s}}ev, Nenad and Radovanovi{\'{c}}, Milo{\v{s}} and Mladeni{\'{c}}, Dunja and Ivanovi{\'{c}}, Mirjana},
doi = {10.1007/978-3-642-20841-6-16},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2011-pakdd-khubs.pdf:pdf},
isbn = {9783642208409},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {183--195},
pmid = {1000172858},
title = {{The role of hubness in clustering high-dimensional data}},
volume = {6634 LNAI},
year = {2011}
}
@article{Kalantidis2014a,
abstract = {We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate nearest neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to provide non-exhaustive search, i.e., inverted lists or a multi-index, the idea is to locally optimize an individual product quantizer (PQ) per cell and use it to encode residuals. Local optimization is over rotation and space decomposition, interestingly, we apply a parametric solution that assumes a normal distribution and is extremely fast to train. With a reasonable space and time overhead that is constant in the data size, we set a new state-of-the-art on several public datasets, including a billion-scale one.},
author = {Kalantidis, Yannis and Avrithis, Yannis},
doi = {10.1109/CVPR.2014.298},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/lopq.pdf:pdf},
isbn = {978-1-4799-5118-5},
issn = {10636919},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {Artificial neural networks,Eigenvalues and eigenfunctions,Encoding,Indexes,Optimization,Quantization (signal),SIFT1B,Vectors,approximate nearest neighbor search,locally optimized product quantization,multi-index,product quantization},
number = {c},
pages = {2329--2336},
title = {{Locally Optimized Product Quantization for Approximate Nearest Neighbor Search}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909695},
volume = {1},
year = {2014}
}
@article{Shalev-Shwartz2011a,
abstract = {Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the cor- rect answer to previous prediction tasks and possibly additional avail- able information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emer- gence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.},
author = {Shalev-Shwartz, Shai},
doi = {10.1561/2200000018},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/OLsurvey.pdf:pdf},
isbn = {9781601985460},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
number = {2},
pages = {107--194},
title = {{Online Learning and Online Convex Optimization}},
volume = {4},
year = {2011}
}
@article{Shalev-Shwartz2007,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
doi = {10.1145/1273496.1273598},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/PegasosMPB.pdf:pdf},
isbn = {9781595937933},
journal = {International Conference on Machine Learning},
keywords = {2000,first,mathematics subject classification,more,second,stochastic gradient descent,svm},
pages = {807--814},
title = {{Pegasos: Primal Estimated sub-GrAdient SOlver for SVM}},
url = {http://portal.acm.org/citation.cfm?doid=1273496.1273598},
year = {2007}
}
@article{Uijlings2013,
abstract = {For object recognition, the current state-of-the-art is based on exhaustive search. However, to enable the use of more expensive features and classifiers and thereby progress beyond the state-of-the-art, a selective search strategy is needed. Therefore, we adapt segmentation as a selective search by reconsidering segmentation: We propose to generate many approximate locations over few and precise object delineations because (1) an object whose location is never generated can not be recognised and (2) appearance and immediate nearby context are most effective for object recognition. Our method is class-independent and is shown to cover 96.7{\%} of all objects in the Pascal VOC 2007 test set using only 1,536 locations per image. Our selective search enables the use of the more expensive bag-of-words method which we use to substantially improve the state-of-the-art by up to 8.5{\%} for 8 out of 20 classes on the Pascal VOC 2010 detection challenge.},
author = {Uijlings, J. R R and van de Sande, K. E. a. and Gevers, T. and Smeulders, a. W M},
doi = {10.1007/s11263-013-0620-5},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/uijlings-ijcv2013-draft.pdf:pdf},
isbn = {9781457711015},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
number = {2},
pages = {154--171},
title = {{Selective Search for Object Recognition}},
url = {http://link.springer.com/10.1007/s11263-013-0620-5},
volume = {104},
year = {2013}
}
@article{Crammer2001,
abstract = {In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy},
author = {Crammer, Koby and Singer, Yoram},
doi = {10.1162/15324430260185628},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/crammer01a.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {kernel machines,multiclass problems,svm},
pages = {265--292},
title = {{On the Algorithmic Implementation of Multiclass Kernel-based Vector Machines}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/CrammerS01.pdf$\backslash$nhttp://www.jmlr.org/papers/volume2/crammer01a/crammer01a.pdf},
volume = {2},
year = {2001}
}
@inproceedings{Joachims2005,
abstract = {This paper presents a Support Vector Method for optimizing multivariate nonlinear performance measures like the F1-score. Taking a multivariate prediction approach, we give an algorithm with which such multivariate SVMs can be trained in polynomial time for large classes of potentially non-linear performance measures, in particular ROCArea and all measures that can be computed from the contingency table. The conventional classification SVM arises as a special case of our method.},
author = {Joachims, Thorsten},
booktitle = {International Conference on Machine Learning},
doi = {10.1145/1102351.1102399},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/joachims{\_}05a.pdf:pdf},
isbn = {1595931805},
pages = {377--384},
pmid = {16525473},
title = {{A Support Vector Method for Multivariate Performance Measures}},
url = {http://portal.acm.org/citation.cfm?doid=1102351.1102399},
volume = {440},
year = {2005}
}
@article{Niu2011,
abstract = {We consider the problem of learning a distance metric from a limited amount of pairwise information as effectively as possible. The proposed SERAPH (SEmi-supervised metRic leArning Paradigm with Hyper sparsity) is a direct and substantially more natural approach for semi-supervised metric learning, since the supervised and unsupervised parts are based on a unified information theoretic framework. Unlike other extensions, the unsupervised part of SERAPH can extract further pairwise information from the unlabeled data according to temporary results of the supervised part, and therefore interacts with the supervised part positively. SERAPH involves both the sparsity of posterior distributions over the unobserved weak labels and the sparsity of the induced projection matrices, which we call the hyper sparsity. The resulting optimization is solved by an EM-like scheme, where the M-Step is convex, and the E-Step has analytical solution. Experimental results show that SERAPH compares favorably with existing metric learning algorithms based on weak labels.},
archivePrefix = {arXiv},
arxivId = {1105.0167},
author = {Niu, Gang and Dai, Bo and Yamada, Makoto and Sugiyama, Masashi},
eprint = {1105.0167},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/seraph.pdf:pdf},
number = {September 2015},
title = {{SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity}},
url = {http://arxiv.org/abs/1105.0167},
year = {2011}
}
@article{Hoi2006,
abstract = {Relevant Component Analysis (RCA) has been proposed for learning distance metrics with contextual constraints for image retrieval. However, RCA has two important disadvantages. One is the lack of exploiting negative constraints which can also be informative, and the other is its incapability of capturing complex nonlinear relationships between data instances with the contextual information. In this paper, we propose two algorithms to overcome these two disadvantages, i.e., Discriminative Component Analysis (DCA) and Kernel DCA. Compared with other complicated methods for distance metric learning, our algorithms are rather simple to understand and very easy to solve. We evaluate the performance of our algorithms on image retrieval in which experimental results show that our algorithms are effective and promising in learning good quality distance metrics for image retrieval.},
author = {Hoi, Steven C H and Liu, Wei and Lyu, Michael R. and Wei-Ying, Ma},
doi = {10.1109/CVPR.2006.167},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/dca.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Computer Vision and Pattern Recognition},
number = {c},
pages = {2072--2078},
title = {{Learning distance metrics with contextual constraints for image retrieval}},
volume = {2},
year = {2006}
}
@article{Tsuruoka2009,
abstract = {Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion. This learning framework is attractive because it often requires much less training time in practice than batch training algorithms. However, L1-regularization, which is be- coming popular in natural language pro- cessing because of its ability to pro- duce compact models, cannot be effi- ciently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gra- dients. We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty. We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging. Experimental re- sults demonstrate that our method can pro- duce compact and accurate models much more quickly than a state-of-the-art quasi- Newton method for L1-regularized log- linear models. 1},
author = {Tsuruoka, Yoshimasa and Tsujii, Jun'ichi and Ananiadou, Sophia},
doi = {10.3115/1687878.1687946},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/P09-1054.pdf:pdf},
isbn = {9781932432459},
journal = {Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP},
pages = {477},
title = {{Stochastic gradient descent training for L1-regularized log-linear models with cumulative penalty}},
url = {http://portal.acm.org/citation.cfm?doid=1687878.1687946},
volume = {1},
year = {2009}
}
@article{Shalev-Shwartz2011,
abstract = {We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems,
such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while
the second method updates the entire weight vector but only uses a single training example at
each iteration. In both methods, the choice of feature or example is uniformly at random. Our
theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art
deterministic approaches, including their deterministic counterparts, when the size of the problem
is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and
natural data sets.},
author = {Shalev-Shwartz, Shai and Tewari, Ambuj},
doi = {10.1145/1553374.1553493},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/shalev-shwartz11a.pdf:pdf},
isbn = {9781605585161},
issn = {1532-4435},
keywords = {Theory {\&} Algorithms},
pages = {1865--1892},
title = {{Stochastic Methods for l1-regularized Loss Minimization}},
url = {http://eprints.pascal-network.org/archive/00008908/},
volume = {12},
year = {2011}
}
@article{Joachims2002,
abstract = {This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.},
author = {Joachims, Thorsten},
doi = {10.1145/775047.775067},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/joachims{\_}02c.pdf:pdf},
isbn = {158113567X},
issn = {10468188},
journal = {Kdd '02},
keywords = {analysis,information,learning,log,rank,retrieval},
pages = {133--142},
pmid = {21474660},
title = {{Optimizing search engines using clickthrough data}},
url = {http://www.cs.cornell.edu/People/tj/publications/joachims{\_}02c.pdf},
year = {2002}
}
@inproceedings{Russakovsky,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.0575v3},
author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Jan, C. V. and Krause, J. and Ma, S.},
booktitle = {arXiv},
eprint = {arXiv:1409.0575v3},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1409.0575v3.pdf:pdf},
keywords = {benchmark,dataset,large-scale,object detection,object recognition},
title = {{ImageNet Large Scale Visual Recognition Challenge}},
url = {http://image-net.org/challenges/LSVRC/2015/},
year = {2014}
}
@article{Weinberger2009,
abstract = {The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.},
author = {Weinberger, Kilian Q and Saul, Lawrence K},
doi = {10.1126/science.277.5323.215},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/weinberger09a.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Metric learning,convex optimization,ing,mahalanobis distance,metric learn-,multi-class classification,semi-definite programming,support vector machines},
pages = {207--244},
pmid = {17490632},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
volume = {10},
year = {2009}
}
@article{Nesterov2009,
abstract = {In this paper we present a new approach for constructing subgradient schemesfordifferent types ofnonsmoothproblems withconvexstructure.Ourmethods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexi- bility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform bounded- ness of subgradients).We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequali- ties, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.},
author = {Nesterov, Yurii},
doi = {10.1007/s10107-007-0149-x},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/GS-Nesterov Primal-dual.pdf:pdf},
issn = {00255610},
journal = {Mathematical Programming},
keywords = {Black-box methods,Convex optimization,Lower complexity bounds,Minimax problems,Non-smooth optimization,Saddle points,Stochastic optimization,Subgradient methods,Variational inequalities},
number = {1 SPEC. ISS.},
pages = {221--259},
title = {{Primal-dual subgradient methods for convex problems}},
volume = {120},
year = {2009}
}
@article{Mcfee2010,
abstract = {We study metric learning as a problem of information retrieval. We present a gen- eral metric learning algorithm, based on the structural SVM framework, to learn a metric such that rankings of data induced by dis- tance from a query can be optimized against various ranking measures, such as AUC, Precision-at-k, MRR, MAP or NDCG. We demonstrate experimental results on stan- dard classification data sets, and a large-scale online dating recommendation problem.},
author = {Mcfee, Brian and Lanckriet, Gert},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/mlr.pdf:pdf},
journal = {International Conference of Machine Learning},
pages = {775--782},
title = {{Metric Learning to Rank}},
year = {2010}
}
@article{Kivinen2004,
abstract = { Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.},
author = {Kivinen, Jyrki and Smola, Alexander J. and Williamson, Robert C.},
doi = {10.1109/TSP.2004.830991},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/P172.pdf:pdf},
isbn = {1049-5258},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {8},
pages = {2165--2176},
title = {{Online learning with kernels}},
volume = {52},
year = {2004}
}
@article{Engel2005,
abstract = {Gaussian Process Temporal Difference
(GPTD) learning offers a Bayesian solution
to the policy evaluation problem of reinforcement
learning. In this paper we extend the
GPTD framework by addressing two pressing
issues, which were not adequately treated
in the original GPTD paper (Engel et al.,
2003). The first is the issue of stochasticity
in the state transitions, and the second is
concerned with action selection and policy
improvement. We present a new generative
model for the value function, deduced from
its relation with the discounted return. We
derive a corresponding on-line algorithm
for learning the posterior moments of the
value Gaussian process. We also present a
SARSA based extension of GPTD, termed
GPSARSA, that allows the selection of
actions and the gradual improvement of
policies without requiring a world-model.},
author = {Engel, Yaakov and Mannor, Shie and Meir, Ron},
doi = {10.1145/1102351.1102377},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/W-gpKernel{\_}icml05.pdf:pdf},
isbn = {1-59593-180-5},
keywords = {Computational, Information-Theoretic Learning with,Learning/Statistics {\&} Optimisation},
title = {{Reinforcement learning with Gaussian processes}},
url = {http://eprints.pascal-network.org/archive/00000895/},
year = {2005}
}
@article{Gharbi2012,
author = {Gharbi, Michael and Malisiewicz, Tomasz and Durand, Fr{\o}do},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/gharbi{\_}techreport{\_}2012.pdf:pdf},
journal = {MIT CSAIL Technical Report},
title = {{A Gaussian Approximation of Feature Space for Fast Image Similarity}},
year = {2012}
}
@article{Tang,
author = {Tang, Tao-yen and Huang, Tzu-wei and Chen, Hwann-tzong},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/reh.pdf:pdf},
title = {{Random Exemplar Hashing}}
}
@article{Jain2013,
author = {Jain, Prateek and Thakurta, Abhradeep},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/kernel.pdf:pdf},
journal = {International Conference on Machine Learning},
pages = {1--9},
title = {{Differentially Private Learning with Kernels}},
url = {papers2://publication/uuid/EDFB3943-A5F4-49C1-8CCF-05501DFBC873},
volume = {28},
year = {2013}
}
@article{Babenko,
author = {Babenko, Artem and Lempitsky, Victor},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Babenko{\_}Tree{\_}Quantization{\_}for{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{Tree Quantization for Large-Scale Similarity Search and Classification}}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm3.pdf:pdf},
number = {5},
pages = {1--5},
title = {{Kernel Square-Loss Examplar Machines Kernelized version}},
year = {2015}
}
@article{Scholkopf2001,
abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
author = {Sch{\"{o}}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J},
doi = {10.1007/3-540-44581-1_27},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/SchHerSmo01.pdf:pdf},
isbn = {3540423435},
issn = {03029743},
journal = {Most},
number = {2000 - 81},
pages = {416--426},
title = {{A Generalized Representer Theorem}},
url = {http://www.springerlink.com/index/V1TVBA62HD4837H9.pdf},
volume = {2111},
year = {2001}
}
@article{Ponce2015,
author = {Ponce, Jean},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm2.pdf:pdf},
number = {5},
pages = {1--5},
title = {{Kernel Square-Loss Examplar Machines Kernelized version}},
year = {2015}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm.pdf:pdf},
number = {8},
pages = {2--5},
title = {{Exemplar SVMs with a square loss LDA and square-loss linear classifiers Kernelized version}},
volume = {1}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm3.pdf:pdf},
number = {5},
pages = {1--5},
title = {{Kernel Square-Loss Examplar Machines Kernelized version}},
year = {2015}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm.pdf:pdf},
number = {8},
pages = {2--5},
title = {{Exemplar SVMs with a square loss LDA and square-loss linear classifiers Kernelized version}},
volume = {1}
}
@article{Cortes2011,
author = {Cortes, Corinna and Mohri, Mehryar and Rostami, Afshin},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cortes, Mohri, Rostami - 2011 - Learning Kernels -Tutorial.pdf:pdf},
journal = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
title = {{Learning Kernels -Tutorial}},
year = {2011}
}
@article{Jakel2007,
abstract = {The abilities to learn and to categorize are fundamental for cognitive systems, be it animals or machines, and therefore have attracted attention from engineers and psychologists alike. Modern machine learning methods and psychological models of categorization are remarkably similar, partly because these two fields share a common history in artificial neural networks and reinforcement learning. However, machine learning is now an independent and mature field that has moved beyond psychologically or neurally inspired algorithms towards providing foundations for a theory of learning that is rooted in statistics and functional analysis. Much of this research is potentially interesting for psychological theories of learning and categorization but also hardly accessible for psychologists. Here, we provide a tutorial introduction to a popular class of machine learning tools, called kernel methods. These methods are closely related to perceptrons, radial-basis-function neural networks and exemplar theories of categorization. Recent theoretical advances in machine learning are closely tied to the idea that the similarity of patterns can be encapsulated in a positive definite kernel. Such a positive definite kernel can define a reproducing kernel Hilbert space which allows one to use powerful tools from functional analysis for the analysis of learning algorithms. We give basic explanations of some key concepts-the so-called kernel trick, the representer theorem and regularization-which may open up the possibility that insights from machine learning can feed back into psychology. {\textcopyright} 2007 Elsevier Inc. All rights reserved.},
author = {J{\"{a}}kel, Frank and Sch{\"{o}}lkopf, Bernhard and Wichmann, Felix a.},
doi = {10.1016/j.jmp.2007.06.002},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/J{\"{a}}kel, Sch{\"{o}}lkopf, Wichmann - 2007 - A tutorial on kernel methods for categorization.pdf:pdf},
issn = {00222496},
journal = {Journal of Mathematical Psychology},
keywords = {Categorization,Generalization,Kernel,Machine learning,Similarity},
number = {6},
pages = {343--358},
title = {{A tutorial on kernel methods for categorization}},
volume = {51},
year = {2007}
}
@article{Hofmann2008,
archivePrefix = {arXiv},
arxivId = {arXiv:math/0701907v3},
author = {Hofmann, Thomas and Scholkopf, Bernhard and Smola, Alexander J.},
doi = {10.1214/009053607000000677},
eprint = {0701907v3},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hofmann, Sch, Smola - 2008 - KERNEL METHODS IN MACHINE LEARNING 1 ¨ lkopf By Thomas Hofmann, Bernhard Sch o and Alexander J. Smola.pdf:pdf},
journal = {The Annals of Statistics},
keywords = {and phrases,chines,graphical models,machine learning,reproducing kernels,support vector ma-},
number = {3},
pages = {1171--1220},
primaryClass = {arXiv:math},
title = {{Kernel Methods in Machine Learning}},
volume = {36},
year = {2008}
}
@article{Saunders1998,
abstract = {In this paper we study a dual version of the Ridge Regression procedure. It allows us to perform non-linear regression by construct- ing a linear regression function in a high di- mensional feature space. The feature space representation can result in a large increase in the number of parameters used by the al- gorithm. In order to combat this “curse of dimensionality”, the algorithm allows the use of kernel functions, as used in Support Vector methods. We also discuss a powerful family of kernel functions which is constructed using the ANOVA decomposition method from the kernel corresponding to splines with an infi- nite number of nodes. This paper introduces a regression estimation algorithm which is a combination of these two elements: the dual version of Ridge Regression is applied to the ANOVA enhancement of the infinite- node splines. Experimental results are then presented (based on the Boston Housing data set) which indicate the performance of this algorithm relative to other algorithms.},
author = {Saunders, C. and Gammerman, a. and Vovk, V.},
doi = {10.1111/j.0033-0124.1985.00197.x},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saunders, Gammerman, Vovk - 1998 - Ridge Regression Learning Algorithm in Dual Variables.pdf:pdf},
isbn = {1-55860-556-8},
issn = {0033-0124},
title = {{Ridge Regression Learning Algorithm in Dual Variables}},
url = {http://eprints.soton.ac.uk/258942/},
year = {1998}
}
@inproceedings{Zhang2015,
author = {Zhang, Ting and Qi, Guo-Jun and Wang, Jingdong},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Zhang{\_}Sparse{\_}Composite{\_}Quantization{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
pages = {4548--4556},
title = {{Sparse Composite Quantization}},
year = {2015}
}
@article{Ponce,
author = {Ponce, Jean},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvm.pdf:pdf},
number = {8},
pages = {2--5},
title = {{Exemplar SVMs with a square loss LDA and square-loss linear classifiers Kernelized version}},
volume = {1}
}
@article{Norouzi2012,
author = {Norouzi, Mohammad and Fleet, David J Dj David J and Salakhutdinov, Ruslan and Blei, David M.},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/4808-hamming-distance-metric-learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {1--9},
title = {{Hamming distance metric learning}},
url = {https://papers.nips.cc/paper/4808-hamming-distance-metric-learning.pdf},
year = {2012}
}
@article{Li2013,
abstract = {A key problem in visual tracking is to represent the appearance of an $\backslash$nobject in a way that is robust to visual changes. To attain this robustness, $\backslash$nincreasingly complex models are used to capture appearance variations. However, $\backslash$nsuch models can be difficult to maintain accurately and efficiently. In this $\backslash$npaper, we propose a visual tracker in which objects are represented by compact $\backslash$nand discriminative binary codes. This representation can be processed very $\backslash$nefficiently, and is capable of effectively fusing information from multiple $\backslash$ncues. An incremental discriminative learner is then used to construct an $\backslash$nappearance model that optimally separates the object from its surrounds. $\backslash$nFurthermore, we design a hyper graph propagation method to capture the $\backslash$ncontextual information on samples, which further improves the tracking accuracy. $\backslash$nExperimental results on challenging videos demonstrate the effectiveness and $\backslash$nrobustness of the proposed tracker.},
author = {Li, Xi and Shen, Chunhua and Dick, Anthony and {Van Den Hengel}, Anton},
doi = {10.1109/CVPR.2013.313},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/hashingTracking.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {SVM,Visual Tracking,appearance model,hashing,hypergraph},
pages = {2419--2426},
title = {{Learning compact binary codes for visual tracking}},
year = {2013}
}
@article{Norouzi2011,
abstract = {Abstract  In annual grasslands that experience a mediterranean-type climate, the synchrony between plant senescence and peak solar radiation over summer results in high litter sun exposure. We examined the decomposition of both shaded and sun-exposed litter over summer and inferred the effects of photodegradation from changes in mass loss and litter chemistry. The carry-over effects of summer litter exposure on wet season decomposition were also assessed, and the attenuation of photodegradation with litter layer thickness was used to estimate the proportion of grass litter lignin susceptible to photodegradation under different treatments of a factorial global change experiment. Over summer, mass loss from grass and forb litter exposed to ambient sunlight ranged from 8{\%} to 10{\%}, whereas lignin decreased in grass litter by approximately 20{\%}. After one year of decomposition, mass losses from grass leaves exposed to sunlight over summer were more than double the mass losses from summer-shaded leaves. When shade litter layer thickness was varied, mass losses over summer for all treatments were also approximately 8{\%}; however, lignin decreased significantly only in the low shade treatments (064 g m2 of shade litter). Aboveground production of annual grasses nearly quadrupled in response to the combined effects of N addition, elevated atmospheric CO2, increased precipitation and warming. The estimated proportion of grass litter lignin experiencing full photodegradation ranged from 100{\%} under ambient conditions to 3162{\%} in plots receiving the combined global change treatments. These results reveal an important role of sun exposure over summer in accelerating litter decomposition in these grasslands and provide evidence that future changes in the quantity of litter deposition may modulate the influence of photodegradation integrated across the litter layer.},
author = {Norouzi, Mohammad and Blei, Dm and Fleet, David},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/min{\_}loss{\_}hashing.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning (ICML-11)},
pages = {353--360},
title = {{Minimal Loss Hashing for Compact Binary Codes}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/ICML2011Norouzi{\_}246.pdf},
year = {2011}
}
@article{Punjani,
author = {Punjani, Ali and Fleet, David J},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/multi{\_}index{\_}hashing.pdf:pdf},
title = {{Fast Search in Hamming Space with Multi-Index Hashing Mohammad Norouzi}}
}
@article{Torralba2008,
abstract = {The Internet contains billions of images, freely available online. Methods for efficiently searching this incredibly rich resource are vital for a large number of applications. These include object recognition, computer graphics, personal photo collections, online image search tools. In this paper, our goal is to develop efficient image search and scene matching techniques that are not only fast, but also require very little memory, enabling their use on standard hardware or even on handheld devices. Our approach uses recently developed machine learning techniques to convert the Gist descriptor (a real valued vector that describes orientation energies at different scales and orientations within an image) to a compact binary code, with a few hundred bits per image. Using our scheme, it is possible to perform real-time searches with millions from the Internet using a single large PC and obtain recognition results comparable to the full descriptor. Using our codes on high quality labeled images from the LabelMe database gives surprisingly powerful recognition results using simple nearest neighbor techniques.},
author = {Torralba, Antonio and Fergus, Rob and Weiss, Yair},
doi = {10.1109/CVPR.2008.4587633},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cvpr2008.pdf:pdf},
isbn = {9781424422432},
issn = {1063-6919},
journal = {26th IEEE Conference on Computer Vision and Pattern Recognition, CVPR},
title = {{Small codes and large image databases for recognition}},
year = {2008}
}
@article{Lin2015,
author = {Lin, Kevin and Yang, Huei-fang and Hsiao, Jen-hao and Chen, Chu-song},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Lin{\_}Deep{\_}Learning{\_}of{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
journal = {Cvpr Ws},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
title = {{Deep Learning of Binary Hash Codes for Fast Image Retrieval}},
year = {2015}
}
@article{Miguel2015,
author = {Miguel, a and Raziperchikolaei, R},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Carreira-Perpinan{\_}Hashing{\_}With{\_}Binary{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
journal = {Cvpr},
keywords = {Himalaya},
mendeley-tags = {Himalaya},
number = {section 4},
title = {{Hashing with Binary Autoencoders}},
year = {2015}
}
@article{Chuang2015,
author = {Chuang, Ming and Sweeney, Pat and Gillett, Don and Evseev, Dennis and Hoppe, Hugues and Kirk, Adam and Sullivan, Steve and Calabrese, David},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/069-0311.done.pdf:pdf},
isbn = {9781450333313},
keywords = {3d video,and accessible form of,geometry compression,media,mesh tracking,mpeg,multi-view stereo,multi-view stereo, surface reconstruction, 3D vide,our goal is to,research pro-,several system,surface reconstruction,totype into a rich,transform free-viewpoint video from},
number = {4},
title = {{High-Quality Streamable Free-Viewpoint Video}},
volume = {34},
year = {2015}
}
@article{Nowozin2011,
author = {Nowozin, Sebastian and Lampert, Christoph H},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/talk-ssvm.pdf:pdf},
number = {June},
pages = {1--56},
title = {{Part 5 : Structured Support Vector Machines}},
url = {http://www.nowozin.net/sebastian/cvpr2011tutorial/slides/talk-ssvm.pdf},
year = {2011}
}
@article{Vandenberghe2009,
author = {Vandenberghe, Lieven},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/vandenberghe{\_}1{\_}2.pdf:pdf},
pages = {1--108},
title = {{Convex Optimization}},
url = {http://www.robots.ox.ac.uk/{~}az/lectures/b1/vandenberghe{\_}1{\_}2.pdf},
year = {2009}
}
@article{Boyce,
author = {Boyce, Jill M and Member, Senior and Ye, Yan and Member, Senior and Chen, Jianle and Ramasubramonian, Adarsh K},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/9385{\_}SHVC.pdf:pdf},
title = {{Overview of SHVC : Scalable Extensions of the High Efficiency Video Coding ( HEVC ) Standard}}
}
@article{Xu2014,
author = {Xu, Jizheng and Member, Senior and Joshi, Rajan and Cohen, Robert A and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/9556{\_}screenContentCoding{\_}HEVC.pdf:pdf},
number = {March},
title = {{Overview of the Emerging HEVC Screen Content}},
year = {2014}
}
@article{Kwon2012,
author = {Kwon, Younghee and Kim, Kwang In and Tompkin, James and Kim, Jin Hyung and Theobalt, Christian},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/efficientlearningforimageenhancement{\_}pami2014.pdf:pdf},
pages = {1--14},
title = {{Efficient Learning of Image Super-resolution and Compression Artifact Removal with Semi-local Gaussian Processes}},
year = {2012}
}
@inproceedings{Zepeda2015a,
author = {Zepeda, Joaquin and Mehmet, Turkan and Thoreau, Dominique},
booktitle = {European Signal Processing Conference},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Approx Template Matching.pdf:pdf},
title = {{Block Prediction Using Approximate Template Matching}},
year = {2015}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/trebst.pdf:pdf},
title = {{Greedy Function Approximation: A Gradient Boosting Machine}}
}
@article{Zepedaa,
author = {Zepeda, Joaquin and Mehmet, T and Thoreau, Dominique},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Approx Template Matching.pdf:pdf},
title = {{BLOCK PREDICTION USING APPROXIMATE TEMPLATE MATCHING Technicolor , 975 Avenue des Champs Blancs , Cesson S ´ evign ´ e , France Izmir University of Economics , Izmir , Turkey}}
}
@article{Martinez2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.2173v1},
author = {Martinez, Julieta and Hoos, Holger H and Little, James J and Nov, C V},
eprint = {arXiv:1411.2173v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1411.2173v1.pdf:pdf},
journal = {arXiv preprint arXiv:1411.2173},
title = {{Stacked Quantizers for Compositional Vector Compression}},
year = {2015}
}
@article{Tariq2013,
author = {Tariq, Usman and Yang, Jianchao and Huang, Thomas S.},
doi = {10.1109/FG.2013.6553794},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/FG2013.pdf:pdf},
isbn = {9781467355452},
journal = {2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2013},
title = {{Maximum margin GMM learning for facial expression recognition}},
year = {2013}
}
@article{Hariharana,
author = {Hariharan, Bharath and Arbel, Pablo and Girshick, Ross},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Hariharan{\_}Hypercolumns{\_}for{\_}Object{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
title = {{Hypercolumns for Object Segmentation and Fine-grained Localization}}
}
@article{Iccv2015,
author = {Iccv, Anonymous and Id, Paper},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Paper 1548.pdf:pdf},
title = {{No spare parts : Sharing part detectors for image categorization}},
year = {2015}
}
@article{Sicre,
author = {Sicre, Ronan},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/cviu-parts.pdf:pdf},
keywords = {computer vision,image classification,part-based,preprint submitted to computer,vision and image understanding,visual recognition},
title = {{Discriminative part model for visual recognition}}
}
@article{Arora2013,
abstract = {We give algorithms with provable guarantees that learn a class of deep nets in the generative model view popularized by Hinton and others. Our generative model is an {\$}n{\$} node multilayer neural net that has degree at most {\$}n{\^{}}{\{}\backslashgamma{\}}{\$} for some {\$}\backslashgamma {\textless}1{\$} and each edge has a random edge weight in {\$}[-1,1]{\$}. Our algorithm learns {\{}$\backslash$em almost all{\}} networks in this class with polynomial running time. The sample complexity is quadratic or cubic depending upon the details of the model. The algorithm uses layerwise learning. It is based upon a novel idea of observing correlations among features and using these to infer the underlying edge structure via a global graph recovery procedure. The analysis of the algorithm reveals interesting structure of neural networks with random edge weights.},
archivePrefix = {arXiv},
arxivId = {1310.6343},
author = {Arora, Sanjeev and Bhaskara, Aditya and Ge, Rong and Ma, Tengyu},
eprint = {1310.6343},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1310.6343v1.pdf:pdf},
isbn = {9781634393973},
journal = {arXiv preprint arXiv:1310.6343},
pages = {18},
title = {{Provable Bounds for Learning Some Deep Representations}},
url = {http://arxiv.org/abs/1310.6343},
year = {2013}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
eprint = {1312.4400},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1312.4400v3.pdf:pdf},
journal = {arXiv preprint},
pages = {10},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@inproceedings{Vedaldi2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6598v2},
author = {{Naderi Parizi}, Sobhan and Vedaldi, Andrea and Zisserman, Andrew and Felzenszwalb, Pedro},
booktitle = {International Conference on Learning and Recognition},
eprint = {arXiv:1412.6598v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1412.6598.pdf:pdf},
title = {{Automatic Discovery and Optimization of Parts for Image Classification}},
year = {2015}
}
@article{Sicre2014,
author = {Sicre, Ronan},
doi = {10.1109/ICPR.2014.345},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/14{\_}icpr{\_}mid{\_}level.pdf:pdf},
isbn = {978-1-4799-5209-0},
journal = {In ICPR},
title = {{Discovering and Aligning Discriminative Mid-Level Features for Image Classification}},
year = {2014}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni et al. - Unknown - Learning the Structure of Deep Architectures via {\$}ell{\_}1{\$} Penalization.pdf:pdf},
title = {{Learning the Structure of Deep Architectures Using 1 Regularization}},
year = {2015}
}
@article{Szegedy2014a,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Szegedy{\_}Going{\_}Deeper{\_}With{\_}2015{\_}CVPR{\_}paper.pdf:pdf},
title = {{Going Deeper with Convolutions}},
year = {2014}
}
@article{Babenko2014,
author = {Babenko, Artem and Lempitsky, Victor},
doi = {10.1109/CVPR.2014.124},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2014{\_}cvpr{\_}Babenko{\_}Additive{\_}Quantization.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
pages = {931--938},
title = {{Additive Quantization for Extreme Vector Compression}},
year = {2014}
}
@article{Cherian2014b,
author = {Cherian, Anoop},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2014{\_}icml{\_}sparse{\_}ann.pdf:pdf},
isbn = {9781634393973},
keywords = {sparse coding, incoherent dictionary learning, nea},
title = {{Nearest Neighbors Using Compact Sparse Codes}},
volume = {32},
year = {2014}
}
@article{Norouzi2013,
abstract = {A fundamental limitation of quantization techniques like the k-means $\backslash$nclustering algorithm is the storage and run-time cost associated with the large $\backslash$nnumbers of clusters required to keep quantization errors small and model $\backslash$nfidelity high. We develop new models with a compositional parameterization of $\backslash$ncluster centers, so representational capacity increases super-linearly in the $\backslash$nnumber of parameters. This allows one to effectively quantize data using $\backslash$nbillions or trillions of centers. We formulate two such models, Orthogonal $\backslash$nk-means and Cartesian k-means. They are closely related to one another, to $\backslash$nk-means, to methods for binary hash function optimization like ITQ (Gong and $\backslash$nLazebnik, 2011), and to Product Quantization for vector quantization (Jegou et $\backslash$nal., 2011). The models are tested on large-scale ANN retrieval tasks (1M GIST, $\backslash$n1B SIFT features), and on codebook learning for object recognition (CIFAR-10).},
author = {Norouzi, Mohammad and Fleet, David J.},
doi = {10.1109/CVPR.2013.388},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2013{\_}Norouzi{\_}Cartesian{\_}K-Means{\_}CVPR{\_}paper.pdf:pdf},
isbn = {978-0-7695-4989-7},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer  Vision and Pattern Recognition},
keywords = {approximate nearest neighbor search,bag of words,cartesian,codebook,euclidean nearest neighbor search,hashing,large-scale,learning,nearest neighbor search,product quantization,quantization,retrieval},
pages = {3017--3024},
title = {{Cartesian k-means}},
year = {2013}
}
@article{Kalantidis2014,
abstract = {We present a simple vector quantizer that combines low distortion with fast search and apply it to approximate near- est neighbor (ANN) search in high dimensional spaces. Leveraging the very same data structure that is used to pro- vide non-exhaustive search, i.e., inverted lists or a multi- index, the idea is to locally optimize an individual product quantizer (PQ) per cell and use it to encode residuals. Lo- cal optimization is over rotation and space decomposition; interestingly, we apply a parametric solution that assumes a normal distribution and is extremely fast to train. With a reasonable space and time overhead that is constant in the data size, we set a new state-of-the-art on several public datasets, including a billion-scale one. 1.},
author = {Kalantidis, Yannis and Avrithis, Yannis},
doi = {10.1109/CVPR.2014.298},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2013{\_}cvpr{\_}opq.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {{Locally Optimized Product Quantization for Approximate Nearest Neighbor Search}},
url = {http://image.ntua.gr/iva/files/lopq.pdf},
volume = {1},
year = {2014}
}
@article{Hoyer2002a,
abstract = {Non-negative sparse coding is a method for decompos-ing multivariate data into non-negative sparse components. In this paper we briefly describe the motivation behind this type of data representation and its relation to standard sparse coding and non-negative matrix factorization. We then give a simple yet efficient multiplicative algorithm for finding the optimal values of the hid-den components. In addition, we show how the basis vectors can be learned from the observed data. Simulations demonstrate the effectiveness of the proposed method.},
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0202009v1},
author = {Hoyer, Patrik O},
eprint = {0202009v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/0202009.pdf:pdf},
primaryClass = {arXiv:cs},
title = {{Non-Negative Sparse Coding}},
year = {2002}
}
@article{Cherian2014a,
author = {Cherian, Anoop},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/icml2014c2{\_}cherian14.pdf:pdf},
isbn = {9781634393973},
keywords = {sparse coding, incoherent dictionary learning, nea},
title = {{Nearest Neighbors Using Compact Sparse Codes}},
volume = {32},
year = {2014}
}
@article{Hartman1959,
author = {Hartman, P},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/euclid.pjm.1103039111.pdf:pdf},
journal = {Pacific Journal of Mathematics},
number = {x},
pages = {707--713},
title = {{On Functions Representable as a Difference of Two Convex Functions}},
volume = {9},
year = {1959}
}
@inproceedings{Zepeda2010b,
author = {Zepeda, Joaquin and Guillemot, Christine and Kijak, Ewa},
booktitle = {IEEE Workshop on Multimedia Signal Processing},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/bitd{\_}mmsp{\_}embbeded.pdf:pdf},
title = {{The Iteration-Tuned Dictionary for Sparse Representations}},
year = {2010}
}
@article{Romero,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0737v4},
author = {Romero, Daniel and Member, Student and Roberto, L},
eprint = {arXiv:1311.0737v4},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1311.0737.pdf:pdf},
number = {3},
pages = {1--15},
title = {{Compression Limits for Random Vectors with Linearly Parameterized Second-Order Statistics}}
}
@incollection{LeCun2002,
author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve and Muller, Klaus-Robert},
booktitle = {Neural Networks: Tricks of the Trade},
pages = {9--50},
title = {{Efficient BackProp}},
year = {2002}
}
@article{Morand2006,
author = {Morand, C. and Benois-Pineau, J. and Domenger, J.-Ph. and Zepeda, J. and Kijak, E. and Guillemot, C.},
journal = {Image Communication, Elsevier},
title = {{Scalable Object-based Video Retrieval in HD Video Databases}},
year = {2006}
}
@inproceedings{Zepeda2009,
abstract = {This paper addresses the problem of efficient SIFT-based image description and searches in large databases within the framework of local querying. A descriptor called the bag-of-features has been introduced previously which first vector quantizes SIFT descriptors and then aggregates the set of resulting codeword indices (so-called visual words) into a histogram of occurrence of the different visual words in the image. The aim is to make the image search complexity tractable by transforming the set of local image descriptor vectors into a single sparse vector as sparsity particularly permits efficient inner product calculations. However, aggregating local descriptors into a single histogram decreases the discerning power of the system when performing local queries. In this paper, we propose a new approach that aims to enjoy the complexity benefits of sparsity while at the same time retaining the local quality of the input descriptor vectors. This is accomplished by searching for a sparse approximation of the input SIFT descriptors. The sparse approximation yields a sparse vector per local SIFT descriptor, and helps preserving local description properties by using each sparse-transformed descriptor independently in a voting system to retrieve indexed images. Our system is shown experimentally to perform better than histogram based systems under query locality, albeit at an increased complexity.},
author = {Zepeda, Joaquin and Kijak, Ewa and Guillemot, Christine},
booktitle = {IEEE International Workshop on Multimedia Signal Processing},
doi = {10.1109/MMSP.2009.5293301},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/VisualSents{\_}bbl{\_}mmsp09.pdf:pdf},
isbn = {978-1-4244-4463-2},
title = {{SIFT-based local image description using sparse representations}},
year = {2009}
}
@inproceedings{Zepeda2006,
author = {Zepeda, Joaquin and Labeau, Fabrice},
booktitle = {IEEE Vehicular Technology Conference},
title = {{Tandem Filter Bank-DFT Code for Bursty Erasure Correction}},
year = {2006}
}
@article{Theriault2013,
abstract = {This paper presents an extension of the HMAX model, a neural network model for image classification. The HMAX model can be described as a four-level architecture, with the first level consisting of multiscale and multiorientation local filters. We introduce two main contributions to this model. First, we improve the way the local filters at the first level are integrated into more complex filters at the last level, providing a flexible description of object regions and combining local information of multiple scales and orientations. These new filters are discriminative and yet invariant, two key aspects of visual classification. We evaluate their discriminative power and their level of invariance to geometrical transformations on a synthetic image set. Second, we introduce a multiresolution spatial pooling. This pooling encodes both local and global spatial information to produce discriminative image signatures. Classification results are reported on three image data sets: Caltech101, Caltech256, and fifteen scenes. We show significant improvements over previous architectures using a similar framework.},
author = {Theriault, Christian and Thome, Nicolas and Cord, Matthieu},
doi = {10.1109/TIP.2012.2222900},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/theriault{\_}TIP-draft-2012.pdf:pdf},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Convolutional network,multiscale,object recognition,spatial pooling,vision},
pages = {764--777},
pmid = {23060335},
title = {{Extended coding and pooling in the HMAX model}},
volume = {22},
year = {2013}
}
@misc{Nowlan1992,
abstract = {One way of simplifying neural networks so they generalize better is to add an extra term to the error funtion that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problem demonstrate that this complexity term is more effective than previous complexity terms.},
author = {Nowlan, Steven J. and Hinton, Geoffrey E.},
booktitle = {Neural Computation},
doi = {10.1162/neco.1992.4.4.473},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/simp{\_}neural{\_}nets{\_}weight{\_}sharing.pdf:pdf},
issn = {0899-7667},
pages = {473--493},
title = {{Simplifying Neural Networks by Soft Weight-Sharing}},
volume = {4},
year = {1992}
}
@inproceedings{Reinhard2015,
author = {Reinhard, Erik and Zepeda, Joaquin and Andrivon, Pierre and Stauder, Jurgen and Bordes, Philippe and Chevance, Christophe and Fran{\c{c}}ois, Edouard and Morvan, Patrick},
booktitle = {IBC},
title = {{Electro-Optical and Opto-Electrical Transfer Functions for High Dynamic Range Content}},
year = {2015}
}
@inproceedings{Burgos2015,
author = {Burgos, Xavier and Zepeda, Joaquin and {Le Clerc}, Fran{\c{c}}ois and P{\'{e}}rez, Patrick},
booktitle = {International Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/iccv15{\_}submitted.pdf:pdf},
title = {{Pose and expression-coherent face recovery in the wild}},
year = {2015}
}
@inproceedings{Martins2006,
author = {Martins, S. and Zepeda, J. and Picard, B. and Radziszewski, P. and Roy, D.},
booktitle = {Autogeneous and Semiautogeneous Grinding Technology Conference},
title = {{Investigating On-The-Shell Acoustics}},
year = {2006}
}
@inproceedings{Zepeda2015,
author = {Zepeda, Joaquin and P{\'{e}}rez, Patrick},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/esvmf{\_}submitted2.pdf:pdf},
title = {{Exemplar SVMs as Visual Feature Encoders}},
year = {2015}
}
@inproceedings{Bilen2015,
author = {Bilen, Cagdas and Zepeda, Joaquin and {Patrick P{\'{e}}rez}},
booktitle = {Signal Processing with Adaptive Sparse Structured Representations},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/SPARS{\_}sparsesimilarity.pdf:pdf},
title = {{Learning Sparsity Inducing Analysis Operators for Discriminative Similarity Metrics}},
year = {2015}
}
@inproceedings{Kulkarni2015,
author = {Kulkarni, Praveen and Zepeda, Joaquin and Jurie, Fr{\'{e}}d{\'{e}}ric and Perez, Patrick and Chevallier, Louis},
booktitle = {BigVision Workshop, Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/max{\_}margin{\_}single{\_}layer{\_}adaptation.pdf:pdf},
title = {{Max-Margin, Single-Layer Adaptation of Transferred Image Features}},
year = {2015}
}
@inproceedings{Zepeda,
author = {Zepeda, Joaquin and Perez, Patrick},
booktitle = {2015},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/SPARS{\_}sparsesimilarity.pdf:pdf},
title = {{Learning Sparsity Inducing Analysis Operators for Discriminative Similarity Metrics}}
}
@article{Cvpr2015,
author = {Cvpr, Anonymous and Id, Paper},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/max{\_}margin{\_}single{\_}layer{\_}adaptation.pdf:pdf},
title = {{Max-Margin , Single-Layer Adaptation of Transferred Image Features}},
year = {2015}
}
@inproceedings{Kulkarni2014,
author = {Kulkarni, Praveen and Sharma, Gaurav and Zepeda, Joaquin and Chevallier, Louis},
booktitle = {IEEE Winter Conference on Applications of Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/attrib{\_}wacv14.pdf:pdf},
title = {{Transfer Learning via Attributes for Improved On-the-fly Classification}},
year = {2014}
}
@inproceedings{DeAraujo2013,
abstract = {Video search has become a very important tool, with the ever-growing size of multimedia collections. This work introduces our Video Semantic Indexing system. Our experiments show that Residual Vectors provide an efficient way of aggregat- ing local descriptors, with complementary gain with respect to BoVW. Also, we show that systems using a limited number of descriptors and machine learning techniques can still be quite effective. Our first participation at the TRECVID evaluation has been very fruitful: our team was ranked 6th in the light version of the Semantic Indexing task.},
author = {de Araujo, Andr{\'{e}}. F. and Silveira, Fernando and Lakshman, Haricharan and Zepeda, Joaquin and Sheth, Anmol and Perez, P{\'{e}}rez and Girod, Bernd},
booktitle = {TRECVID},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/stanford{\_}trecvid.pdf:pdf},
keywords = {bovw,centrist,dense ex-,for each run,harlap keypoint detector,l a stanford1 1,oppsift,residual,sift,spm,traction},
title = {{The Stanford/Technicolor/Fraunhofer HHI Video Semantic Indexing System}},
year = {2013}
}
@article{Bengio2009,
abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua},
doi = {10.1561/2200000006},
eprint = {0500581},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/TR1312.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {Foundations and Trends{\textregistered} in Machine Learning},
pages = {1--127},
pmid = {17348934},
primaryClass = {submit},
title = {{Learning Deep Architectures for AI}},
volume = {2},
year = {2009}
}
@article{Pendse2011,
author = {Pendse, Gautam V},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/pendseLassoShooting.pdf:pdf},
pages = {1--16},
title = {{A tutorial on the LASSO and the ” shooting algorithm ” List of Figures}},
year = {2011}
}
@article{LeRoux2008,
abstract = {Deep belief networks (DBN) are generative neural network models with many layers of hidden explanatory factors, recently introduced by Hinton, Osindero, and Teh (2006) along with a greedy layer-wise unsupervised learning algorithm. The building block of a DBN is a probabilistic model called a restricted Boltzmann machine (RBM), used to represent one layer of the model. Restricted Boltzmann machines are interesting because inference is easy in them and because they have been successfully used as building blocks for training deeper models. We first prove that adding hidden units yields strictly improved modeling power, while a second theorem shows that RBMs are universal approximators of discrete distributions. We then study the question of whether DBNs with more layers are strictly more powerful in terms of representational power. This suggests a new and less greedy criterion for training RBMs within DBNs.},
archivePrefix = {arXiv},
arxivId = {110008689868},
author = {{Le Roux}, Nicolas and Bengio, Yoshua},
doi = {10.1162/neco.2008.04-07-510},
eprint = {110008689868},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/representational{\_}power.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
pages = {1631--1649},
pmid = {18254699},
title = {{Representational power of restricted boltzmann machines and deep belief networks.}},
volume = {20},
year = {2008}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different “thinned” networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/srivastava14a.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Deep basics,deep learning,model combination,neural networks,regularization},
mendeley-tags = {Deep basics},
pages = {1929--1958},
title = {{Dropout : A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Hinton2006a,
abstract = {We show how to use complementary priors to eliminate the explaining- away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa- tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive ver- sion of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribu- tion of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning al- gorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/fastnc.pdf:pdf},
journal = {Neural Computation},
pages = {1527--1554},
title = {{Communicated by Yann Le Cun A Fast Learning Algorithm for Deep Belief Nets 500 units 500 units}},
volume = {18},
year = {2006}
}
@article{Zeiler2013,
abstract = {We introduce a simple and effective method for regularizing large convolutional neural networks. We replace the conventional deterministic pooling operations with a stochastic procedure, randomly picking the activation within each pooling region according to a multinomial distribution, given by the activities within the pooling region. The approach is hyper-parameter free and can be combined with other regularization approaches, such as dropout and data augmentation. We achieve state-of-the-art performance on four image datasets, relative to other approaches that do not utilize data augmentation.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3557v1},
author = {Zeiler, Matthew and Fergus, Rob},
eprint = {arXiv:1301.3557v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/iclr2013.pdf:pdf},
journal = {arXiv preprint arXiv:1301.3557},
pages = {1--9},
title = {{Stochastic pooling for regularization of deep convolutional neural networks}},
url = {http://arxiv.org/abs/1301.3557},
year = {2013}
}
@inproceedings{Ng2014,
author = {Ng, Hong-Wei and Winkler, Stefan},
booktitle = {International Conference on Image Processing},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/icip2014a.pdf:pdf},
title = {{A Data-Driven Approach to Cleaning Large Face Datasets}},
year = {2014}
}
@book{Lu2012,
author = {Lu, Zhaosong and Monteiro, Renato D C and Yuan, Ming},
booktitle = {Mathematical Programming},
doi = {10.1007/s10107-010-0350-1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/dimreduct{\_}MP.pdf:pdf},
isbn = {0001405101},
issn = {00255610},
keywords = {Cone programming,Dimension reduction,First-order method,Multivariate linear regression,Nuclear or trace norm,Smooth saddle point problem},
pages = {163--194},
title = {{Convex optimization methods for dimension reduction and coefficient estimation in multivariate linear regression}},
volume = {131},
year = {2012}
}
@article{Eynard,
author = {Eynard, Davide and Kovnatsky, Artiom and Bronstein, Michael M and Member, Senior and Glashoff, Klaus and Bronstein, Alexander M},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/EynKovBroGlaBroPAMI13.pdf:pdf},
pages = {1--14},
title = {{Multimodal manifold analysis by simultaneous diagonalization of Laplacians}}
}
@article{Ghodsi2006,
author = {Ghodsi, Ali},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/tutorial{\_}stat890.pdf:pdf},
journal = {Science},
pages = {25},
title = {{Dimensionality Reduction A Short Tutorial}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.3592{\&}rep=rep1{\&}type=pdf$\backslash$nhttp://stats.stackexchange.com/questions/7111/pca-for-images-arrays-with-high-dimensionality},
year = {2006}
}
@article{Shalev-Shwartz2009,
abstract = {We describe and analyze two stochastic methods for ℓ1 regularized loss minimization problems, such as the Lasso. The ﬁrst method updates the weight of a single feature at each iteration while the second method updates the entire weight vector but only uses a single training example at each iteration. In both methods, the choice of feature or example is uniformly at random. Our theoretical runtime analysis suggests that the stochastic methods should outperform state-of-the-art deterministic approaches, including their deterministic counterparts, when the size of the problem is large. We demonstrate the advantage of stochastic methods by experimenting with synthetic and natural data sets.},
author = {Shalev-Shwartz, Shai and Tewari, a},
doi = {10.1145/1553374.1553493},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/ShalevTewari09.pdf:pdf},
isbn = {9781605585161},
journal = {26th International Conference on Machine Learning},
keywords = {coordinate descent,l1 regularization,mirror descent,optimization,sparsity},
pages = {929--936},
title = {{Stochastic Methods for ℓ1-regularized Loss Minimization}},
url = {http://eprints.pascal-network.org/archive/00005418/},
volume = {12},
year = {2009}
}
@article{Kostadinov2014,
author = {Kostadinov, Dimche},
doi = {10.1117/12.2042506},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/SPIE-2014-EXTENTION-FINAL{\_}SVv1.pdf:pdf},
isbn = {9780819499455},
issn = {1996756X},
journal = {Is{\&}T/Spie {\ldots}},
keywords = {compressive sensing,encoding,face recognition,sparse approximations},
number = {22},
title = {{Robust human face recognition based on locality preserving sparse over complete block approximation}},
url = {http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=1833104},
volume = {41},
year = {2014}
}
@article{Diephuis,
author = {Diephuis, Maurits},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2014.WIC.dimce.pdf:pdf},
title = {{Visual information encoding for face recognition : sparse coding vs vector quantization Problem Formulation}}
}
@article{Voloshynovskiy2015,
author = {Voloshynovskiy, S and Diephuis, M and Holotyak, T},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/SKETCHprint{\_}SPIE2015.pdf:pdf},
title = {{Mobile visual object identification : from SIFT-BoF-RANSAC to SketchPrint}},
year = {2015}
}
@article{Fischer2014,
abstract = {Latest results indicate that features learned via convolutional neural networks outperform previous descriptors on classification tasks by a large margin. It has been shown that these networks still work well when they are applied to datasets or recognition tasks different from those they were trained on. However, descriptors like SIFT are not only used in recognition but also for many correspondence problems that rely on descriptor matching. In this paper we compare features from various layers of convolutional neural nets to standard SIFT descriptors. We consider a network that was trained on ImageNet and another one that was trained without supervision. Surprisingly, convolutional neural networks clearly outperform SIFT on descriptor matching.},
archivePrefix = {arXiv},
arxivId = {1405.5769},
author = {Fischer, Philipp and Dosovitskiy, Alexey and Brox, Thomas},
eprint = {1405.5769},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1405.5769v1.pdf:pdf},
pages = {1--10},
title = {{Descriptor Matching with Convolutional Neural Networks: a Comparison to SIFT}},
url = {http://arxiv.org/abs/1405.5769},
year = {2014}
}
@article{Dosovitskiy2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.6909v1},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
eprint = {arXiv:1406.6909v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/exemplar{\_}CNN.pdf:pdf},
journal = {arXiv preprint arXiv: {\ldots}},
pages = {1--13},
title = {{Discriminative Unsupervised Feature Learning with Convolutional Neural Networks}},
year = {2014}
}
@article{Dosovitskiy,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.6909v1},
author = {Dosovitskiy, Alexey and Springenberg, Jost Tobias and Riedmiller, Martin and Brox, Thomas},
eprint = {arXiv:1406.6909v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/exemplar{\_}CNN{\_}supp.pdf:pdf},
number = {5},
pages = {1--13},
title = {{Discriminative Unsupervised Feature Learning with Convolutional Neural Networks}},
volume = {0}
}
@article{Feldman2013,
abstract = {Signal and image processing have seen an explosion of interest in the last few years in a new form of signal/image characterization via the concept of sparsity with respect to a dictionary. An active field of research is dictionary learning: the representation of a given large set of vectors (e.g. signals or images) as linear combinations of only few vectors (patterns). To further reduce the size of the representation, the combinations are usually required to be sparse, i.e., each signal is a linear combination of only a small number of patterns. This paper suggests a new computational approach to the problem of dictionary learning, known in computational geometry as coresets. A coreset for dictionary learning is a small smart non-uniform sample from the input signals such that the quality of any given dictionary with respect to the input can be approximated via the coreset. In particular, the optimal dictionary for the input can be approximated by learning the coreset. Since the coreset is small, the learning is faster. Moreover, using merge-and-reduce, the coreset can be constructed for streaming signals that do not fit in memory and can also be computed in parallel. We apply our coresets for dictionary learning of images using the K-SVD algorithm and bound their size and approximation error analytically. Our simulations demonstrate gain factors of up to 60 in computational time with the same, and even better, performance. We also demonstrate our ability to perform computations on larger patches and high-definition images, where the traditional approach breaks down.},
author = {Feldman, Dan and Feigin, Micha and Sochen, Nir},
doi = {10.1007/s10851-013-0431-x},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/JMIV.pdf:pdf},
isbn = {0924-9907},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {Coresets,Dictionary learning,K-SVD,Sparsity},
pages = {276--291},
title = {{Learning big (image) data via coresets for dictionaries}},
volume = {46},
year = {2013}
}
@article{Ferdowsi,
author = {Ferdowsi, Sohrab},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Sohrab{\_}WIC2014.pdf:pdf},
title = {{Content identification : machine learning meets coding Multi-class Classification : A Coding Based Ap- proach}}
}
@inproceedings{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. A particularly significant one is data augmentation, which achieves a boost in performance in shallow methods analogous to that observed with CNN-based methods. Finally, we are planning to provide the configurations and code that achieve the state-of-the-art performance on the PASCAL VOC Classification challenge, along with alternative configurations trading-off performance, computation speed and compactness.},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
booktitle = {British Machine Vision Conference},
eprint = {1405.3531},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/chatfield14.pdf:pdf},
title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
url = {http://arxiv.org/abs/1405.3531},
year = {2014}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Dimche{\_}Kostadinov{\_}EUSIPCO2015.pdf:pdf},
title = {{VECTOR QUANTIZATION WITH CONSTRAINED LIKELIHOOD FOR FACE RECOGNITION Dimche Kostadinov , Sviatoslav Voloshynovskiy , Maurits Diephuis and Sohrab Ferdowsi University of Geneva Computer Science Department , Stochastic Information Processing Group 7 Route de}}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Scenario2.pdf:pdf},
pages = {5--9},
title = {{SPARSE MULTI-LAYER IMAGE APPROXIMATION : IMAGE COMPRESSION Sohrab Ferdowsi , Svyatoslav Voloshynovskiy , Dimche Kostadinov University of Geneva Department of Computer Science Battelle Bˆ at . A , 7 rte de Drize , 1227 Carouge , Switzerland}}
}
@article{Juefei-xu,
author = {Juefei-xu, Felix and Pal, Dipan K and Savvides, Marios},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Juefei-Xu{\_}Hallucinating{\_}the{\_}Full{\_}2014{\_}CVPR{\_}paper.pdf:pdf},
title = {{Hallucinating the Full Face from the Periocular Region via Dimensionally Weighted K-SVD}}
}
@article{Wang2014,
abstract = {Learning fine-grained image similarity is a challenging task. It needs to capture between-class and within-class image differences. This paper proposes a deep ranking model that employs deep learning techniques to learn similarity metric directly from images.It has higher learning capability than models based on hand-crafted features. A novel multiscale network structure has been developed to describe the images effectively. An efficient triplet sampling algorithm is proposed to learn the model with distributed asynchronized stochastic gradient. Extensive experiments show that the proposed algorithm outperforms models based on hand-crafted visual features and deep classification models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1404.4661v1},
author = {Wang, Jiang and Song, Yang and Leung, Thomas and Rosenberg, Chuck and Wang, Jingbin and Philbin, James and Chen, Bo and Wu, Ying},
doi = {10.1109/CVPR.2014.180},
eprint = {arXiv:1404.4661v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/deep ranking.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
journal = {Cvpr},
pages = {1386--1393},
title = {{Learning Fine-grained Image Similarity with Deep Ranking}},
year = {2014}
}
@article{Domingos2012,
author = {Domingos, Pedro},
doi = {10.1145/2347736.2347755},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
pages = {78},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Hu2014,
author = {Hu, Junlin and Lu, Jiwen and Tan, Yap-peng},
doi = {10.1109/CVPR.2014.242},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Hu{\_}Discriminative{\_}Deep{\_}Metric{\_}2014{\_}CVPR{\_}paper.pdf:pdf},
isbn = {9781479951178},
issn = {10636919},
pages = {1875--1882},
title = {{Discriminative Deep Metric Learning for Face Verification in the Wild}},
year = {2014}
}
@article{Verma,
author = {Verma, Nakul},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/metric{\_}learning{\_}tutorial{\_}verma.pdf:pdf},
title = {{A tutorial on Metric Learning with some recent advances}}
}
@inproceedings{Schroff2015,
abstract = {By a Liouville structure on a symplectic manifold {\$}(M, \backslashomega){\$} we mean a choice of symplectic potential: that is, a choice of one-form {\$}\backslashtheta{\$} on {\$}M{\$} such that {\$}{\{}\backslashrm d{\}} \backslashtheta = \backslashomega{\$}. We determine precisely all the automorphisms of a Liouville structure in case {\$}(M, \backslashomega){\$} is a symplectic vector space and {\$}\backslashtheta{\$} differs from its canonical symplectic potential by the differential of a homogeneous monomial.},
archivePrefix = {arXiv},
arxivId = {1503.0383},
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
eprint = {1503.0383},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1503.03832v1.pdf:pdf},
pages = {7},
title = {{FaceNet: A Unified Embedding for Face Recognition and Clustering}},
url = {http://arxiv.org/abs/1503.0383},
year = {2015}
}
@techreport{Hoffer2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6622v2},
author = {Hoffer, Elad and Ailon, Nir},
eprint = {arXiv:1412.6622v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1412.6622v2.pdf:pdf},
number = {2010},
pages = {1--8},
title = {{Deep metric learning using triplet network}},
year = {2015}
}
@article{Wiegand2003,
abstract = {H.264/AVC is newest video coding standard of the ITU-T Video Coding Experts Group and the ISO/IEC Moving Picture Experts Group. The main goals of the H.264/AVC standardization effort have been enhanced compression performance and provision of a "network-friendly" video representation addressing "conversational" (video telephony) and "nonconversational" (storage, broadcast, or streaming) applications. H.264/AVC has achieved a significant improvement in rate-distortion efficiency relative to existing standards. This article provides an overview of the technical features of H.264/AVC, describes profiles and applications for the standard, and outlines the history of the standardization process.},
author = {Wiegand, T.},
doi = {10.1109/TCSVT.2003.815165},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/AVC{\_}overview{\_}1.pdf:pdf},
issn = {1051-8215},
journal = {{\ldots} and Systems for Video {\ldots}},
number = {7},
pages = {560 --576},
title = {{Overview of the H. 264/AVC video coding standard}},
url = {http://ieeexplore.ieee.org/ielx5/76/27384/01218189.pdf?tp={\&}arnumber=1218189{\&}isnumber=27384$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1218189},
volume = {13},
year = {2003}
}
@article{Sugimoto2004,
author = {Sugimoto, Kazuo and Kobayashi, Mitsuru and Suzuki, Yoshinori and Kato, Sadaatsu and Boon, Choong Seng},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/01418791.pdf:pdf},
isbn = {0780385543},
pages = {465--468},
title = {{Multimedia Laboratories, NTT DoCoMo, Inc. 3-5, Hikarinooka, Yokosuka, Kanagawa, Japan}},
year = {2004}
}
@article{Yi2014,
abstract = {Various hand-crafted features and metric learning methods prevail in the field of person re-identification. Compared to these methods, this paper proposes a more general way that can learn a similarity metric from image pixels directly. By using a "siamese" deep neural network, the proposed method can jointly learn the color feature, texture feature and metric in a unified framework. The network has a symmetry structure with two sub-networks which are connected by Cosine function. To deal with the big variations of person images, binomial deviance is used to evaluate the cost between similarities and labels, which is proved to be robust to outliers. Compared to existing researches, a more practical setting is studied in the experiments that is training and test on different datasets (cross dataset person re-identification). Both in "intra dataset" and "cross dataset" settings, the superiorities of the proposed method are illustrated on VIPeR and PRID.},
archivePrefix = {arXiv},
arxivId = {1407.4979},
author = {Yi, Dong and Lei, Zhen and Li, Stan Z.},
doi = {10.1109/ICPR.2014.16},
eprint = {1407.4979},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1407.4979v1.pdf:pdf},
isbn = {978-1-4799-5209-0},
number = {4},
pages = {1--11},
title = {{Deep Metric Learning for Practical Person Re-Identification}},
url = {http://arxiv.org/abs/1407.4979},
volume = {11},
year = {2014}
}
@article{Zheng2008,
author = {Zheng, Yunfei and Yin, Peng and Escoda, Divorra and Li, Xin and Gomila, Cristina},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/04711707.pdf:pdf},
isbn = {9781424417643},
issn = {1522-4880},
journal = {International Conference on Image Processing},
pages = {125--128},
title = {{Intra Prediction Using Template Matching with Adaptive Illumination Compensation}},
year = {2008}
}
@article{Guo2008,
author = {Guo, Yi and Wang, Ye-kui and Li, Houqiang},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/04607635.pdf:pdf},
isbn = {9781424425716},
pages = {1117--1120},
title = {{PRIORITY-BASED TEMPLATE MATCHING INTRA PREDICTION University of Science and Technology of China , Hefei , China , 230027}},
year = {2008}
}
@article{Criminisi2004,
abstract = {A new algorithm is proposed for removing large objects from digital images. The challenge is to fill in the hole that is left behind in a visually plausible way. In the past, this problem has been addressed by two classes of algorithms: 1) "texture synthesis" algorithms for generating large image regions from sample textures and 2) "inpainting" techniques for filling in small image gaps. The former has been demonstrated for "textures"--repeating two-dimensional patterns with some stochasticity; the latter focus on linear "structures" which can be thought of as one-dimensional patterns, such as lines and object contours. This paper presents a novel and efficient algorithm that combines the advantages of these two approaches. We first note that exemplar-based texture synthesis contains the essential process required to replicate both texture and structure; the success of structure propagation, however, is highly dependent on the order in which the filling proceeds. We propose a best-first algorithm in which the confidence in the synthesized pixel values is propagated in a manner similar to the propagation of information in inpainting. The actual color values are computed using exemplar-based synthesis. In this paper, the simultaneous propagation of texture and structure information is achieved by a single, efficient algorithm. Computational efficiency is achieved by a block-based sampling process. A number of examples on real and synthetic images demonstrate the effectiveness of our algorithm in removing large occluding objects, as well as thin scratches. Robustness with respect to the shape of the manually selected target region is also demonstrated. Our results compare favorably to those obtained by existing techniques.},
author = {Criminisi, Antonio and P{\'{e}}rez, Patrick and Toyama, Kentaro},
doi = {10.1109/TIP.2004.833105},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/criminisi{\_}tip2004.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {image inpainting,multaneous texture and structure,object removal,propagation,si-,texture synthesis},
number = {9},
pages = {1200--1212},
pmid = {15449582},
title = {{Region filling and object removal by exemplar-based image inpainting}},
volume = {13},
year = {2004}
}
@article{Bertalmio2003,
abstract = {An algorithm for the simultaneous filling-in of texture and structure in regions of missing image information is presented in this paper. The basic idea is to first decompose the image into the sum of two functions with different basic characteristics, and then reconstruct each one of these functions separately with structure and texture filling-in algorithms. The first function used in the decomposition is of bounded variation, representing the underlying image structure, while the second function captures the texture and possible noise. The region of missing information in the bounded variation image is reconstructed using image inpainting algorithms, while the same region in the texture image is filled-in with texture synthesis techniques. The original image is then reconstructed adding back these two sub-images. The novel contribution of this paper is then in the combination of these three previously developed components, image decomposition with inpainting and texture synthesis, which permits the simultaneous use of filling-in algorithms that are suited for different image characteristics. Examples on real images show the advantages of this proposed approach.},
author = {Bertalmio, Marcelo and Vese, Luminita and Sapiro, Guillermo and Osher, Stanley},
doi = {10.1109/TIP.2003.815261},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/01217265.pdf:pdf},
isbn = {0-7695-1900-8},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bounded variation,Filling-in,Image decomposition,Inpainting,Structure,Texture,Texture synthesis},
number = {8},
pages = {882--889},
pmid = {18237962},
title = {{Simultaneous Structure and Texture Image Inpainting}},
volume = {12},
year = {2003}
}
@article{Gray1998,
author = {Gray, Robert M and Neuhoff, David L},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Gray{\_}and{\_}Neuhoff{\_}{\_}1998.pdf:pdf},
number = {6},
pages = {2325--2383},
title = {{Quantization}},
volume = {44},
year = {1998}
}
@inproceedings{Yang2006,
author = {Yang, Jiheng and Yin, Baocai and Sun, Yanfeng and Zhang, Nan},
booktitle = {ICME},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/04036697.pdf:pdf},
pages = {705--708},
title = {{A BLOCK-MATCHING BASED INTRA FRAME PREDICTION FOR H . 264 / AVC}},
year = {2006}
}
@article{Shuman2013,
abstract = {In applications such as social, energy, transportation, sensor, and neuronal networks, high-dimensional data naturally reside on the vertices of weighted graphs. The emerging field of signal processing on graphs merges algebraic and spectral graph theoretic concepts with computational harmonic analysis to process such signals on graphs. In this tutorial overview, we outline the main challenges of the area, discuss different ways to define graph spectral domains, which are the analogs to the classical frequency domain, and highlight the importance of incorporating the irregular structures of graph data domains when processing signals on graphs. We then review methods to generalize fundamental operations such as filtering, translation, modulation, dilation, and downsampling to the graph setting and survey the localized, multiscale transforms that have been proposed to efficiently extract information from high-dimensional data on graphs. We conclude with a brief discussion of open issues and possible extensions.},
archivePrefix = {arXiv},
arxivId = {1211.0053},
author = {Shuman, David I. and Narang, Sunil K. and Frossard, Pascal and Ortega, Antonio and Vandergheynst, Pierre},
doi = {10.1109/MSP.2012.2235192},
eprint = {1211.0053},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/06494675.pdf:pdf},
isbn = {1053-5888 VO - 30},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {may},
pages = {83--98},
title = {{The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains}},
volume = {30},
year = {2013}
}
@article{Goyal2001,
abstract = {Discusses various aspects of transform coding, including: source
coding, constrained source coding, the standard theoretical model for
transform coding, entropy codes, Huffman codes, quantizers, uniform
quantization, bit allocation, optimal transforms, transforms
visualization, partition cell shapes, autoregressive sources, transform
optimization, synthesis transform optimization, orthogonality and
independence, and departures form the standard model},
author = {Goyal, V.K.},
doi = {10.1109/79.952802},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Goyal{\_}SigProcMag2001{\_}TC.pdf:pdf},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
number = {September},
pages = {9--21},
title = {{Theoretical foundations of transform coding}},
volume = {18},
year = {2001}
}
@article{Sensitivity,
author = {Sensitivity, Contrast},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Contrast sensitivity of the human eye and its effects on image quality - Barten.pdf:pdf},
title = {{and Its Effects on Image Quality}}
}
@article{Miller2013,
abstract = {As the performance of electronic display systems continues to increase, the limitations of current signal coding methods become more and more apparent. With bit depth limitations set by industry standard interfaces, a more efficient coding system is desired to allow image quality to increase without requiring expansion of legacy infrastructure bandwidth. A good approach to this problem is to let the human visual system determine the quantization curve used to encode video signals. In this way optimal efficiency is maintained across the luminance range of interest, and the visibility of quantization artifacts is kept to a uniformly small level. },
author = {Miller, S. and Nezamabadi, M. and Daly, S.},
doi = {10.5594/j18290},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/SMPTE Mot  Imag  J-2013-Miller-52-9.pdf:pdf},
isbn = {9781622765386},
issn = {1545-0279},
journal = {SMPTE Motion Imaging Journal},
pages = {52--59},
title = {{Perceptual Signal Coding for More Efficient Usage of Bit Codes}},
url = {http://journal.smpte.org/cgi/doi/10.5594/j18290},
volume = {122},
year = {2013}
}
@article{Bengio2013,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1206.5538.pdf:pdf},
isbn = {0162-8828 VO - 35},
issn = {01628828},
journal = {Pattern Analysis and {\ldots}},
number = {1993},
pages = {1--30},
pmid = {23459267},
title = {{Representation learning: A review and new perspectives}},
url = {http://arxiv.org/abs/1206.5538$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6472238},
year = {2013}
}
@article{Makhzani2014,
abstract = {Recently, it has been observed that when rep- resentations are learnt in a way that encour- ages sparsity, improved performance is ob- tained on classification tasks. These meth- ods involve combinations of activation func- tions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the “k- sparse autoencoder”, which is an autoen- coder with linear activation function, where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than de- noising autoencoders, networks trained with dropout, and RBMs. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problemsizes, where conventional sparse cod- ing algorithms cannot be applied. 1.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.5663v2},
author = {Makhzani, Alireza and Frey, Brendan},
eprint = {arXiv:1312.5663v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1312.5663v2.pdf:pdf},
journal = {Iclr},
keywords = {()},
title = {{k-Sparse Autoencoders}},
year = {2014}
}
@article{Fawzi2014,
abstract = {Classifiers based on sparse representations have recently been shown to provide excellent results in many visual recognition and classification tasks. However, the high cost of computing sparse representations is a major obstacle that limits the applicability of these methods in large-scale problems, or in scenarios where computational power is restricted. We consider in this paper a simple yet efficient alternative to sparse coding for feature extraction. We study a classification scheme built by applying the soft-thresholding nonlinear mapping in a dictionary, followed by a linear classifier. A novel supervised dictionary learning algorithm tailored for this low complexity classification architecture is proposed. The dictionary learning problem, which jointly estimates the dictionary and linear classifier, is cast as a difference of convex (DC) program, and solved efficiently with an iterative DC solver. We conduct extensive experiments on several datasets, and show that our simple soft-thresholding based classifier competes with state-of-the-art sparse coding classifiers, when the dictionary is learned appropriately. Our classification scheme moreover achieves significant gains in terms of computational time at the testing stage, compared to other classifiers. The proposed scheme shows the potential of the soft-thresholding mapping for classification, and paves the way towards the development of very efficient classification methods for vision problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1973v1},
author = {Fawzi, Alhussein and Davies, Mike and Frossard, Pascal},
doi = {10.1007/s11263-014-0784-7},
eprint = {arXiv:1402.1973v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1402.1973v2.pdf:pdf},
issn = {15731405},
pages = {1--16},
title = {{Dictionary learning for fast classification based on soft-thresholding}},
year = {2014}
}
@article{Tan2006,
author = {Tan, Thiow Keng and Boon, Choong Seng and Suzuki, Yoshinori},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/04106874.pdf:pdf},
isbn = {1424404819},
journal = {International Conference on Image Processing},
pages = {1693--1696},
title = {{Intra prediction by template matching}},
year = {2006}
}
@article{Tan2007,
author = {Tan, Thiow Keng and Boon, Choong Seng and Suzuki, Yoshinori},
doi = {10.1109/CCNC.2007.86},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/04199174.pdf:pdf},
isbn = {1424406668},
journal = {2007 4th Annual IEEE Consumer Communications and Networking Conference, CCNC 2007},
keywords = {Image coding,Image generation,Video coding},
pages = {405--409},
title = {{Intra prediction by averaged template matching predictors}},
year = {2007}
}
@article{Mensink2013,
abstract = {We study large-scale image classification methods that can incorporate new classes and training images continuously over time at negligible cost. To this end, we consider two distance-based classifiers, the k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers, and introduce a new metric learning approach for the latter. We also introduce an extension of the NCM classifier to allow for richer class representations. Experiments on the ImageNet 2010 challenge dataset, which contains over 10(6) training images of 1,000 classes, show that, surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier. Moreover, the NCM performance is comparable to that of linear SVMs which obtain current state-of-the-art performance. Experimentally, we study the generalization performance to classes that were not used to learn the metrics. Using a metric learned on 1,000 classes, we show results for the ImageNet-10K dataset which contains 10,000 classes, and obtain performance that is competitive with the current state-of-the-art while being orders of magnitude faster. Furthermore, we show how a zero-shot class prior based on the ImageNet hierarchy can improve performance when few training images are available.},
author = {Mensink, Thomas and Verbeek, Jakob and Perronnin, Florent and Csurka, Gabriela},
doi = {10.1109/TPAMI.2013.83},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/mensink13pami.pdf:pdf},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Models,Pattern Recognition,Theoretical},
month = {nov},
number = {11},
pages = {2624--37},
pmid = {24051724},
title = {{Distance-based image classification: generalizing to new classes at near-zero cost.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24051724},
volume = {35},
year = {2013}
}
@article{Cherian2014,
abstract = {This paper presents a new nearest neighbor (NN) retrieval framework: robust sparse hashing (RSH). Our approach is inspired by the success of dictionary learning for sparse coding. Our key idea is to sparse code the data using a learned dictionary, and then to generate hash codes out of these sparse codes for accurate and fast NN retrieval. But, direct application of sparse coding to NN retrieval poses a technical difficulty: when data are noisy or uncertain (which is the case with most real-world data sets), for a query point, an exact match of the hash code generated from the sparse code seldom happens, thereby breaking the NN retrieval. Borrowing ideas from robust optimization theory, we circumvent this difficulty via our novel robust dictionary learning and sparse coding framework called RSH, by learning dictionaries on the robustified counterparts of the perturbed data points. The algorithm is applied to NN retrieval on both simulated and real-world data. Our results demonstrate that RSH holds significant promise for efficient NN retrieval against the state of the art.},
author = {Cherian, Anoop and Sra, Suvrit and Morellas, Vassilios and Papanikolopoulos, Nikolaos},
doi = {10.1109/TIP.2014.2324280},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Anoop Cherian Efficient Nearest Neighbors sparse hashing IEEE-IP 2014.pdf:pdf},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
month = {aug},
number = {8},
pages = {3646--55},
pmid = {25122742},
title = {{Efficient nearest neighbors via robust sparse hashing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25122742},
volume = {23},
year = {2014}
}
@inproceedings{Gong2014,
abstract = {Deep convolutional neural networks (CNN) have shown their promise as a universal representation for recognition. However, global CNN activations lack geometric invariance, which limits their robustness for classification and matching of highly variable scenes. To improve the invariance of CNN activations without degrading their discriminative power, this paper presents a simple but effective scheme called multi-scale orderless pooling (MOP-CNN). This scheme extracts CNN activations for local patches at multiple scale levels, performs orderless VLAD pooling of these activations at each level separately, and concatenates the result. The resulting MOP-CNN representation can be used as a generic feature for either supervised or unsupervised recognition tasks, from image classification to instance-level retrieval; it consistently outperforms global CNN activations without requiring any joint training of prediction layers for a particular target dataset. In absolute terms, it achieves state-of-the-art results on the challenging SUN397 and MIT Indoor Scenes classification datasets, and competitive results on ILSVRC2012/2013 classification and INRIA Holidays retrieval datasets.},
archivePrefix = {arXiv},
arxivId = {1403.1840},
author = {Gong, Yunchao and Wang, Liwei and Guo, Ruiqi and Lazebnik, Svetlana},
booktitle = {European Conference on Computer Vision},
eprint = {1403.1840},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/orderless pooling.pdf:pdf},
month = {mar},
title = {{Multi-scale Orderless Pooling of Deep Convolutional Activation Features}},
url = {http://arxiv.org/abs/1403.1840},
year = {2014}
}
@article{Mairal2014,
author = {Mairal, Julien},
doi = {10.1561/0600000058},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/review{\_}sparse{\_}arxiv.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
number = {2-3},
pages = {85--283},
title = {{Sparse Modeling for Image and Vision Processing}},
url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-computer-graphics-and-vision/CGV-058},
volume = {8},
year = {2014}
}
@article{Argyriou,
author = {Argyriou, Andreas and Pontil, Massimiliano},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/3143-multi-task-feature-learning.pdf:pdf},
title = {{Multi-Task Feature Learning}}
}
@article{Lainema2012,
author = {Lainema, Jani and Bossen, Frank and Han, Woo-Jin and Min, Junghye and Ugur, Kemal},
doi = {10.1109/TCSVT.2012.2221525},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/06317153.pdf:pdf},
issn = {1051-8215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
month = {dec},
number = {12},
pages = {1792--1801},
title = {{Intra Coding of the HEVC Standard}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6317153},
volume = {22},
year = {2012}
}
@article{Turkan2012,
author = {Turkan, Mehmet},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/turkan{\_}thesis.pdf:pdf},
title = {image prediction and image inpainting},
year = {2012}
}
@article{Anderson,
author = {Anderson, Robert Finis and Kirtzic, J Steven and Daescu, Ovidiu},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/VECPAR GPU Template Matching.pdf:pdf},
title = {{Applying Parallel Design Techniques to Template Matching with GPUs}}
}
@article{Anderson2009,
author = {Anderson, Robert Finis and Schweitzer, Haim},
doi = {10.1109/ICSMC.2009.5346256},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/FixedTime.pdf:pdf},
isbn = {978-1-4244-2793-2},
journal = {2009 IEEE International Conference on Systems, Man and Cybernetics},
month = {oct},
number = {2},
pages = {1359--1364},
publisher = {Ieee},
title = {{Fixed time template matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5346256},
volume = {2},
year = {2009}
}
@article{Sun2014,
author = {Sun, Yuekai and Montanari, Andrea and Edu, Montanari Stanford},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/sunb14.pdf:pdf},
title = {{Learning Mixtures of Linear Classifiers}},
volume = {32},
year = {2014}
}
@article{Haeffele2014,
author = {Haeffele, Benjamin D and Young, Eric D and Edu, Eyoung J H U},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/haeffele14.pdf:pdf},
title = {{Structured Low-Rank Matrix Factorization : Optimality , Algorithm , and Applications to Image Processing}},
volume = {32},
year = {2014}
}
@article{Hsieh2014,
author = {Hsieh, Cho-jui and Si, Si and Dhillon, Inderjit S},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/hsieha14.pdf:pdf},
title = {{A Divide-and-Conquer Solver for Kernel Support Vector Machines}},
volume = {32},
year = {2014}
}
@article{Zhang2014,
author = {Zhang, Ting and Wang, Jingdong and Com, Jingdw Microsoft},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/zhangd14.pdf:pdf},
keywords = {Composite quantization, hashing, compact code, nea},
title = {{Composite Quantization for Approximate Nearest Neighbor Search}},
volume = {32},
year = {2014}
}
@inproceedings{Mathias2014,
author = {Mathias, Markus and Benenson, Rodrigo and Pedersoli, Marco and Gool, Luc Van},
booktitle = {European Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/2014{\_}eccv{\_}face{\_}detection{\_}with{\_}supplementary{\_}material.pdf:pdf},
title = {{Face detection without the bells and whistles}},
year = {2014}
}
@inproceedings{Zeiler2012a,
author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and LeCun, Yann and Fergus, Rob},
booktitle = {International Conference of Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/dropc.pdf:pdf},
title = {{Regularization of Neural Networks using DropConnect}},
year = {2013}
}
@inproceedings{Shalev-Shwartz2008,
author = {Shalev-Shwartz, Shai and Srebro, Nathan},
booktitle = {International Conference on Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/266.pdf:pdf},
title = {{SVM Optimization : Inverse Dependence on Training Set Size}},
year = {2008}
}
@inproceedings{Tropp,
author = {Tropp, Joel A},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/nips-paper 2014.pdf:pdf},
pages = {1--9},
title = {{Time – Data Tradeo ff s by Aggressive Smoothing}}
}
@article{Yuan2014,
author = {Yuan, Yuan and Au, Oscar C and Zheng, Amin and Yang, Haitao and Tang, Ketan and Sun, Wenxiu},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/06853954.pdf:pdf},
isbn = {9781479928934},
pages = {2025--2029},
title = {{IMAGE COMPRESSION VIA SPARSE RECONSTRUCTION The Hong Kong University of Science and Technology Huawei Technologies Co ., Ltd}},
year = {2014}
}
@article{Liu,
author = {Liu, Lingqiao and Shen, Chunhua and Wang, Lei and Hengel, Anton Van Den and Wang, Chao},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/5285-encoding-high-dimensional-local-features-by-sparse-coding-based-fisher-vectors.pdf:pdf},
pages = {1--9},
title = {{Encoding High Dimensional Local Features by Sparse Coding Based Fisher Vectors}}
}
@article{Henot,
author = {Henot, Jean-pierre and Ropert, Micha{\"{e}}l and Tanou, Julien Le and Kypr{\'{e}}os, Jean and Guionnet, Thomas},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/06621675.pdf:pdf},
pages = {1--6},
title = {{High Efficiency Video Coding ( HEVC ): replacing or complementing existing compression standards ?}}
}
@article{He2013,
abstract = {In this paper, we propose a new unsupervised feature learning framework, namely Deep Sparse Coding (DeepSC), that extends sparse coding to a multi-layer architecture for visual object recognition tasks. The main innovation of the framework is that it connects the sparse-encoders from different layers by a sparse-to-dense module. The sparse-to-dense module is a composition of a local spatial pooling step and a low-dimensional embedding process, which takes advantage of the spatial smoothness information in the image. As a result, the new method is able to learn several levels of sparse representation of the image which capture features at a variety of abstraction levels and simultaneously preserve the spatial smoothness between the neighboring image patches. Combining the feature representations from multiple layers, DeepSC achieves the state-of-the-art performance on multiple object recognition tasks.},
archivePrefix = {arXiv},
arxivId = {1312.5783},
author = {He, Yunlong and Kavukcuoglu, Koray and Wang, Yun and Szlam, Arthur and Qi, Yanjun},
eprint = {1312.5783},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1312.5783v1.pdf:pdf},
month = {dec},
pages = {9},
title = {{Unsupervised Feature Learning by Deep Sparse Coding}},
url = {http://arxiv.org/abs/1312.5783},
year = {2013}
}
@article{Giraldo,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.7381v2},
author = {Giraldo, Luis G Sanchez},
eprint = {arXiv:1312.7381v2},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1312.7381v2.pdf:pdf},
title = {{Rate-Distortion Auto-Encoders}}
}
@article{Zahalka,
author = {Zah{\'{a}}lka, Jan and Worring, Marcel},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/p205-zahalka.pdf:pdf},
isbn = {9781450330633},
keywords = {deep nets,interactive city exploration,semantic concept detectors,sign,social media,topic models,user-centered de-},
pages = {205--208},
title = {{New Yorker Melange : Interactive Brew of Personalized Venue Recommendations}}
}
@article{Video2003,
author = {Video, H A V C and Standard, Compression and Marpe, Detlev and Schwarz, Heiko and Wiegand, Thomas},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/01218195.pdf:pdf},
number = {7},
pages = {620--636},
title = {{Context-Based Adaptive Binary Arithmetic Coding}},
volume = {13},
year = {2003}
}
@article{Tian2011,
address = {Berlin, Heidelberg},
author = {Tian, Xiaohua and Le, Thinh M. and Lian, Yong},
doi = {10.1007/978-3-642-14703-6},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/9783642147029-c1.pdf:pdf},
isbn = {978-3-642-14702-9},
pages = {29--40},
publisher = {Springer Berlin Heidelberg},
series = {Signals and Communication Technology},
title = {{Entropy Coders of the H.264/AVC Standard}},
url = {http://link.springer.com/10.1007/978-3-642-14703-6},
year = {2011}
}
@article{Pourazad2012,
author = {Pourazad, Mahsa T. and Doutre, Colin and Azimi, Maryam and Nasiopoulos, Panos},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/HEVC{\_}NewGoldStandard.pdf:pdf},
journal = {IEEE Consumer Electronics Magazine},
number = {July},
title = {{HEVC : The New Gold Standard for Video Compression}},
year = {2012}
}
@article{Standard2012,
author = {Standard, Hevc and Sullivan, Gary J and Ohm, Jens-rainer and Han, Woo-jin and Wiegand, Thomas},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/06316136.pdf:pdf},
number = {12},
pages = {1649--1668},
title = {{Overview of the High Efficiency Video Coding}},
volume = {22},
year = {2012}
}
@article{Guan2009,
author = {Guan, Wei and Gray, Alex and Leyffer, Sven},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/P1697.pdf:pdf},
title = {{Mixed-Integer Support Vector Machine Mixed-Integer Support Vector Machine}},
year = {2009}
}
@article{Domm2005,
author = {Domm, M. and Engel, a. and Pierre-Louis, P. and Goldberg, J.},
doi = {10.1109/SNPD-SAWN.2005.16},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/01434881.pdf:pdf},
isbn = {0-7695-2294-7},
journal = {Sixth International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing and First ACIS International Workshop on Self-Assembling Wireless Networks (SNPD/SAWN'05)},
pages = {144--149},
publisher = {Ieee},
title = {{An Integer Support Vector Machine}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1434881},
year = {2005}
}
@inproceedings{Szegedy2015,
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Computer Vision and Pattern Recognition},
eprint = {1409.4842},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1409.4842v1.pdf:pdf},
keywords = {CNN SOTA,Christopher,Himalaya,Praveen},
mendeley-tags = {CNN SOTA,Christopher,Himalaya,Praveen},
month = {sep},
pages = {1--12},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842v1},
year = {2015}
}
@article{Memisevic,
author = {Memisevic, Roland},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/6.pdf:pdf},
pages = {1--9},
title = {{On spatio-temporal sparse coding : Analysis and an algorithm}}
}
@article{Cascade-correlation1997,
author = {Cascade-correlation, Recurrent and Chunking, Neural Sequence},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Hochreiter97{\_}lstm.pdf:pdf},
number = {8},
pages = {1--32},
title = {{2 PREVIOUS WORK}},
volume = {9},
year = {1997}
}
@article{Vinyals,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.4555v1},
author = {Vinyals, Oriol and Toshev, Alexander},
eprint = {arXiv:1411.4555v1},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/1411.4555v1.pdf:pdf},
title = {{Show and Tell: A Neural Image Caption Generator}}
}
@article{Lapin,
author = {Lapin, Maksim and Schiele, Bernt and Hein, Matthias},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Lapin{\_}Scalable{\_}Multitask{\_}Representation{\_}2014{\_}CVPR{\_}paper.pdf:pdf},
title = {{Scalable Multitask Representation Learning for Scene Classification}}
}
@article{Maurer2013,
author = {Maurer, Andreas and Pontil, Massimiliano},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/maurer13.pdf:pdf},
title = {{Sparse coding for multitask and transfer learning}},
volume = {28},
year = {2013}
}
@article{Cheng2013,
author = {Cheng, Li},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Che{\_}ICML13.pdf:pdf},
title = {{Riemannian Similarity Learning}},
volume = {28},
year = {2013}
}
@article{Tanskanen2013,
author = {Tanskanen, Petri and Kolev, Kalin and Meier, Lorenz and Camposeco, Federico and Saurer, Olivier and Pollefeys, Marc},
doi = {10.1109/ICCV.2013.15},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/LiveMetric3DReconstructionICCV2013.pdf:pdf},
isbn = {978-1-4799-2840-8},
journal = {2013 IEEE International Conference on Computer Vision},
month = {dec},
pages = {65--72},
publisher = {Ieee},
title = {{Live Metric 3D Reconstruction on Mobile Phones}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751117},
year = {2013}
}
@article{Chopra,
author = {Chopra, S. and Hadsell, R. and LeCun, Y.},
doi = {10.1109/CVPR.2005.202},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/chopra-05.pdf:pdf},
isbn = {0-7695-2372-2},
journal = {2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
pages = {539--546},
publisher = {Ieee},
title = {{Learning a Similarity Metric Discriminatively, with Application to Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1467314},
volume = {1}
}
@article{Sharma,
author = {Sharma, Gaurav},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharma - Unknown - EPML Expanded Parts based Metric Learning for Occlusion Robust Face Verification.pdf:pdf},
pages = {1--15},
title = {{EPML : Expanded Parts based Metric Learning for Occlusion Robust Face Verification}}
}
@article{Bachb,
author = {Bach, Francis R and Project, Inria Willow},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/bach{\_}harchaoui{\_}2008{\_}diffrac.pdf:pdf},
title = {{DIFFRAC : a discriminative and flexible framework for clustering}}
}
@article{Philbinc,
author = {Philbin, James and Isard, Michael and Sivic, Josef and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/philbin10b.pdf:pdf},
title = {{Descriptor Learning for Efficient Retrieval}}
}
@article{Bi2011,
author = {Bi, Jinbo and Wu, Dijia and Lu, Le and Liu, Meizhu and Tao, Yimo and Wolf, Matthias},
doi = {10.1109/CVPR.2011.5995363},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/AdaBoost on low-rank PSD matrices for metric learning Bi.pdf:pdf},
isbn = {978-1-4577-0394-2},
journal = {Cvpr 2011},
month = {jun},
pages = {2617--2624},
publisher = {Ieee},
title = {{AdaBoost on low-rank PSD matrices for metric learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995363},
year = {2011}
}
@article{Sadeghi2011,
author = {Sadeghi, Mohammad Amin and Farhadi, Ali},
doi = {10.1109/CVPR.2011.5995711},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/recognition{\_}using{\_}visual{\_}phrases.pdf:pdf},
isbn = {978-1-4577-0394-2},
journal = {Cvpr 2011},
month = {jun},
pages = {1745--1752},
publisher = {Ieee},
title = {{Recognition using visual phrases}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995711},
year = {2011}
}
@article{Malisiewicz2011a,
author = {Malisiewicz, Tomasz},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/10.1.1.220.2220.pdf:pdf},
number = {August},
title = {{Exemplar-based Representations for Object Detection , Association and Beyond}},
year = {2011}
}
@inproceedings{Malisiewicza,
author = {Malisiewicz, Tomasz and Shrivastava, Abhinav and Gupta, Abhinav and Efros, Alexei A.},
booktitle = {International Conference of Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/946.pdf:pdf},
title = {{Exemplar-SVMs for Visual Object Detection , Label Transfer and Image Retrieval}},
year = {2012}
}
@misc{,
title = {{Deep-er Kernels}},
url = {http://videolectures.net/roks2013{\_}shawe{\_}taylor{\_}kernels/}
}
@article{Cho,
author = {Cho, Youngmin and Saul, Lawrence K},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cho, Saul - Unknown - Kernel Methods for Deep Learning.pdf:pdf},
pages = {1--9},
title = {{Kernel Methods for Deep Learning}}
}
@article{Weningera,
author = {Weninger, Felix and Roux, Jonathan Le and Hershey, John R and Watanabe, Shinji and Electric, Mitsubishi},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weninger et al. - Unknown - Discriminative NMF and its application to single-channel source separation(2).pdf:pdf},
number = {2},
pages = {2--6},
title = {{Discriminative NMF and its application to single-channel source separation}}
}
@article{Perez2014a,
author = {P{\'{e}}rez, Patrick},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{e}}rez - 2014 - A tour of data embedding(2).pdf:pdf},
number = {6},
pages = {1--23},
title = {{A tour of data embedding}},
year = {2014}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - DISCRIMINATIVE NON-NEGATIVE MATRIX FACTORIZATION FOR SINGLE-CHANNEL SPEECH SEPARATION Department of Computer Scie(2).pdf:pdf},
title = {{DISCRIMINATIVE NON-NEGATIVE MATRIX FACTORIZATION FOR SINGLE-CHANNEL SPEECH SEPARATION Department of Computer Science and Technology Tsinghua University Department of Computer Science University of Southern California Los Angeles , CA 90089}}
}
@inproceedings{Kulkarni,
author = {Kulkarni, Praveen and Zepeda, Joaquin and Jurie, Frederic and Perez, Patrick and Chevallier, Louis},
booktitle = {International Conference on Acoustics, Speech and Signal Processing},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kulkarni et al. - Unknown - HYBRID MULTI-LAYER DEEP CNN AGGREGATOR FEATURE FOR IMAGE CLASSIFICATION Technicolor 975 avenue des Champs B.pdf:pdf},
title = {{Hybrid Multi-Layer Deep CNN / Aggregator Feature for Image Classification}},
year = {2015}
}
@inproceedings{Rana,
author = {Rana, Aakanksha and Zepeda, Joaquin and Perez, Patrick},
booktitle = {Asian Computer Vision and Pattern Recognition Workshops},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rana, Zepeda, Perez - Unknown - Feature Learning for the Image Retrieval Task.pdf:pdf},
title = {{Feature Learning for the Image Retrieval Task}},
year = {2014}
}
@article{Chen2012,
author = {Chen, Dong and Cao, Xudong and Wang, Liwei and Wen, Fang and Sun, Jian},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - Unknown - Bayesian Face Revisited A Joint Formulation.pdf:pdf},
number = {1},
title = {{Bayesian Face Revisited: A Joint Formulation}}
}
@inproceedings{Chatfielda,
author = {Chatfield, Ken and Zisserman, Andrew},
booktitle = {Asian Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatfield, Zisserman - Unknown - VISOR Towards On-the-Fly Large-Scale Object Category Retrieval.pdf:pdf},
title = {{VISOR : Towards On-the-Fly Large-Scale Object Category Retrieval}},
year = {2012}
}
@misc{GIS,
author = {Inc., Google},
title = {{Google Image Search}},
url = {https://www.google.com/images}
}
@inproceedings{Raina2007,
author = {Raina, Rajat and Battle, Alexis and Lee, Honglak and Packer, Benjamin and Ng, Andrew Y.},
booktitle = {International Conference on Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raina, Ng - 2000 - Self-taught Learning Transfer Learning from Unlabeled Data.pdf:pdf},
title = {{Self-taught Learning : Transfer Learning from Unlabeled Data}},
year = {2007}
}
@article{Li2004,
archivePrefix = {arXiv},
arxivId = {arXiv:cs/0111054v3},
author = {Li, Ming and Chen, Xin and Li, Xin and Ma, Bin and Vit, Paul M B},
eprint = {0111054v3},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2004 - The Similarity Metric.pdf:pdf},
primaryClass = {arXiv:cs},
title = {{The Similarity Metric}},
volume = {XX},
year = {2004}
}
@article{Sprechmann2012,
author = {Sprechmann, Pablo},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sprechmann - 2012 - Learning Efficient Structured Sparse Models.pdf:pdf},
title = {{Learning Efficient Structured Sparse Models}},
year = {2012}
}
@article{Sebban,
archivePrefix = {arXiv},
arxivId = {arXiv:1306.6709v4},
author = {Sebban, Marc},
eprint = {arXiv:1306.6709v4},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sebban - Unknown - A Survey on Metric Learning for Feature Vectors and Structured Data.pdf:pdf},
keywords = {(),edit distance,mahalanobis distance,metric learning,similarity learning},
title = {{A Survey on Metric Learning for Feature Vectors and Structured Data}}
}
@misc{gplus,
abstract = {Google Inc.},
author = {Inc., Google},
title = {{Google Plus Photos}},
url = {www.google.com/Photos}
}
@article{Holden2006,
author = {Holden, A J and Robbins, D J and Stewart, W J and Smith, D R and Schultz, S and Wegener, M and Linden, S and Hormann, C and Enkrich, C and Soukoulis, C M and Schurig, D and Taylor, A J and Highstrete, C and Lee, M and Averitt, R D and Markos, P and Mcpeake, D and Ramakrishna, S A and Pendry, J B and Shalaev, V M and Maksimchuk, M and Umstadter, D and Chen, W and Shen, Y R and Moloney, J V},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holden et al. - 2006 - Reducing the Dimensionality of.pdf:pdf},
number = {July},
pages = {504--507},
title = {{Reducing the Dimensionality of}},
volume = {313},
year = {2006}
}
@article{Parkhia,
author = {Parkhi, Omkar M and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew and Group, Visual Geometry},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parkhi et al. - Unknown - A Compact and Discriminative Face Track Descriptor.pdf:pdf},
title = {{A Compact and Discriminative Face Track Descriptor}}
}
@article{Hinton2006,
author = {Hinton, Geoffrey E},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinton - 2006 - Communicated by Yann Le Cun A Fast Learning Algorithm for Deep Belief Nets 500 units 500 units.pdf:pdf},
pages = {1527--1554},
title = {{Communicated by Yann Le Cun A Fast Learning Algorithm for Deep Belief Nets 500 units 500 units}},
volume = {1554},
year = {2006}
}
@article{Mehmet2011,
author = {Mehmet, G},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mehmet - 2011 - Multiple Kernel Learning Algorithms.pdf:pdf},
keywords = {kernel machines,multiple kernel learning,support vector machines},
pages = {2211--2268},
title = {{Multiple Kernel Learning Algorithms}},
volume = {12},
year = {2011}
}
@misc{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
booktitle = {International Conference on Learning Representations},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
keywords = {Deep vision,Praveen},
mendeley-tags = {Deep vision,Praveen},
month = {sep},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}
@inproceedings{Zepeda2010a,
author = {Zepeda, Joaquin and Guillemot, Christine and Kijak, Ewa},
booktitle = {International Conference on Acoustics, Speech and Signal Processing},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zepeda, Guillemot, Kijak - 2010 - Approximate nearest neighbors using sparse representations.pdf:pdf},
isbn = {9781424442966},
pages = {2370--2373},
title = {{Approximate nearest neighbors using sparse representations}},
year = {2010}
}
@inproceedings{Ge2013,
author = {Ge, Tiezheng and He, Kaiming and Ke, Qifa and Sun, Jian},
booktitle = {Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2013.379},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ge et al. - 2013 - Optimized Product Quantization for Approximate Nearest Neighbor Search.pdf:pdf},
isbn = {978-0-7695-4989-7},
month = {jun},
publisher = {Ieee},
title = {{Optimized Product Quantization for Approximate Nearest Neighbor Search}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619223},
year = {2013}
}
@article{Donahue2013,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
archivePrefix = {arXiv},
arxivId = {1310.1531},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
eprint = {1310.1531},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donahue et al. - 2013 - DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition.pdf:pdf},
month = {oct},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1310.1531},
year = {2013}
}
@article{Timofte2013,
author = {Timofte, Radu and De, Vincent and Gool, Luc Van},
doi = {10.1109/ICCV.2013.241},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Timofte, De, Gool - 2013 - Anchored Neighborhood Regression for Fast Example-Based Super-Resolution.pdf:pdf},
isbn = {978-1-4799-2840-8},
journal = {2013 IEEE International Conference on Computer Vision},
month = {dec},
pages = {1920--1927},
publisher = {Ieee},
title = {{Anchored Neighborhood Regression for Fast Example-Based Super-Resolution}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751349},
year = {2013}
}
@article{Szegedy,
author = {Szegedy, Christian and Goodfellow, Ian and Bruna, Joan and Fergus, Rob},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Szegedy et al. - Unknown - Intriguing properties of neural networks.pdf:pdf},
pages = {1--9},
title = {{Intriguing properties of neural networks}}
}
@article{Guleryuz2006,
abstract = {We combine the main ideas introduced in Part I with adaptive techniques to arrive at a powerful algorithm that estimates missing data in nonstationary signals. The proposed approach operates automatically based on a chosen linear transform that is expected to provide sparse decompositions over missing regions such that a portion of the transform coefficients over missing regions are zero or close to zero. Unlike prevalent algorithms, our method does not necessitate any complex preconditioning, segmentation, or edge detection steps, and it can be written as a progression of denoising operations. We show that constructing estimates based on nonlinear approximants is fundamentally a nonconvex problem and we propose a progressive algorithm that is designed to deal with this issue directly. The algorithm is applied to images through an extensive set of simulation examples, primarily on missing regions containing textures, edges, and other image features that are not readily handled by established estimation and recovery methods. We discuss the properties required of good transforms, and in conjunction, show the types of regions over which well-known transforms provide good predictors. We further discuss extensions of the algorithm where the utilized transforms are also chosen adaptively, where unpredictable signal components in the progressions are identified and not predicted, and where the prediction scenario is more general.},
author = {Guleryuz, Onur G},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guleryuz - 2006 - Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising--Part II Ada.pdf:pdf},
issn = {1057-7149},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {Algorithms,Artificial Intelligence,Computer Graphics,Computer Simulation,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models, Statistical,Nonlinear Dynamics,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity,Signal Processing, Computer-Assisted},
month = {mar},
number = {3},
pages = {555--71},
pmid = {16519343},
title = {{Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising--Part II: Adaptive algorithms.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16519343},
volume = {15},
year = {2006}
}
@inproceedings{He,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.4729v2},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Pattern Analysis and Machine Intelligence},
eprint = {arXiv:1406.4729v2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/He et al. - Unknown - Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition.pdf:pdf},
title = {{Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition}},
year = {2015}
}
@article{Guleryuz2006a,
abstract = {We study the robust estimation of missing regions in images and video using adaptive, sparse reconstructions. Our primary application is on missing regions of pixels containing textures, edges, and other image features that are not readily handled by prevalent estimation and recovery algorithms. We assume that we are given a linear transform that is expected to provide sparse decompositions over missing regions such that a portion of the transform coefficients over missing regions are zero or close to zero. We adaptively determine these small magnitude coefficients through thresholding, establish sparsity constraints, and estimate missing regions in images using information surrounding these regions. Unlike prevalent algorithms, our approach does not necessitate any complex preconditioning, segmentation, or edge detection steps, and it can be written as a sequence of denoising operations. We show that the region types we can effectively estimate in a mean-squared error sense are those for which the given transform provides a close approximation using sparse nonlinear approximants. We show the nature of the constructed estimators and how these estimators relate to the utilized transform and its sparsity over regions of interest. The developed estimation framework is general, and can readily be applied to other nonstationary signals with a suitable choice of linear transforms. Part I discusses fundamental issues, and Part II is devoted to adaptive algorithms with extensive simulation examples that demonstrate the power of the proposed techniques.},
author = {Guleryuz, Onur G},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guleryuz - 2006 - Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising--Part I Theo.pdf:pdf},
issn = {1057-7149},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {Algorithms,Artificial Intelligence,Computer Graphics,Computer Simulation,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models, Statistical,Nonlinear Dynamics,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity,Signal Processing, Computer-Assisted},
month = {mar},
number = {3},
pages = {539--54},
pmid = {16519342},
title = {{Nonlinear approximation based image recovery using adaptive sparse reconstructions and iterated denoising--Part I: Theory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16519342},
volume = {15},
year = {2006}
}
@article{Nagi2012,
author = {Nagi, Jawad and Caro, Gianni a. Di and Giusti, Alessandro and Nagi, Farrukh and Gambardella, Luca M.},
doi = {10.1109/ICMLA.2012.14},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nagi et al. - 2012 - Convolutional Neural Support Vector Machines Hybrid Visual Pattern Classifiers for Multi-robot Systems.pdf:pdf},
isbn = {978-1-4673-4651-1},
journal = {2012 11th International Conference on Machine Learning and Applications},
month = {dec},
pages = {27--32},
publisher = {Ieee},
title = {{Convolutional Neural Support Vector Machines: Hybrid Visual Pattern Classifiers for Multi-robot Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6406584},
year = {2012}
}
@inproceedings{Oquaba,
author = {Oquab, M. and Bottou, L. and Laptev, I. and Sivic, J.},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab, Bottou - Unknown - Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks.pdf:pdf},
title = {{Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks}},
year = {2014}
}
@article{Peyr,
author = {Peyr, Gabriel},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Peyr - Unknown - No Title.pdf:pdf},
keywords = {analysis prior,dictionary learning,total varia-},
number = {1},
pages = {2--5},
title = {{No Title}},
volume = {2}
}
@article{Sanchez2011,
author = {Sanchez, Jorge and Perronnin, Florent},
doi = {10.1109/CVPR.2011.5995504},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sanchez, Perronnin - 2011 - High-dimensional signature compression for large-scale image classification.pdf:pdf},
isbn = {978-1-4577-0394-2},
journal = {Cvpr 2011},
month = {jun},
pages = {1665--1672},
publisher = {Ieee},
title = {{High-dimensional signature compression for large-scale image classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5995504},
year = {2011}
}
@article{Wangb,
author = {Wang, Huahua},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - Unknown - Learning with Application to Novel Document Detection.pdf:pdf},
pages = {1--9},
title = {{Learning with Application to Novel Document Detection}}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - FAST ONLINE L 1 -DICTIONARY LEARNING ALGORITHMS FOR NOVEL DOCUMENT DETECTION Shiva Prasad Kasiviswanathan General El.pdf:pdf},
pages = {0--4},
title = {{FAST ONLINE L 1 -DICTIONARY LEARNING ALGORITHMS FOR NOVEL DOCUMENT DETECTION Shiva Prasad Kasiviswanathan General Electric Research , San Ramon , California , kasivisw@gmail.com}}
}
@article{Inpainting2014,
author = {Inpainting, Sparsity-based Image and Li, Fang and Zeng, Tieyong},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inpainting, Li, Zeng - 2014 - A Universal Variational Framework for.pdf:pdf},
number = {10},
pages = {4242--4254},
title = {{A Universal Variational Framework for}},
volume = {23},
year = {2014}
}
@article{Fadili2008,
author = {Fadili, M.J. and Starck, J.-L. and Murtagh, F.},
doi = {10.1093/comjnl/bxm055},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fadili, Starck, Murtagh - 2008 - Inpainting and Zooming Using Sparse Representations.pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
keywords = {em algorithm,inpainting,interpolation,penalized likelihood,received 30 june 2006,revised 15 february 2007,sparse representations},
month = {feb},
number = {1},
pages = {64--79},
title = {{Inpainting and Zooming Using Sparse Representations}},
url = {http://comjnl.oxfordjournals.org/cgi/doi/10.1093/comjnl/bxm055},
volume = {52},
year = {2008}
}
@article{Simonyan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6034v2},
author = {Simonyan, Karen},
eprint = {arXiv:1312.6034v2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps arXiv 1312 . 6034v2 cs.pdf:pdf},
pages = {1--8},
title = {{Deep Inside Convolutional Networks : Visualising Image Classification Models and Saliency Maps arXiv : 1312 . 6034v2 [ cs . CV ] 19 Apr 2014}},
year = {2013}
}
@article{Ruvoloa,
author = {Ruvolo, Paul and Olin, Franklin W and Eaton, Eric},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruvolo, Olin, Eaton - Unknown - Online Multi-Task Learning via Sparse Dictionary Optimization Multi-Task Learning Using K-SVD.pdf:pdf},
number = {1},
title = {{Online Multi-Task Learning via Sparse Dictionary Optimization Multi-Task Learning Using K-SVD}}
}
@article{Ruvolo,
author = {Ruvolo, Paul and Eaton, Eric},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ruvolo, Eaton - Unknown - Online Multi-Task Learning based on K-SVD.pdf:pdf},
title = {{Online Multi-Task Learning based on K-SVD}}
}
@article{Field2012,
abstract = {A wide variety of recent studies have argued that the human visual system provides an efficient means of processing the information in the natural environment. However, the amount of information (entropy) in the signal can be estimated in a number of ways, and it is has been unclear how much of the information is carried by the different sources of redundancy. The primary difficulty is that there has been no rational way to estimate the entropy of such complex scenes. In this paper, we provide a technique that uses a recent approach to estimating the entropy and dimensionality of natural scenes [D. M. Chandler and D. J. Field, J. Opt. Soc. Am. A 24, 922-941 (2007)] to estimate the amount of information attributable to the power and phase spectra in natural-scene patches. By comparing the entropies of patches that have swapped phase spectra and fixed phase spectra, we demonstrate how to estimate both the amount of information in each type of spectrum and the amount of information that is shared by these spectra (mutual information). We applied this technique to small patches (4×4 and 8×8). From our estimates, we show that the power spectrum of 8×8 patches carries approximately 54{\%} of the total information, the phase spectrum carries 56{\%}, and 10{\%} is mutual information (54{\%}+56{\%}-10{\%}=100{\%}). This technique is currently limited to relatively small image patches, due to the number of patches currently in our collection (on the order of 10⁶). However, the technique can, in theory, be extended to larger images. Even with these relatively small patches, we discuss how these results can provide important insights into both compression techniques and efficient coding techniques that work with relatively small image patches (e.g., JPEG, sparse coding, independent components analysis).},
author = {Field, David J and Chandler, Damon M},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Field, Chandler - 2012 - Method for estimating the relative contribution of phase and power spectra to the total information in natural-.pdf:pdf},
issn = {1520-8532},
journal = {Journal of the Optical Society of America. A, Optics, image science, and vision},
keywords = {Entropy,Humans,Light,Photic Stimulation,Spectrum Analysis,Visual Perception},
month = {jan},
number = {1},
pages = {55--67},
pmid = {22218351},
title = {{Method for estimating the relative contribution of phase and power spectra to the total information in natural-scene patches.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22218351},
volume = {29},
year = {2012}
}
@article{Chandler2007,
abstract = {Natural scenes, like most all natural data sets, show considerable redundancy. Although many forms of redundancy have been investigated (e.g., pixel distributions, power spectra, contour relationships, etc.), estimates of the true entropy of natural scenes have been largely considered intractable. We describe a technique for estimating the entropy and relative dimensionality of image patches based on a function we call the proximity distribution (a nearest-neighbor technique). The advantage of this function over simple statistics such as the power spectrum is that the proximity distribution is dependent on all forms of redundancy. We demonstrate that this function can be used to estimate the entropy (redundancy) of 3x3 patches of known entropy as well as 8x8 patches of Gaussian white noise, natural scenes, and noise with the same power spectrum as natural scenes. The techniques are based on assumptions regarding the intrinsic dimensionality of the data, and although the estimates depend on an extrapolation model for images larger than 3x3, we argue that this approach provides the best current estimates of the entropy and compressibility of natural-scene patches and that it provides insights into the efficiency of any coding strategy that aims to reduce redundancy. We show that the sample of 8x8 patches of natural scenes used in this study has less than half the entropy of 8x8 white noise and less than 60{\%} of the entropy of noise with the same power spectrum. In addition, given a finite number of samples ({\textless}2(20)) drawn randomly from the space of 8x8 patches, the subspace of 8x8 natural-scene patches shows a dimensionality that depends on the sampling density and that for low densities is significantly lower dimensional than the space of 8x8 patches of white noise and noise with the same power spectrum.},
author = {Chandler, Damon M and Field, David J},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandler, Field - 2007 - Estimates of the information content and dimensionality of natural scenes from proximity distributions.pdf:pdf},
issn = {1084-7529},
journal = {Journal of the Optical Society of America. A, Optics, image science, and vision},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Information Theory,Models, Statistical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Statistical Distributions},
month = {apr},
number = {4},
pages = {922--41},
pmid = {17361279},
title = {{Estimates of the information content and dimensionality of natural scenes from proximity distributions.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17361279},
volume = {24},
year = {2007}
}
@article{Bon-wooHwang2003,
author = {{Bon-woo Hwang} and Lee, Seong-whan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bon-woo Hwang, Lee - 2003 - Face Reconstruction with a Morphable Face Model.pdf:pdf},
number = {3},
pages = {365--372},
title = {{Face Reconstruction with a Morphable Face Model}},
volume = {25},
year = {2003}
}
@article{Hwang2003,
author = {Hwang, Bon-Woo and Lee, Seong-Whan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hwang, Lee - 2003 - Reconstruction of partially damaged face images based on a morphable face model.pdf:pdf},
journal = {IEEE transactions on pattern analysis and machine intelligence},
number = {3},
pages = {365--372},
title = {{Reconstruction of partially damaged face images based on a morphable face model}},
volume = {25},
year = {2003}
}
@article{Conference2014,
author = {Conference, Ieee International and Processing, Signal},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Conference, Processing - 2014 - TRANSFORMATION-INVARIANT DICTIONARY LEARNING FOR CLASSIFICATION WITH 1-SPARSE REPRESENTATIONS Ahmet Cane.pdf:pdf},
isbn = {9781479928934},
pages = {3586--3590},
title = {{TRANSFORMATION-INVARIANT DICTIONARY LEARNING FOR CLASSIFICATION WITH 1-SPARSE REPRESENTATIONS Ahmet Caner Y ¨ uz ¨ ug ¨ Middle East Technical University Dept . of Electrical and Electronics Engineering Elif Vural and Pascal Frossard Ecole Polytechnique F }},
year = {2014}
}
@article{Fei-Fei2006,
abstract = {Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.},
author = {Fei-Fei, Li and Fergus, Rob and Perona, Pietro},
doi = {10.1109/TPAMI.2006.79},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fei-Fei, Fergus, Perona - 2006 - One-shot learning of object categories.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Bayes Theorem,Cluster Analysis,Computer Simulation,Image Enhancement,Image Enhancement: methods,Image Interpretation, Computer-Assisted,Image Interpretation, Computer-Assisted: methods,Imaging, Three-Dimensional,Imaging, Three-Dimensional: methods,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models, Biological,Models, Statistical,Pattern Recognition, Automated,Pattern Recognition, Automated: methods,Reproducibility of Results,Sensitivity and Specificity},
month = {apr},
number = {4},
pages = {594--611},
pmid = {16566508},
title = {{One-shot learning of object categories.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16566508},
volume = {28},
year = {2006}
}
@article{Lu2013,
author = {Lu, Chaochao and Zhao, Deli and Tang, Xiaoou},
doi = {10.1109/ICCV.2013.408},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu, Zhao, Tang - 2013 - Face Recognition Using Face Patch Networks.pdf:pdf},
isbn = {978-1-4799-2840-8},
journal = {2013 IEEE International Conference on Computer Vision},
month = {dec},
pages = {3288--3295},
publisher = {Ieee},
title = {{Face Recognition Using Face Patch Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751520},
year = {2013}
}
@inproceedings{Lu,
archivePrefix = {arXiv},
arxivId = {arXiv:1404.3840v2},
author = {Lu, Chaochao and Tang, Xiaoou},
booktitle = {AAAI Conference on Artificial Intelligence},
eprint = {arXiv:1404.3840v2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lu - Unknown - Surpassing Human-Level Face Verification Performance on LFW with GaussianFace.pdf:pdf},
title = {{Surpassing Human-Level Face Verification Performance on LFW with GaussianFace}},
year = {2015}
}
@inproceedings{Girshick2014,
author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra and Berkeley, U C},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Girshick et al. - 2012 - Rich feature hierarchies for accurate object detection and semantic segmentation.pdf:pdf},
title = {{Rich feature hierarchies for accurate object detection and semantic segmentation}},
year = {2014}
}
@misc{Mermin,
author = {Mermin, N. David},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mermin - Unknown - MERMIN What's wrong with these equations.pdf:pdf},
title = {{MERMIN: What's wrong with these equations}}
}
@misc{,
title = {{Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning}},
url = {https://peerj.com/articles/488/}
}
@article{Jenatton2010a,
author = {Jenatton, Rodolphe and Obozinski, Guillaume and Bach, Francis},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jenatton, Obozinski, Bach - 2010 - Structured Sparse Principal Component Analysis.pdf:pdf},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {366--373},
title = {{Structured Sparse Principal Component Analysis}},
volume = {9},
year = {2010}
}
@inproceedings{Yang2008,
author = {Yang, Jianchao and Tang, Hao and Ma, Yi and Huang, T.},
booktitle = {IEEE International Conference on Image Processing},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2008 - Face Hallucination via Sparse Coding.pdf:pdf},
number = {1},
pages = {1264--1267},
title = {{Face Hallucination via Sparse Coding}},
year = {2008}
}
@book{Luenberger,
author = {Luenberger, David G.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luenberger - Unknown - Linear and non-linear programming.pdf:pdf},
isbn = {9780387745022},
title = {{Linear and non-linear programming}}
}
@article{Aharon2008,
author = {Aharon, Michal and Elad, Michael},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aharon, Elad - 2008 - Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary.pdf:pdf},
keywords = {07070156x,10,1137,62h35,68u10,ams subject classifications,denoising,dictionary,doi,image-signature,learning,matching pursuit,mod,sparse representation},
number = {3},
pages = {228--247},
title = {{Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary}},
volume = {1},
year = {2008}
}
@inproceedings{Bousquet,
author = {Bottou, L{\'{e}}on and Bousquet, Olivier},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bousquet, Google - Unknown - The Tradeoffs of Large Scale Learning.pdf:pdf},
title = {{The Tradeoffs of Large Scale Learning}},
year = {2007}
}
@inproceedings{Mairal2009,
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {International Conference on Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal et al. - 2009 - Online Dictionary Learning for Sparse Coding.pdf:pdf},
title = {{Online Dictionary Learning for Sparse Coding}},
year = {2009}
}
@article{Leec,
author = {Lee, Honglak and Ng, Andrew Y},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Ng - Unknown - Efficient sparse coding algorithms(3).pdf:pdf},
title = {{Efficient sparse coding algorithms}}
}
@phdthesis{Alejandro2006,
author = {Zepeda, Joaquin},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alejandro, Salvatierra - 2006 - Tandem Filterbank DFT Code for Burst.pdf:pdf},
isbn = {9780494286371},
number = {August},
school = {McGill University},
title = {{Tandem Filterbank / DFT Code for Bursty Erasure Correction}},
type = {Master's Thesis},
year = {2006}
}
@article{Romaniuk,
author = {Romaniuk, Michal P and Rao, Anil W and Wolz, Robin and Hajnal, Joseph V},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Romaniuk et al. - Unknown - Compressed sensing Wavelet packets.pdf:pdf},
title = {{Compressed sensing Wavelet packets}}
}
@article{Rustamov,
author = {Rustamov, Raif M and Guibas, Leonidas},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rustamov, Guibas - Unknown - Wavelets on Graphs via Deep Learning.pdf:pdf},
pages = {1--9},
title = {{Wavelets on Graphs via Deep Learning}}
}
@article{Chechik2010,
author = {Chechik, Gal and Sharma, Varun and Shalit, Uri and Bengio, Samy},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chechik, Shalit - Unknown - Large Scale Online Learning of Image Similarity Through Ranking.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1109--1135},
title = {{Large Scale Online Learning of Image Similarity Through Ranking}},
year = {2010}
}
@phdthesis{Zepeda2010,
author = {Zepeda, Joaquin},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zepeda - 2010 - Nouvelles m{\'{e}}thodes de repr{\'{e}}sentations parcimonieuses Application {\`{a}} la compression et l'indexation d'images.pdf:pdf},
school = {Universit{\'{e}} de Rennes 1 (INRIA)},
title = {{Nouvelles m{\'{e}}thodes de repr{\'{e}}sentations parcimonieuses; Application {\`{a}} la compression et l'indexation d'images}},
year = {2010}
}
@inproceedings{Zepeda2011a,
author = {Zepeda, Joaquin and Guillemot, Christine and Kijak, Ewa},
booktitle = {International Conference on Acoustics, Speech and Signal Processing},
pages = {793--796},
title = {{Image compression using the Iteration-Tuned and Aligned Dictionary}},
year = {2011}
}
@article{Zepeda2011,
author = {Zepeda, Joaquin and Guillemot, Christine and Kijak, Ewa},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Zepeda, Guillemot, Kijak - 2011 - Image Compression Using Sparse Representations and the Iteration-Tuned and Aligned Dictionary.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {5},
pages = {1061--1073},
title = {{Image Compression Using Sparse Representations and the Iteration-Tuned and Aligned Dictionary}},
volume = {5},
year = {2011}
}
@article{Aharon2006,
author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aharon, Elad, Bruckstein - 2006 - K -SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {11},
pages = {4311--4322},
title = {{K -SVD : An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
volume = {54},
year = {2006}
}
@techreport{Bacha,
author = {Bach, Francis},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach - 2010 - Sparse methods for machine learning Theory and algorithms.pdf:pdf},
keywords = {Christopher},
mendeley-tags = {Christopher},
title = {{Sparse methods for machine learning Theory and algorithms}},
url = {http://www.di.ens.fr/{~}fbach/Cours{\_}peyresq{\_}2010.pdf},
year = {2010}
}
@misc{,
title = {{YouTube Faces DB}},
url = {http://www.cs.tau.ac.il/{~}wolf/ytfaces/index.html{\#}download}
}
@article{Mo2004,
author = {Mo, Z. and Lewis, J.P. and Neumann, U.},
doi = {10.5244/C.18.37},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mo, Lewis, Neumann - 2004 - Face Inpainting with Local Linear Representations.pdf:pdf},
isbn = {1-901725-25-1},
journal = {Procedings of the British Machine Vision Conference 2004},
pages = {37.1--37.10},
publisher = {British Machine Vision Association},
title = {{Face Inpainting with Local Linear Representations}},
url = {http://www.bmva.org/bmvc/2004/papers/paper{\_}129.html},
year = {2004}
}
@article{Mignon2013,
author = {Mignon, Alexis and Jurie, Frederic},
doi = {10.5244/C.27.103},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mignon, Jurie - 2013 - Reconstructing faces from their signatures using RBF regression.pdf:pdf},
isbn = {1-901725-49-9},
journal = {Procedings of the British Machine Vision Conference 2013},
pages = {103.1--103.11},
publisher = {British Machine Vision Association},
title = {{Reconstructing faces from their signatures using RBF regression}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0103/index.html},
year = {2013}
}
@article{Wright2009c,
abstract = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by l{\{}1{\}}-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as Eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
author = {Wright, John and Yang, Allen Y. and Ganesh, Arvind and Sastry, S. Shankar and Ma, Yi},
doi = {10.1109/TPAMI.2008.79},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright et al. - 2009 - Robust face recognition via sparse representation.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Biometry,Biometry: methods,Cluster Analysis,Computer-Assisted,Computer-Assisted: methods,Face,Face: anatomy {\&} histology,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique},
month = {feb},
number = {2},
pages = {210--27},
pmid = {19110489},
title = {{Robust Face Recognition via Sparse Representations}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21646680},
volume = {31},
year = {2008}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - IMAGE COMPRESSION USING LEARNED DICTIONARIES BY RLS-DLA AND COMPARED WITH K-SVD Karl Skretting and Kjersti Engan Uni.pdf:pdf},
number = {2},
pages = {0--3},
title = {{IMAGE COMPRESSION USING LEARNED DICTIONARIES BY RLS-DLA AND COMPARED WITH K-SVD Karl Skretting and Kjersti Engan University of Stavanger Department of Electrical Engineering and Computer Science}}
}
@article{Horev,
author = {Horev, Inbal and Bryt, Ori},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Horev, Bryt - Unknown - Adaptive Image Compression Using Sparse Dictionaries.pdf:pdf},
keywords = {dictionary learning,image coding,image compression,sparse representation},
pages = {1--17},
title = {{Adaptive Image Compression Using Sparse Dictionaries}}
}
@article{Bryt2008,
author = {Bryt, Ori and Elad, Michael},
doi = {10.1016/j.jvcir.2008.03.001},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bryt, Elad - 2008 - Compression of facial images using the K-SVD algorithm.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
month = {may},
number = {4},
pages = {270--282},
title = {{Compression of facial images using the K-SVD algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1047320308000254},
volume = {19},
year = {2008}
}
@article{JoaquinZepedaChristineGuillemot2011,
author = {Zepeda, Joaquin and Guillemot, Christine and Kijak, Ewa},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/Zepeda, Guillemot, Kijak - 2011 - Image Compression Using Sparse Representations and the Iteration-Tuned and Aligned Dictionary.pdf:pdf},
journal = {IEEE Journal of Selected Topics in Signal Processing},
number = {5},
pages = {1061--1073},
title = {{Image Compression Using Sparse Representations and the Iteration-Tuned and Aligned Dictionary}},
volume = {5},
year = {2011}
}
@inproceedings{JoaquinZepedaChristineGuillemot2011a,
address = {Prague},
author = {Zepeda, Joaquin and Guillemot, Christine and Kijak, Ewa},
booktitle = {ICASSP},
pages = {793--796},
title = {{Image compression using the Iteration-Tuned and Aligned Dictionary}},
year = {2011}
}
@article{Giusti2013,
author = {Giusti, Alessandro and Cire, Dan C and Masci, Jonathan and Gambardella, Luca M},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giusti et al. - 2013 - Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks san urgen Schmidhuber Fast Image Scanning.pdf:pdf},
title = {{Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks san urgen Schmidhuber Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks san}},
year = {2013}
}
@article{Milanfar2013,
author = {Milanfar, Peyman},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milanfar - 2013 - A Tour of Modern Image Filtering.pdf:pdf},
number = {january},
pages = {106--128},
title = {{A Tour of Modern Image Filtering}},
year = {2013}
}
@article{Mignon2012,
author = {Mignon, Alexis and Umr, Cnrs},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mignon, Umr - 2012 - PCCA A New Approach for Distance Learning from Sparse Pairwise Constraints.pdf:pdf},
title = {{PCCA : A New Approach for Distance Learning from Sparse Pairwise Constraints}},
year = {2012}
}
@article{Sermanet,
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6229v4},
author = {Sermanet, Pierre and Eigen, David},
eprint = {arXiv:1312.6229v4},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sermanet, Eigen - Unknown - OverFeat Integrated Recognition , Localization and Detection using Convolutional Networks arXiv 1312 . 622.pdf:pdf},
title = {{OverFeat : Integrated Recognition , Localization and Detection using Convolutional Networks arXiv : 1312 . 6229v4 [ cs . CV ] 24 Feb 2014}}
}
@techreport{,
annote = {ICLR - 
ACCV - Asian Conference on Computer Vision

      },
title = {{List of relevant conferences}}
}
@misc{Jegou,
author = {Jegou, Herve},
title = {{INRIA Holidays dataset}},
url = {http://lear.inrialpes.fr/people/jegou/data.php}
}
@article{Oquab,
author = {Oquab, Maxime and Laptev, Ivan and Sivic, Josef},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Oquab, Laptev, Sivic - Unknown - Weakly supervised object recognition with convolutional neural networks(2).pdf:pdf},
pages = {1--10},
title = {{Weakly supervised object recognition with convolutional neural networks}}
}
@inproceedings{Krizhevsky2012,
author = {Krizhevsky, Alex and Sutskever, I. and Hinton, Geoffrey},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
keywords = {Christopher,Deep vision},
mendeley-tags = {Christopher,Deep vision},
pages = {1--9},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Mikolajczyk2005,
author = {Mikolajczyk, K. and Tuytelaars, T. and Schmid, C. and Zisserman, a. and Matas, J. and Schaffalitzky, F. and Kadir, T. and Gool, L. Van},
doi = {10.1007/s11263-005-3848-x},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolajczyk et al. - 2005 - A Comparison of Affine Region Detectors.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {oct},
number = {1-2},
pages = {43--72},
title = {{A Comparison of Affine Region Detectors}},
url = {http://link.springer.com/10.1007/s11263-005-3848-x},
volume = {65},
year = {2005}
}
@article{Mikolajczyk,
author = {Mikolajczyk, Krystian and Schmid, Cordelia},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolajczyk, Schmid - Unknown - An affine invariant interest point detector.pdf:pdf},
keywords = {image features,matching,recognition},
title = {{An affine invariant interest point detector}}
}
@article{Kadir,
author = {Kadir, Timor and Zisserman, Andrew and Brady, Michael},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kadir, Zisserman, Brady - Unknown - An affine invariant salient region detector.pdf:pdf},
title = {{An affine invariant salient region detector}}
}
@article{Lowe2004,
author = {Lowe, David G.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lowe - 2004 - Distinctive image features from scale-invariant keypoints.pdf:pdf},
journal = {International Journal of Computer Vision},
number = {2},
pages = {91--110},
title = {{Distinctive image features from scale-invariant keypoints}},
volume = {60},
year = {2004}
}
@article{Wright2009,
author = {Wright, Stephen J and Nowak, Robert D and Member, Senior and Figueiredo, M{\'{a}}rio A T},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright et al. - 2009 - Sparse Reconstruction by Separable Approximation.pdf:pdf},
number = {7},
pages = {2479--2493},
title = {{Sparse Reconstruction by Separable Approximation}},
volume = {57},
year = {2009}
}
@article{Gosselin2013,
author = {Gosselin, Philippe-henri},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gosselin - 2013 - Inria Xerox @ FGcomp Boosting the Fisher vector for fine-grained classification.pdf:pdf},
number = {December},
title = {{Inria + Xerox @ FGcomp : Boosting the Fisher vector for fine-grained classification}},
year = {2013}
}
@article{Tolias2014,
author = {Tolias, Giorgos and J{\'{e}}gou, Herv{\'{e}}},
doi = {10.1016/j.patcog.2014.04.007},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tolias, J{\'{e}}gou - 2014 - Visual query expansion with or without geometry Refining local descriptors by feature aggregation.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {hamming embedding,image retrieval,query expansion},
month = {oct},
number = {10},
pages = {3466--3476},
title = {{Visual query expansion with or without geometry: Refining local descriptors by feature aggregation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320314001381},
volume = {47},
year = {2014}
}
@article{Wang,
author = {Wang, Wei and Huang, Yan and Wang, Yizhou and Wang, Liang},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Generalized Autoencoder A Neural Network Framework for Dimensionality Reduction.pdf:pdf},
pages = {490--497},
title = {{Generalized Autoencoder : A Neural Network Framework for Dimensionality Reduction}}
}
@inproceedings{Sharif,
author = {Razavian, Ali Sharif and Azizpour, Hossein and Sullivan, Josephine and Carlsson, Stefan},
booktitle = {Computer Vision and Pattern Recognition Workshops},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sharif et al. - Unknown - CNN Features off-the-shelf an Astounding Baseline for Recognition.pdf:pdf},
title = {{CNN Features off-the-shelf : an Astounding Baseline for Recognition}},
year = {2014}
}
@inproceedings{Sydorov2014,
author = {Sydorov, Vladyslav and Sakurada, Mayu and Lampert, Christoph},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sydorov, Sakurada, Lampert - 2014 - Deep Fisher Kernels–End to End Learning of the Fisher Kernel GMM Parameters.pdf:pdf},
title = {{Deep Fisher Kernels - End to End Learning of the Fisher Kernel GMM Parameters}},
url = {http://ist.ac.at/{~}chl/papers/sydorov-cvpr2014.pdf},
year = {2014}
}
@article{Lim2013,
author = {Lim, Joseph J. and Zitnick, C. Lawrence and Dollar, Piotr},
doi = {10.1109/CVPR.2013.406},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lim, Zitnick, Dollar - 2013 - Sketch Tokens A Learned Mid-level Representation for Contour and Object Detection.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {Computer Vision and Pattern Recognition},
month = {jun},
pages = {3158--3165},
publisher = {Ieee},
title = {{Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619250},
year = {2013}
}
@article{Zhang2012,
author = {Zhang, Jian and Zhao, Chen and Xiong, Ruiqin and Ma, Siwei and Zhao, Debin},
doi = {10.1109/ISCAS.2012.6271583},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2012 - Image super-resolution via dual-dictionary learning and sparse representation.pdf:pdf},
isbn = {978-1-4673-0219-7},
journal = {2012 IEEE International Symposium on Circuits and Systems},
month = {may},
pages = {1688--1691},
publisher = {Ieee},
title = {{Image super-resolution via dual-dictionary learning and sparse representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6271583},
year = {2012}
}
@article{Wright2008,
author = {Wright, John and Huang, Thomas},
doi = {10.1109/CVPR.2008.4587647},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright, Huang - 2008 - Image super-resolution as sparse representation of raw image patches.pdf:pdf},
isbn = {978-1-4244-2242-5},
journal = {2008 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {1--8},
publisher = {Ieee},
title = {{Image super-resolution as sparse representation of raw image patches}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587647},
year = {2008}
}
@article{Yang2010,
abstract = {This paper presents a new approach to single-image super-resolution, based on sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image patches, we can enforce the similarity of sparse representations between the low resolution and high resolution image patch pair with respect to their own dictionaries. Therefore, the sparse representation of a low resolution image patch can be applied with the high resolution image patch dictionary to generate a high resolution image patch. The learned dictionary pair is a more compact representation of the patch pairs, compared to previous approaches, which simply sample a large amount of image patch pairs, reducing the computational cost substantially. The effectiveness of such a sparsity prior is demonstrated for both general image super-resolution and the special case of face hallucination. In both cases, our algorithm generates high-resolution images that are competitive or even superior in quality to images produced by other similar SR methods. In addition, the local sparse modeling of our approach is naturally robust to noise, and therefore the proposed algorithm can handle super-resolution with noisy inputs in a more unified framework.},
author = {Yang, Jianchao and Wright, John and Huang, Thomas S and Ma, Yi},
doi = {10.1109/TIP.2010.2050625},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang et al. - 2010 - Image super-resolution via sparse representation.pdf:pdf},
issn = {1941-0042},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
month = {nov},
number = {11},
pages = {2861--73},
pmid = {20483687},
title = {{Image super-resolution via sparse representation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20483687},
volume = {19},
year = {2010}
}
@article{Lin,
author = {Lin, Di and Lu, Cewu and Jia, Jiaya},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Lu, Jia - Unknown - Learning Important Spatial Pooling Regions for Scene Classification.pdf:pdf},
number = {i},
title = {{Learning Important Spatial Pooling Regions for Scene Classification}}
}
@inproceedings{Jia2014,
author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
booktitle = {ACM Multimedia},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature Embedding ∗ Categories and Subject Descriptors.pdf:pdf},
keywords = {computation,computer vision,corresponding authors,machine learning,neural networks,open source,parallel},
title = {{Caffe : Convolutional Architecture for Fast Feature Embedding}},
year = {2014}
}
@article{Bilen,
author = {Bilen, Hakan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bilen - Unknown - Object Classification with Adaptable Regions.pdf:pdf},
title = {{Object Classification with Adaptable Regions}}
}
@article{Weninger,
author = {Weninger, Felix and Roux, Jonathan Le and Hershey, John R and Watanabe, Shinji and Electric, Mitsubishi},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weninger et al. - Unknown - Discriminative NMF and its application to single-channel source separation.pdf:pdf},
number = {2},
pages = {2--6},
title = {{Discriminative NMF and its application to single-channel source separation}}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - DISCRIMINATIVE NON-NEGATIVE MATRIX FACTORIZATION FOR SINGLE-CHANNEL SPEECH SEPARATION Department of Computer Science.pdf:pdf},
title = {{DISCRIMINATIVE NON-NEGATIVE MATRIX FACTORIZATION FOR SINGLE-CHANNEL SPEECH SEPARATION Department of Computer Science and Technology Tsinghua University Department of Computer Science University of Southern California Los Angeles , CA 90089}}
}
@article{Zeiler2012,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
eprint = {arXiv:1311.2901v3},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zeiler, Fergus - 2012 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
keywords = {Christopher},
mendeley-tags = {Christopher},
title = {{Visualizing and Understanding Convolutional Networks}},
year = {2012}
}
@article{Magoarou2013,
author = {Magoarou, Luc Le},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Magoarou - 2013 - Auxiliary data informed audio source separation.pdf:pdf},
title = {{Auxiliary data informed audio source separation}},
year = {2013}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Graduate{\_}Thesis{\_}ch4{\_}revisions.pdf.pdf:pdf},
title = {{Graduate{\_}Thesis{\_}ch4{\_}revisions.pdf}}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - prior art presentation.pdf.pdf:pdf},
title = {prior art presentation.pdf}
}
@article{Name2013a,
author = {Name, Author},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Name - 2013 - Thesis Title by Author Name Declaration of Authorship.pdf:pdf},
number = {August},
title = {{Thesis Title by Author Name Declaration of Authorship}},
year = {2013}
}
@article{Name2013,
author = {Name, Author},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Name - 2013 - Thesis Title by Author Name Declaration of Authorship(2).pdf:pdf},
number = {August},
title = {{Thesis Title by Author Name Declaration of Authorship}},
year = {2013}
}
@article{America,
author = {America, N E C Laboratories},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/America - Unknown - Learning with Large Datasets Why Large-scale Datasets.pdf:pdf},
title = {{Learning with Large Datasets Why Large-scale Datasets ?}}
}
@inproceedings{Zhang,
author = {Zhang, Weiyu and Yu, Stella X. and Teng, Shang-Hua},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - Unknown - Power SVM Generalization with Exemplar Classification Uncertainty.pdf:pdf},
title = {{Power SVM : Generalization with Exemplar Classification Uncertainty}},
year = {2012}
}
@article{Collobert,
author = {Collobert, Ronan and Weston, Jason and Bai, Bing and Kuksa, Pavel},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - Unknown - Large Scale Learning Which Is Actually Useful.pdf:pdf},
title = {{Large Scale Learning Which Is Actually Useful}}
}
@article{Sindhwani,
author = {Sindhwani, Vikas},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sindhwani - Unknown - Newton Methods for Fast Solution of Semi- supervised Linear SVMs.pdf:pdf},
pages = {1--23},
title = {{Newton Methods for Fast Solution of Semi- supervised Linear SVMs}}
}
@article{Bordes2009,
author = {Bordes, Antoine and Pierre, Universit{\'{e}}},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bordes, Pierre - 2009 - SGD-QN Careful Quasi-Newton Stochastic Gradient Descent.pdf:pdf},
keywords = {stochastic gradient descent,support vector machine},
pages = {1737--1754},
title = {{SGD-QN : Careful Quasi-Newton Stochastic Gradient Descent}},
volume = {10},
year = {2009}
}
@misc{,
annote = {Oral 1A - Matching and reconstruction
------------------------------------------------
          
Reconstructing PASCAL VOC
        Sara Vicente, Joao Carreira, Lourdes Agapito, Jorge Batista
        
          
Fast and Accurate Image Matching with Cascade Hashing for 3D Reconstruction
        Jian Cheng, Cong Leng, Jiaxiang Wu, Hainan Cui, Hanqing Lu

        
        Fine-Grained Visual Comparisons with Local Learning
        Aron Yu, Kristen Grauman

        
        An Exemplar-based CRF for Multi-instance Object Segmentation
        Xuming He, Stephen Gould

          

        
      },
title = {{CVPR 2014 papers}}
}
@article{Shivaswamy2003,
author = {Shivaswamy, Pannagadatta K and Jebara, Tony},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shivaswamy, Jebara - 2003 - Permutation Invariant SVMs.pdf:pdf},
title = {{Permutation Invariant SVMs}},
year = {2003}
}
@article{Raskutti,
author = {Raskutti, Bhavani},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Raskutti - Unknown - Extreme Re-balancing for SVMs a case study U {\$}.pdf:pdf},
number = {1},
title = {{Extreme Re-balancing for SVMs : a case study U {\$}}},
volume = {6}
}
@article{Branson2013,
author = {Branson, Steve and Beijbom, Oscar and Belongie, Serge},
doi = {10.1109/CVPR.2013.236},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Branson, Beijbom, Belongie - 2013 - Efficient Large-Scale Structured Learning.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {1806--1813},
publisher = {Ieee},
title = {{Efficient Large-Scale Structured Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619080},
year = {2013}
}
@article{Jurie,
annote = {{\#} Broader list of recent papers on mid-level features and attributes   
- Berg, T. L., Berg, A. C., {\&} Shih, J. (2010). Automatic attribute discovery and characterization from noisy web data (pp. 663–676). Presented at the ECCV, Springer.   
- Boureau, Y. L., Bach, F., LeCun, Y., {\&} Ponce, J. (2010). Learning mid-level features for recognition (pp. 2559–2566). Presented at the CVPR. doi:10.1109/CVPR.2010.5539963   
- Chua, J., Givoni, I., Adams, R., {\&} Frey, B. (2012). Learning structural element patch models with hierarchical palettes (pp. 2416–2423). Presented at the CVPR. doi:10.1109/CVPR.2012.6247955   
- Doersch, C., Singh, S., Gupta, A., Sivic, J., {\&} Efros, A. A. (2012). What makes Paris look like Paris? (Vol. 31, pp. 1–9). Presented at the ACM Transactions on Graphics. doi:10.1145/2185520.2185597   
- Duan, K., Parikh, D., Crandall, D., {\&} Grauman, K. (2012). Discovering localized attributes for fine-grained recognition, 3474–3481.   
- Endres, I., Shih, K. J., Jiaa, J., {\&} Hoiem, D. (2013). Learning Collections of Part Models for Object Recognition. Presented at the ICCV, CVPR. doi:10.1109/ICCV.2013.369   
- Farhadi, A., Endres, I., {\&} Hoiem, D. (2010a). Attribute-centric recognition for cross-category generalization (pp. 2352–2359). Presented at the CVPR, IEEE.   
- Farhadi, A., Hejrati, M., Sadeghi, M. A., Young, P., Rashtchian, C., Hockenmaier, J., {\&} Forsyth, D. (2010b). Every picture tells a story: Generating sentences from images (pp. 15–29). Presented at the ECCV, Springer.   
- Ferrari, V., {\&} Zisserman, A. (2013). Learning Visual Attributes (pp. 1–8). Presented at the NIPS. 
Hariharan, B., Malik, J., {\&} Ramanan, D. (2012). Discriminative decorrelation for clustering and classification (pp. 459–472). Presented at the ECCV, Springer.   
- Hwang, S. J., Sha, F., {\&} Grauman, K. (2011). Sharing features between objects and their attributes (pp. 1761–1768). Presented at the CVPR. doi:10.1109/CVPR.2011.5995543   
- Juneja, M., Vedaldi, A., Jawahar, C. V., {\&} Zisserman, A. (2013). Blocks That Shout: Distinctive Parts for Scene Classification (pp. 923–930). Presented at the CVPR,  IEEE Computer Society. doi:10.1109/CVPR.2013.124   
- Kokkinos, I. (2012). Bounding Part Scores for Rapid Detection with Deformable Part Models (pp. 1–10). Presented at the ECCV.   
- Kovashka, A., Vijayanarasimhan, S., {\&} Grauman, K. (2011). Actively selecting annotations among objects and attributes (pp. 1403–1410). Presented at the ICCV. doi:10.1109/ICCV.2011.6126395   
- Kumar, N., Berg, A. C., Belhumeur, P. N., {\&} Nayar, S. K. (2011). Describable Visual Attributes for Face Verification and Image Search. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 33(10), 1962–1977. doi:10.1109/TPAMI.2011.48   
- Liao, Z., Farhadi, A., Wang, Y., Endres, I., {\&} Forsyth, D. (2012). Building a dictionary of image fragments (pp. 3442–3449). Presented at the CVPR. doi:10.1109/CVPR.2012.6248085   
- Lim, J. J., Zitnick, C. L., {\&} Dollar, P. (2013). Sketch Tokens: A Learned Mid-level Representation for Contour and Object Detection, 1–8.   
- Ma, S., Sclaroff, S., {\&} Ikizler-Cinbis, N. (2012). Unsupervised learning of discriminative relative visual attributes. Presented at the ECCV, Springer.   
- Mahajan, D., Sellamanickam, S., {\&} Nair, V. (2011). A joint learning framework for attribute models and object descriptions, 1227–1234.   
- Maji, S. (2012). Discovering a Lexicon of Parts and Attributes (pp. 1–10). Presented at the ECCV.   
- Maji, S., {\&} Shakhnarovich, G. (2013). Part Discovery from Partial Correspondence. Presented at the CVPR.   
- Mining Multiple Queries for Image Retrieval: On-the-fly learning of an Object-specific Mid-level Representation. (2013). Mining Multiple Queries for Image Retrieval: On-the-fly learning of an Object-specific Mid-level Representation (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.316   
- Modeling Occlusion by Discriminative AND-OR Structures. (2013). Modeling Occlusion by Discriminative AND-OR Structures (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.318   
- Parikh, D. (2011). Recognizing jumbled images: the role of local and global information in image classification, 519–526.   
- Parikh, D., {\&} Grauman, K. (2011a). Interactively building a discriminative vocabulary of nameable attributes, 1681–1688.   
- Parikh, D., {\&} Grauman, K. (2011b). Relative attributes (pp. 503–510). Presented at the ICCV. doi:10.1109/ICCV.2011.6126281   
- Parkash, A., {\&} Parikh, D. (2012). Attributes for classifier feedback (pp. 354–368). Presented at the ECCV, Springer.   
- Pirsiavash, H., {\&} Ramanan, D. (2012). Steerable part models (pp. 3226–3233). Presented at the CVPR. doi:10.1109/CVPR.2012.6248058   
- Rastegari, M., Farhadi, A., {\&} Forsyth, D. (2012). Attribute discovery via predictable discriminative binary codes (pp. 876–889). Presented at the ECCV, Springer.   
- Rohrbach, M., Stark, M., Szarvas, G., Gurevych, I., {\&} Schiele, B. (2010). What Helps Where – And Why? Semantic Relatedness for Knowledge Transfer (pp. 910–917). Presented at the CVPR, IEEE.   
- Russakovsky, O., {\&} Fei-Fei, L. (2012). Attribute learning in large-scale datasets (pp. 1–14). Presented at the ECCV, Springer.   
- Santosh K Divvala, A. A. E. A. M. H. (2012). How Important Are “Deformable Parts” in the Deformable Parts Model? (pp. 1–10). Presented at the ECCV.   
- Sharma, G., Jurie, F., {\&} Schmid, C. (2013). Expanded Parts Model for Human Attribute and Action Recognition in Still Images. Presented at the CVPR,  IEEE Computer Society. doi:10.1109/CVPR.2013.90   
- Sharmanska, V., Quadrianto, N., {\&} Lampert, C. H. (2012). Augmented attribute representations (pp. 242–255). Presented at the ECCV, Springer.   
- Shrivastava, A., Singh, S., {\&} Gupta, A. (2012). Constrained semi-supervised learning using attributes and comparative attributes (pp. 369–383). Presented at the ECCV, Springer.   
- Shufflets: shared mid-level parts for fast object detection. (2013). Shufflets: shared mid-level parts for fast object detection (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.176   
- Si, Z., {\&} Zhu, S.-C. (2012). Learning hybrid image templates (hit) by information projection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 34(7), 1354–1367.   
- Si, Z., {\&} Zhu, S.-C. (2013). Learning AND-OR Templates for Object Recognition and Detection. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(9), 2189–2205. doi:10.1109/TPAMI.2013.35   
- Singh, S., Gupta, A., {\&} Efros, A. A. (2012). Unsupervised discovery of mid-level discriminative patches. Presented at the ECCV, Springer.   
- Style-aware Mid-level Representation for Discovering Visual Connections in Space and Time. (2013). Style-aware Mid-level Representation for Discovering Visual Connections in Space and Time (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.233   
- Su, Y., {\&} Jurie, F. (2012). Learning Compact Visual Attributes for Large-Scale Image Classification. Presented at the ECCV Workshops, Springer.   
- Wang, Y., {\&} Mori, G. (2010). A discriminative latent model of object classes and attributes (pp. 155–168). Presented at the ECCV, Springer. },
author = {Jurie, Frederic},
title = {{Action recognition, broader list, ref list}}
}
@misc{Juriea,
annote = {{\#} Selected ICCV'13 papers on action recognition (sorry authors' names are missing)   
- Action and Event Recognition with Fisher Vectors on a Compact Feature Set. (2013). Action and Event Recognition with Fisher Vectors on a Compact Feature Set (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.228   
- Action Recognition and Localization by Hierarchical Space-Time Segments. (2013). Action Recognition and Localization by Hierarchical Space-Time Segments (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.341   
- Action Recognition with Actons. (2013). Action Recognition with Actons (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.442   
- ACTIVE: Activity Concept Transitions in Video Event Classification. (2013). ACTIVE: Activity Concept Transitions in Video Event Classification (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.453   
- Cross-view Action Recognition over Heterogeneous Feature Spaces. (2013). Cross-view Action Recognition over Heterogeneous Feature Spaces (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.81   
- Dynamic Pooling for Complex Event Recognition. (2013). Dynamic Pooling for Complex Event Recognition (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.339   
- Finding Actors and Actions in Movies. (2013). Finding Actors and Actions in Movies (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.283  
- From Actemes to Action: A Strongly-supervised Representation for Detailed Action Understanding. (2013). From Actemes to Action: A Strongly-supervised Representation for Detailed Action Understanding (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.280   
- How Related Exemplars Help Complex Event Detection in Web Videos? (2013). How Related Exemplars Help Complex Event Detection in Web Videos? (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.456   
- Latent Multitask Learning for View-Invariant Action Recognition. (2013). Latent Multitask Learning for View-Invariant Action Recognition (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.388   
- Learning to Share Latent Tasks for Action Recognition. (2013). Learning to Share Latent Tasks for Action Recognition (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.281   
Mining Motion Atoms and Phrases for Complex Action Recognition. (2013). Mining Motion Atoms and Phrases for Complex   
- Action Recognition (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.333 Towards understanding action recognition. (2013). Towards understanding action recognition (pp. 1–8). Presented at the ICCV. doi:10.1109/ICCV.2013.396 },
author = {Jurie, Frederic},
title = {{Action recognition ICCV 2013, ref list}}
}
@misc{Jurieb,
annote = {{\#} References of the papers we discussed today (related to midlevel features by clustering/pruning/image coding)   
- Singh, S., Gupta, A., {\&} Efros, A. A. (2012). Unsupervised discovery of mid-level discriminative patches. ECCV.   
- Su, Y., {\&} Jurie, F. (2012). Learning Compact Visual Attributes for Large-Scale Image Classification. ECCV Workshops.   
- Doersch, C., Singh, S., Gupta, A., Sivic, J., {\&} Efros, A. A. (2012). What makes Paris look like Paris? ACM Transactions on Graphics.   
- Sharma, G., Jurie, F., {\&} Schmid, C. (2013). Expanded Parts Model for Human Attribute and Action Recognition in Still Images (pp. 652–659). CVPR {\&}{\#}039;13   
- Juneja, M., Vedaldi, A., Jawahar, C. V., {\&} Zisserman, A. (2013). Blocks That Shout: Distinctive Parts for Scene Classification (pp. 923–930). CVPR   
- Joo, J., Wang, S., {\&} Zhu, S.-C. (2013). Human Attribute Recognition by Rich Appearance Dictionary. ICCV   
- Endres, I., Shih, K. J., Jiaa, J., {\&} Hoiem, D. (2013). Learning Collections of Part Models for Object Recognition. CVPR   
- Maji, S., {\&} Shakhnarovich, G. (2013). Part Discovery from Partial Correspondence. CVPR. },
author = {Jurie, Frederic},
title = {{Mid-level features, ref list}}
}
@misc{Juriec,
annote = {{\textgreater} Albatal, R., Mulhem, P. {\&} Chiaramella, Y., 2010, Content-Based Multimedia Indexing (CBMI), 2010 International Workshop on, Visual Phrases for automatic images annotation. pp. 1-6.
{\textgreater}
{\textgreater} Jiang, Y., Meng, J. {\&} Yuan, J., 2012, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, Randomized visual phrases for object search. pp. 3100-7.
{\textgreater}
{\textgreater} Sadeghi, M.A. {\&} Farhadi, A., 2011, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, Recognition using visual phrases. pp. 1745-52.
{\textgreater}
{\textgreater} Yuan, J., Wu, Y. {\&} Yang, M., 2007, Computer Vision and Pattern Recognition, 2007. CVPR{\&}{\#}039;07. IEEE Conference on, Discovery of collocation patterns: from visual words to visual phrases. pp. 1-8.
{\textgreater}
{\textgreater} Zhang, S., Tian, Q., Hua, G., Huang, Q. {\&} Gao, W., 2011a, Generating descriptive visual words and visual phrases for large-scale image applications, Image Processing, IEEE Transactions on, 20(9), pp. 2664-77.
{\textgreater}
{\textgreater} Zhang, Y., Jia, Z. {\&} Chen, T., 2011b, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, Image retrieval with geometry-preserving visual phrases. pp. 809-16.
{\textgreater}
{\textgreater} Zheng, Q.-F., Wang, W.-Q. {\&} Gao, W., 2006, Proceedings of the 14th annual ACM international conference on Multimedia, Effective and efficient object-based image retrieval using visual phrases
{\textgreater} {\textgreater}
{\textgreater}},
author = {Jurie, Frederic},
title = {{Visual phrases, ref list}}
}
@article{Boyd2003,
author = {Boyd, Stephen and Xiao, Lin and Mutapcic, Almir},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boyd, Xiao, Mutapcic - 2003 - Subgradient Methods The subgradient method.pdf:pdf},
pages = {1--21},
title = {{Subgradient Methods The subgradient method}},
volume = {1},
year = {2003}
}
@article{Wang2012,
author = {Wang, Zhuang},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang - 2012 - Breaking the Curse of Kernelization Budgeted Stochastic Gradient Descent for Large-Scale SVM Training.pdf:pdf},
pages = {3103--3131},
title = {{Breaking the Curse of Kernelization : Budgeted Stochastic Gradient Descent for Large-Scale SVM Training}},
volume = {13},
year = {2012}
}
@article{Zhao2013,
author = {Zhao, Wan-Lei and Gravier, Guillaume and Jegou, Herve},
doi = {10.5244/C.27.99},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Gravier, Jegou - 2013 - Oriented pooling for dense and non-dense rotation-invariant features.pdf:pdf},
isbn = {1-901725-49-9},
journal = {Procedings of the British Machine Vision Conference 2013},
pages = {99.1--99.11},
publisher = {British Machine Vision Association},
title = {{Oriented pooling for dense and non-dense rotation-invariant features}},
url = {http://www.bmva.org/bmvc/2013/Papers/paper0099/index.html},
year = {2013}
}
@article{Huang2014,
author = {Huang, Yongzhen and Wu, Zifeng and Wang, Liang and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2014 - Feature Coding in Image Classification.pdf:pdf},
number = {3},
pages = {493--506},
title = {{Feature Coding in Image Classification :}},
volume = {36},
year = {2014}
}
@article{Murray,
author = {Murray, Naila and Perronnin, Florent},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Murray, Perronnin - Unknown - Generalized Max Pooling.pdf:pdf},
title = {{Generalized Max Pooling}}
}
@article{Tong2001,
author = {Tong, Simon and Koller, Daphne},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tong, Koller - 2001 - with Applications to Text Classification.pdf:pdf},
keywords = {active learning,classifica-,relevance feedback,selective sampling,support vector machines,tion},
pages = {45--66},
title = {{with Applications to Text Classification}},
year = {2001}
}
@article{Malisiewicz2008,
author = {Malisiewicz, Tomasz and Efros, Alexei a.},
doi = {10.1109/CVPR.2008.4587462},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malisiewicz, Efros - 2008 - Recognition by association via learning per-exemplar distances.pdf:pdf},
isbn = {978-1-4244-2242-5},
journal = {Computer Vision and Pattern Recognition},
month = {jun},
publisher = {Ieee},
title = {{Recognition by association via learning per-exemplar distances}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587462},
year = {2008}
}
@article{Frome,
author = {Frome, Andrea},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Frome - Unknown - Image Retrieval and Classification Using Local Distance Functions.pdf:pdf},
number = {4},
title = {{Image Retrieval and Classification Using Local Distance Functions}}
}
@article{Tsang2005,
author = {Tsang, Ivor W and Kwok, James T},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsang, Kwok - 2005 - Core Vector Machines Fast SVM Training on Very Large Data Sets.pdf:pdf},
keywords = {approximation algorithm,core set,kernel methods,minimum enclosing ball,scalabil-},
pages = {363--392},
title = {{Core Vector Machines : Fast SVM Training on Very Large Data Sets}},
volume = {6},
year = {2005}
}
@article{Platt,
author = {Platt, John C.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Platt - Unknown - Fast training of support vector machines using sequential minimal optimization.pdf:pdf},
title = {{Fast training of support vector machines using sequential minimal optimization}}
}
@article{Rose2002,
author = {Rose, Tony and Stevenson, Mark and Whitehead, Miles},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rose, Stevenson, Whitehead - 2002 - The Reuters Corpus Volume 1 - from Yesterday ' s News to Tomorrow ' s Language Resources.pdf:pdf},
title = {{The Reuters Corpus Volume 1 - from Yesterday ' s News to Tomorrow ' s Language Resources}},
volume = {1},
year = {2002}
}
@article{Cao2013,
author = {Cao, Liujuan and Ji, Rongrong and Liu, Wei and Yao, Hongxun and Tian, Qi},
doi = {10.1016/j.sigpro.2012.05.001},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao et al. - 2013 - Weakly supervised codebook learning by iterative label propagation with graph quantization.pdf:pdf},
issn = {0165-1684},
journal = {Signal Processing},
keywords = {Image search,Patch quantization,Visual data indexing,Visual vocabulary,Weakly supervised learning},
number = {8},
pages = {2274--2283},
publisher = {Elsevier},
title = {{Weakly supervised codebook learning by iterative label propagation with graph quantization}},
url = {http://dx.doi.org/10.1016/j.sigpro.2012.05.001},
volume = {93},
year = {2013}
}
@article{Jiu2012,
author = {Jiu, Mingyuan and Wolf, Christian and Garcia, Christophe},
doi = {10.1007/s12559-012-9137-4},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jiu, Wolf, Garcia - 2012 - Supervised Learning and Codebook Optimization for Bag-of-Words Models.pdf:pdf},
keywords = {bag-of-words models {\'{a}} supervised,learning {\'{a}},neural networks {\'{a}} action,recognition},
title = {{Supervised Learning and Codebook Optimization for Bag-of-Words Models}},
year = {2012}
}
@inproceedings{Wang2010,
author = {Wang, Jinjun and Yang, Jianchao and Yu, Kai and Lv, Fengjun and Huang, Thomas and Gong, Yihong},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2010 - Locality-constrained Linear Coding for Image Classification(2).pdf:pdf},
isbn = {9781424469857},
pages = {3360--3367},
title = {{Locality-constrained Linear Coding for Image Classification}},
year = {2010}
}
@article{Ye,
author = {Ye, Jieping},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ye - Unknown - Discriminative K-means for Clustering.pdf:pdf},
number = {1},
title = {{Discriminative K-means for Clustering}}
}
@article{Yea,
author = {Ye, Jieping},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ye - Unknown - Discriminative K-means for Clustering.pdf:pdf},
number = {1},
title = {{Discriminative K-means for Clustering}}
}
@article{Aytar2012,
author = {Aytar, Yusuf and Zisserman, Andrew},
doi = {10.5244/C.26.79},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aytar, Zisserman - 2012 - Enhancing Exemplar SVMs using Part Level Transfer Regularization.pdf:pdf},
isbn = {1-901725-46-4},
journal = {British Machine Vision Conference},
pages = {79.1--79.11},
publisher = {British Machine Vision Association},
title = {{Enhancing Exemplar SVMs using Part Level Transfer Regularization}},
url = {http://www.bmva.org/bmvc/2012/BMVC/paper079/index.html},
year = {2012}
}
@article{Malisiewicz2011,
author = {Malisiewicz, Tomasz and Gupta, Abhinav and Efros, Alexei a.},
doi = {10.1109/ICCV.2011.6126229},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malisiewicz, Gupta, Efros - 2011 - Ensemble of exemplar-SVMs for object detection and beyond.pdf:pdf},
isbn = {978-1-4577-1102-2},
journal = {2011 International Conference on Computer Vision},
month = {nov},
pages = {89--96},
publisher = {Ieee},
title = {{Ensemble of exemplar-SVMs for object detection and beyond}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126229},
year = {2011}
}
@article{Joachims1998,
author = {Joachims, Thorsten and Dortmund, Universitat and Joachimscsuni-dortmundde, Thorsten},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims, Dortmund, Joachimscsuni-dortmundde - 1998 - Making large-scale SVM learning practical.pdf:pdf},
title = {{Making large-scale SVM learning practical}},
year = {1998}
}
@article{Hsieh2008,
address = {New York, New York, USA},
author = {Hsieh, Cho-Jui and Chang, Kai-Wei and Lin, Chih-Jen and Keerthi, S. Sathiya and Sundararajan, S.},
doi = {10.1145/1390156.1390208},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsieh et al. - 2008 - A dual coordinate descent method for large-scale linear SVM.pdf:pdf},
isbn = {9781605582054},
journal = {Proceedings of the 25th international conference on Machine learning - ICML '08},
number = {2},
pages = {408--415},
publisher = {ACM Press},
title = {{A dual coordinate descent method for large-scale linear SVM}},
url = {http://portal.acm.org/citation.cfm?doid=1390156.1390208},
year = {2008}
}
@article{Chen2013,
author = {Chen, Dong and Cao, Xudong and Wen, Fang and Sun, Jian},
doi = {10.1109/CVPR.2013.389},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2013 - Blessing of Dimensionality High-Dimensional Feature and Its Efficient Compression for Face Verification.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {3025--3032},
publisher = {Ieee},
title = {{Blessing of Dimensionality: High-Dimensional Feature and Its Efficient Compression for Face Verification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6619233},
year = {2013}
}
@article{Cai2011,
abstract = {In this paper, we present Linear Discriminant Projections (LDP) for reducing dimensionality and improving discriminability of local image descriptors. We place LDP into the context of state-of-the-art discriminant projections and analyze its properties. LDP requires a large set of training data with point-to-point correspondence ground truth. We demonstrate that training data produced by a simulation of image transformations leads to nearly the same results as the real data with correspondence ground truth. This makes it possible to apply LDP as well as other discriminant projection approaches to the problems where the correspondence ground truth is not available, such as image categorization. We perform an extensive experimental evaluation on standard data sets in the context of image matching and categorization. We demonstrate that LDP enables significant dimensionality reduction of local descriptors and performance increases in different applications. The results improve upon the state-of-the-art recognition performance with simultaneous dimensionality reduction from 128 to 30.},
author = {Cai, Hongping and Mikolajczyk, Krystian and Matas, Jiri},
doi = {10.1109/TPAMI.2010.89},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cai, Mikolajczyk, Matas - 2011 - Learning linear discriminant projections for dimensionality reduction of image descriptors.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {feb},
number = {2},
pages = {338--52},
pmid = {20421668},
title = {{Learning linear discriminant projections for dimensionality reduction of image descriptors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20421668},
volume = {33},
year = {2011}
}
@article{Jost,
author = {Jost, Philippe and Vandergheynst, Pierre and F, Ecole Polytechnique},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jost, Vandergheynst, F - Unknown - On Finding Nearest Neighbors in a Set of Compressible Signals.pdf:pdf},
title = {{On Finding Nearest Neighbors in a Set of Compressible Signals}}
}
@article{Jose2013,
author = {Jose, Cijo and Goyal, Prasoon and Aggrwal, Parv},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jose, Goyal, Aggrwal - 2013 - Local Deep Kernel Learning for Efficient Non-linear SVM Prediction.pdf:pdf},
title = {{Local Deep Kernel Learning for Efficient Non-linear SVM Prediction}},
volume = {28},
year = {2013}
}
@article{Fan2005,
author = {Fan, Rong-en and Chen, Pai-hsuen and Lin, Chih-jen},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Chen, Lin - 2005 - Working Set Selection Using Second Order Information for Training Support Vector Machines.pdf:pdf},
keywords = {decomposition methods,mization,sequential minimal opti-,support vector machines,working set selection},
pages = {1889--1918},
title = {{Working Set Selection Using Second Order Information for Training Support Vector Machines}},
volume = {6},
year = {2005}
}
@article{Cowen2014,
author = {Cowen, Alan S and Chun, Marvin M and Kuhl, Brice A},
doi = {10.1016/j.neuroimage.2014.03.018},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cowen, Chun, Kuhl - 2014 - Training Reconstruction.pdf:pdf},
issn = {1053-8119},
journal = {NeuroImage},
publisher = {Elsevier Inc.},
title = {{Training Reconstruction}},
url = {http://dx.doi.org/10.1016/j.neuroimage.2014.03.018},
year = {2014}
}
@inproceedings{Jegou2014,
author = {J{\'{e}}gou, Herv{\'{e}} and Zisserman, Andrew},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/J{\'{e}}gou, Zisserman - 2014 - Triangulation embedding and democratic aggregation for image search.pdf:pdf},
title = {{Triangulation Embedding and Democratic Aggregation for Image Search}},
year = {2014}
}
@article{Lin2007,
author = {Lin, Chih-jen and Weng, Ruby C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin, Weng - 2007 - Trust Region Newton Methods for Large-Scale Logistic Regression.pdf:pdf},
title = {{Trust Region Newton Methods for Large-Scale Logistic Regression}},
year = {2007}
}
@misc{Zisserman2011,
author = {Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zisserman - 2014 - Lecture 2 The SVM classifier.pdf:pdf},
title = {{Lecture 2 : The SVM classifier}},
url = {http://www.robots.ox.ac.uk/{~}az/lectures/ml/2011/lect2.pdf},
year = {2011}
}
@article{Gradient,
author = {Gradient, The and Method, Projection},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gradient, Method - Unknown - Nonlinear Constraints.pdf:pdf},
number = {23},
pages = {371--374},
title = {{Nonlinear Constraints}},
volume = {0}
}
@inproceedings{Singh,
author = {Singh, Saurabh and Gupta, Abhinav and Efros, Alexei A.},
booktitle = {European Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Singh, Gupta, Efros - Unknown - Unsupervised Discovery of Mid-Level Discriminative Patches.pdf:pdf},
title = {{Unsupervised Discovery of Mid-Level Discriminative Patches}},
year = {2012}
}
@article{Shrivastava2011,
author = {Shrivastava, Abhinav and Malisiewicz, Tomasz and Gupta, Abhinav and Efros, Alexei a.},
doi = {10.1145/2070781.2024188},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shrivastava et al. - 2011 - Data-driven visual similarity for cross-domain image matching.pdf:pdf},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {image matching,image matching, visual similarity, saliency, image,image re-,paintings,re-photography,saliency,sketches,trieval,visual memex,visual similarity},
month = {dec},
number = {6},
pages = {1},
title = {{Data-driven visual similarity for cross-domain image matching}},
url = {http://dl.acm.org/citation.cfm?doid=2070781.2024188},
volume = {30},
year = {2011}
}
@article{Juneja2013,
author = {Juneja, Mayank and Vedaldi, Andrea and Jawahar, C.V. and Zisserman, Andrew},
doi = {10.1109/CVPR.2013.124},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Juneja et al. - 2013 - Blocks That Shout Distinctive Parts for Scene Classification.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {923--930},
publisher = {Ieee},
title = {{Blocks That Shout: Distinctive Parts for Scene Classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6618968},
year = {2013}
}
@inproceedings{Ge,
author = {Ge, Tiezheng and He, Kaiming and Sun, Jian},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ge, He - Unknown - Product Sparse Coding.pdf:pdf},
title = {{Product Sparse Coding}},
year = {2014}
}
@article{Avila2013,
author = {Avila, S and Thome, N and Cord, M and Valle, E and Ara, A De A},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Avila et al. - 2013 - Pooling in image representation the visual codeword point of view 1.pdf:pdf},
keywords = {bag-of-words,coding,dictionary,image classification,image representation,pattern recognition,pooling,svm,visual},
number = {5},
pages = {453--465},
title = {{Pooling in image representation : the visual codeword point of view 1}},
volume = {117},
year = {2013}
}
@article{Feng,
author = {Feng, Jiashi},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng - Unknown - Online Robust PCA via Stochastic Optimization.pdf:pdf},
pages = {1--9},
title = {{Online Robust PCA via Stochastic Optimization}}
}
@article{Doersch2011,
author = {Doersch, Carl and Singh, Saurabh},
keywords = {big,computational geography,data,data mining,reference art,visual perception,visual summarization},
title = {{What Makes Paris Look like Paris ?}},
year = {2011}
}
@article{Perez2014,
author = {P{\'{e}}rez, Patrick},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/P{\'{e}}rez - 2014 - A tour of data embedding.pdf:pdf},
number = {6},
pages = {1--23},
title = {{A tour of data embedding}},
year = {2014}
}
@article{Hariharan,
author = {Hariharan, Bharath and Malik, Jitendra and Ramanan, Deva},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hariharan, Malik, Ramanan - Unknown - Discriminative Decorrelation for Clustering and Classification.pdf:pdf},
pages = {1--14},
title = {{Discriminative Decorrelation for Clustering and Classification}},
volume = {1}
}
@article{Su,
author = {Su, Yu},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su - Unknown - Learning Compact Visual Attributes for Large-scale Image Classification.pdf:pdf},
title = {{Learning Compact Visual Attributes for Large-scale Image Classification}}
}
@inproceedings{Simonyan,
author = {Simonyan, K. and Parkhi, O. M. and Vedaldi, A. and Zisserman, A.},
booktitle = {British Machine Vision Conference},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan, Vedaldi, Zisserman - Unknown - Fisher Vector Faces in the Wild.pdf:pdf},
title = {{Fisher Vector Faces in the Wild}},
year = {2013}
}
@article{Chandrasekhar,
author = {Chandrasekhar, Vijay and Takacs, Gabriel and Chen, David and Tsai, Sam and Grzeszczuk, Radek and Girod, Bernd},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrasekhar et al. - Unknown - CHoG Compressed Histogram of Gradients.pdf:pdf},
title = {{CHoG : Compressed Histogram of Gradients}}
}
@article{Tolias2013,
author = {Tolias, Giorgos and Avrithis, Yannis and Jegou, Herve},
doi = {10.1109/ICCV.2013.177},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tolias, Avrithis, Jegou - 2013 - To Aggregate or Not to aggregate Selective Match Kernels for Image Search.pdf:pdf},
isbn = {978-1-4799-2840-8},
journal = {2013 IEEE International Conference on Computer Vision},
month = {dec},
pages = {1401--1408},
publisher = {Ieee},
title = {{To Aggregate or Not to aggregate: Selective Match Kernels for Image Search}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6751284},
year = {2013}
}
@article{Shrivastava2011a,
author = {Shrivastava, Abhinav and Malisiewicz, Tomasz and Gupta, Abhinav and Efros, Alexei a.},
doi = {10.1145/2070781.2024188},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {image matching,image re-,paintings,re-photography,saliency,sketches,trieval,visual memex,visual similarity},
month = {dec},
number = {6},
pages = {1},
title = {{Data-driven visual similarity for cross-domain image matching}},
url = {http://dl.acm.org/citation.cfm?doid=2070781.2024188},
volume = {30},
year = {2011}
}
@article{Cheng1995,
author = {Cheng, Yizong},
doi = {10.1109/34.400568},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng - 1995 - Mean shift, mode seeking, and clustering.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {790--799},
title = {{Mean shift, mode seeking, and clustering}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=400568},
volume = {17},
year = {1995}
}
@article{Doersch,
author = {Doersch, Carl and Efros, Alexei A},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doersch, Efros - Unknown - Mid-level Visual Element Discovery as Discriminative Mode Seeking.pdf:pdf},
pages = {1--11},
title = {{Mid-level Visual Element Discovery as Discriminative Mode Seeking}}
}
@article{Dhillon2005,
author = {Dhillon, Inderjit and Guan, Yuqiang and Kulis, Brian},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dhillon, Guan, Kulis - 2005 - A Unified View of Kernel k-means , Spectral Clustering and Graph Cuts.pdf:pdf},
keywords = {clustering,eigenvectors,graph partitioning,kernel k -means,spectral methods,trace maxi-},
pages = {1--20},
title = {{A Unified View of Kernel k-means , Spectral Clustering and Graph Cuts}},
year = {2005}
}
@article{Abou-moustafa,
author = {Abou-Moustafa, Karim and Shah, Mohak and {De La Torre}, Fernando and Ferrie, Frank},
doi = {10.1007/978-3-642-23123-0_19},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abou-moustafa, Shah - Unknown - Relaxed Exponential Kernels for Unsupervised Learning.pdf:pdf},
isbn = {9783642231223},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {184--195},
title = {{Relaxed exponential kernels for unsupervised learning}},
volume = {6835 LNCS},
year = {2011}
}
@article{Koren,
author = {Koren, Yehuda and Ave, Park and Park, Florham},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koren, Ave, Park - Unknown - Factorization Meets the Neighborhood a Multifaceted Collaborative Filtering Model.pdf:pdf},
isbn = {9781605581934},
keywords = {collaborative filtering,recommender systems},
title = {{Factorization Meets the Neighborhood : a Multifaceted Collaborative Filtering Model}}
}
@article{Ridgway2006,
author = {Ridgway, Ged},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ridgway - 2006 - Matrix Inversion Identities.pdf:pdf},
title = {{Matrix Inversion Identities}},
year = {2006}
}
@article{Plagianakos,
author = {Plagianakos, V P and Magoulas, G D and Vrahatis, M N},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plagianakos, Magoulas, Vrahatis - Unknown - Chapter 2 LEARNING RATE ADAPTATION IN STOCHASTIC GRADIENT DESCENT.pdf:pdf},
keywords = {backpropagation neural networks,batch training,ing rate adaptation,learn-,line training,on,stochastic gradient descent},
pages = {15--26},
title = {{Chapter 2 LEARNING RATE ADAPTATION IN STOCHASTIC GRADIENT DESCENT}}
}
@incollection{Bottou2012,
author = {Bottou, Leon},
booktitle = {Neural Networks: Tricks of the Trade},
edition = {2nd},
editor = {Montavon, Gr{\'{e}}goire and Orr, Genevi{\`{e}}ve and M{\"{u}}ller, Klaus-Rober},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bottou - 2012 - Stochastic gradient descent tricks.pdf:pdf},
publisher = {Springer},
title = {{Stochastic gradient descent tricks}},
year = {2012}
}
@article{Duchi,
author = {Duchi, John},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi - Unknown - Properties of the Trace and Matrix Derivatives.pdf:pdf},
pages = {1--4},
title = {{Properties of the Trace and Matrix Derivatives}}
}
@article{Luxburg2012,
author = {Luxburg, Ulrike Von and Williamson, Robert C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luxburg, Williamson - 2012 - Clustering Science or Art.pdf:pdf},
pages = {65--79},
title = {{Clustering : Science or Art ?}},
year = {2012}
}
@article{Fernando2011,
author = {Fernando, Basura and Fromont, Elisa and Muselet, Damien and Sebban, Marc},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fernando et al. - 2011 - Supervised Learning of Gaussian Mixture Models for Visual Vocabulary Generation.pdf:pdf},
keywords = {bags of visual words,supervised gaussian mixture model},
pages = {1--27},
title = {{Supervised Learning of Gaussian Mixture Models for Visual Vocabulary Generation}},
year = {2011}
}
@article{Lampert2009,
author = {Lampert, C.H. and Nickisch, H. and Harmeling, S.},
doi = {10.1109/CVPR.2009.5206594},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lampert, Nickisch, Harmeling - 2009 - Learning to detect unseen object classes by between-class attribute transfer.pdf:pdf},
isbn = {978-1-4244-3992-8},
journal = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {951--958},
publisher = {Ieee},
title = {{Learning to detect unseen object classes by between-class attribute transfer}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206594},
year = {2009}
}
@article{Moosmann,
author = {Moosmann, Frank and Triggs, Bill and Jurie, Frederic},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moosmann, Triggs, Jurie - Unknown - Fast Discriminative Visual Codebooks using Randomized Clustering Forests.pdf:pdf},
title = {{Fast Discriminative Visual Codebooks using Randomized Clustering Forests}}
}
@article{Lewis2005,
author = {Lewis, Adrian S. and Sendov, Hristo S.},
doi = {10.1007/s11228-004-7197-7},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewis, Sendov - 2005 - Nonsmooth Analysis of Singular Values. Part I Theory.pdf:pdf},
issn = {0927-6947},
journal = {Set-Valued Analysis},
keywords = {ams 1991 subject classification,and phrases,differential,horizon subdifferential,inequality,limiting subdifferential,nonsmooth analysis,regular sub-,simultaneous diagonalization,singular values,von neumann trace},
month = {sep},
number = {3},
pages = {213--241},
title = {{Nonsmooth Analysis of Singular Values. Part I: Theory}},
url = {http://link.springer.com/10.1007/s11228-004-7197-7},
volume = {13},
year = {2005}
}
@article{Lewis2005a,
author = {Lewis, Adrian S. and Sendov, Hristo S.},
doi = {10.1007/s11228-004-7198-6},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewis, Sendov - 2005 - Nonsmooth Analysis of Singular Values. Part II Applications.pdf:pdf},
issn = {0927-6947},
journal = {Set-Valued Analysis},
keywords = {clarke subdifferential,lidskii,limiting subdifferential,lower semicontinuous,nonsmooth analysis,proximal subdifferential,regular subdifferential,singular values},
month = {sep},
number = {3},
pages = {243--264},
title = {{Nonsmooth Analysis of Singular Values. Part II: Applications}},
url = {http://link.springer.com/10.1007/s11228-004-7198-6},
volume = {13},
year = {2005}
}
@article{Yu2013,
author = {Yu, Felix X. and Cao, Liangliang and Feris, Rogerio S. and Smith, John R. and Chang, Shih-Fu},
doi = {10.1109/CVPR.2013.105},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu et al. - 2013 - Designing Category-Level Attributes for Discriminative Visual Recognition.pdf:pdf},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {771--778},
publisher = {Ieee},
title = {{Designing Category-Level Attributes for Discriminative Visual Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6618949},
year = {2013}
}
@article{Tian2013,
author = {Tian, Xinmei and Lu, Yijuan},
doi = {10.1016/j.sigpro.2012.04.018},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tian, Lu - 2013 - Discriminative codebook learning for Web image search.pdf:pdf},
issn = {01651684},
journal = {Signal Processing},
keywords = {Bag-of-visual word,Codebook learning,Subspace learning,Web image search},
month = {aug},
number = {8},
pages = {2284--2292},
publisher = {Elsevier},
title = {{Discriminative codebook learning for Web image search}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0165168412001405},
volume = {93},
year = {2013}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - gradients2.pdf.pdf:pdf},
title = {gradients2.pdf}
}
@article{Lina,
author = {Lin, Chih-jen},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lin - Unknown - Support Vector Machine Solvers.pdf:pdf},
pages = {1--27},
title = {{Support Vector Machine Solvers}}
}
@article{Zhu2012,
author = {Zhu, Xiangxin and Vondrick, Carl and Ramanan, Deva and Fowlkes, Charless},
doi = {10.5244/C.26.80},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhu et al. - 2012 - Do We Need More Training Data or Better Models for Object Detection.pdf:pdf},
isbn = {1-901725-46-4},
journal = {Procedings of the British Machine Vision Conference 2012},
pages = {80.1--80.11},
publisher = {British Machine Vision Association},
title = {{Do We Need More Training Data or Better Models for Object Detection?}},
url = {http://www.bmva.org/bmvc/2012/BMVC/paper080/index.html},
year = {2012}
}
@article{Harchaoui2013,
author = {Harchaoui, Zaid},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Harchaoui - 2013 - Large-scale learning for image classification Large-scale image datasets.pdf:pdf},
number = {July},
title = {{Large-scale learning for image classification Large-scale image datasets}},
year = {2013}
}
@article{Zhang2006,
author = {Zhang, J. and Marsza{\l}ek, M. and Lazebnik, S. and Schmid, C.},
doi = {10.1007/s11263-006-9794-4},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2006 - Local Features and Kernels for Classification of Texture and Object Categories A Comprehensive Study.pdf:pdf},
isbn = {1126300697944},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {image classification,kernel methods,object recognition,scale- and affine-invariant keypoints,support vector machines,texture recognition},
month = {sep},
number = {2},
pages = {213--238},
title = {{Local Features and Kernels for Classification of Texture and Object Categories: A Comprehensive Study}},
url = {http://link.springer.com/10.1007/s11263-006-9794-4},
volume = {73},
year = {2006}
}
@article{Tolias2011,
author = {Tolias, Giorgos and Avrithis, Yannis},
doi = {10.1109/ICCV.2011.6126427},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tolias, Avrithis - 2011 - Speeded-up, relaxed spatial matching.pdf:pdf},
isbn = {978-1-4577-1102-2},
journal = {2011 International Conference on Computer Vision},
month = {nov},
pages = {1653--1660},
publisher = {Ieee},
title = {{Speeded-up, relaxed spatial matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126427},
year = {2011}
}
@article{Avrithis2013,
author = {Avrithis, Yannis and Tolias, Giorgos},
doi = {10.1007/s11263-013-0659-3},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Avrithis, Tolias - 2013 - Hough Pyramid Matching Speeded-Up Geometry Re-ranking for Large Scale Image Retrieval.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
month = {oct},
number = {1},
pages = {1--19},
title = {{Hough Pyramid Matching: Speeded-Up Geometry Re-ranking for Large Scale Image Retrieval}},
url = {http://link.springer.com/10.1007/s11263-013-0659-3},
volume = {107},
year = {2013}
}
@article{Fan2011,
author = {Fan, Bin and Wu, Fuchao},
doi = {10.1109/ICCV.2011.6126294},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Wu - 2011 - Local Intensity Order Pattern for feature description(2).pdf:pdf},
isbn = {978-1-4577-1102-2},
journal = {2011 International Conference on Computer Vision},
month = {nov},
pages = {603--610},
publisher = {Ieee},
title = {{Local Intensity Order Pattern for feature description}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126294},
year = {2011}
}
@misc{,
title = {{LIBSVM}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/libsvm/}
}
@article{Anthony2014,
author = {Anthony, Samuel E and Member, Student and Nakayama, Ken and Cox, David D},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Anthony et al. - 2014 - Perceptual Annotation Measuring Human.pdf:pdf},
title = {{Perceptual Annotation : Measuring Human}},
year = {2014}
}
@article{Fan2012,
author = {Fan, Rong-en and Wang, Xiang-rui and Lin, Chih-jen},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Wang, Lin - 2012 - LIBLINEAR A Library for Large Linear Classification.pdf:pdf},
keywords = {large-scale linear classification,logistic regression,machine learning,open source,support vector machines},
number = {2008},
pages = {1871--1874},
title = {{LIBLINEAR : A Library for Large Linear Classification}},
volume = {9},
year = {2012}
}
@misc{,
title = {{Liblinear}},
url = {http://www.csie.ntu.edu.tw/{~}cjlin/liblinear/}
}
@misc{,
title = {{SVMperf}},
url = {http://www.cs.cornell.edu/people/tj/svm{\_}light/svm{\_}perf.html}
}
@article{List,
author = {List, Nikolas and Simon, Hans Ulrich},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/List, Simon - Unknown - SVM-Optimization and Steepest-Descent Line Search ∗.pdf:pdf},
title = {{SVM-Optimization and Steepest-Descent Line Search ∗}}
}
@article{Juneja2013a,
author = {Juneja, Mayank and Vedaldi, Andrea and Jawahar, C.V. and Zisserman, Andrew},
doi = {10.1109/CVPR.2013.124},
isbn = {978-0-7695-4989-7},
journal = {2013 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {923--930},
publisher = {Ieee},
title = {{Blocks That Shout: Distinctive Parts for Scene Classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6618968},
year = {2013}
}
@article{Doersch2011a,
author = {Doersch, Carl and Singh, Saurabh},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Doersch, Singh - 2011 - What Makes Paris Look like Paris.pdf:pdf},
keywords = {big,computational geography,data,data mining,reference art,visual perception,visual summarization},
title = {{What Makes Paris Look like Paris ?}},
year = {2011}
}
@article{Singha,
author = {Singh, Saurabh and Gupta, Abhinav and Efros, Alexei A},
title = {{Unsupervised Discovery of Mid-Level Discriminative Patches}}
}
@article{Tsochantaridis2005,
author = {Tsochantaridis, Ioannis and Joachims, Thorsten and Hofmann, Thomas and Altun, Yasemin},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tsochantaridis, Hofmann - 2005 - Large Margin Methods for Structured and Interdependent Output Variables.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {1453--1484},
title = {{Large Margin Methods for Structured and Interdependent Output Variables}},
volume = {6},
year = {2005}
}
@article{Bosch2008,
author = {Bosch, Anna and Zisserman, Andrew and Mun, Xavier},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosch, Zisserman, Mun - 2008 - Scene Classification Using a Hybrid Generative Discriminative Approach.pdf:pdf},
number = {4},
pages = {1--16},
title = {{Scene Classification Using a Hybrid Generative / Discriminative Approach}},
volume = {30},
year = {2008}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Fisher gradients.pdf.pdf:pdf},
title = {{Fisher gradients.pdf}}
}
@article{Moosmann2008,
author = {Moosmann, Frank and Member, Student and Nowak, Eric},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moosmann, Member, Nowak - 2008 - Randomized Clustering Forests for Image Classification.pdf:pdf},
number = {9},
pages = {1632--1646},
title = {{Randomized Clustering Forests for Image Classification}},
volume = {30},
year = {2008}
}
@article{Nowak,
author = {Nowak, Eric and Armand, Louis},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nowak, Armand - Unknown - Learning Visual Similarity Measures for Comparing Never Seen Objects 655 avenue de l ' Europe.pdf:pdf},
title = {{Learning Visual Similarity Measures for Comparing Never Seen Objects 655 avenue de l ' Europe}}
}
@article{Lobel,
author = {Lobel, Hans and Mery, Domingo and Soto, Alvaro},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lobel, Mery, Soto - Unknown - Joint Dictionary and Classifier learning for Categorization of Images using a Max-margin Framework.pdf:pdf},
keywords = {Himalaya,Xinrui},
mendeley-tags = {Himalaya,Xinrui},
title = {{Joint Dictionary and Classifier learning for Categorization of Images using a Max-margin Framework}}
}
@article{Nowak2007,
author = {Nowak, Eric and Armand, Louis},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nowak, Armand - 2007 - Learning Visual Similarity Measures for Comparing Never Seen Objects 655 avenue de l ' Europe.pdf:pdf},
isbn = {1424411807},
title = {{Learning Visual Similarity Measures for Comparing Never Seen Objects 655 avenue de l ' Europe}},
year = {2007}
}
@article{Qian1999,
abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
author = {Qian, Ning},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Qian - 1999 - On the momentum term in gradient descent learning algorithms.pdf:pdf},
isbn = {1212543521},
issn = {1879-2782},
journal = {Neural networks : the official journal of the International Neural Network Society},
keywords = {critical damping,damped harmonic oscillator,gradient descent learning algorithm,learning rate,momentum,speed of convergence},
month = {jan},
number = {1},
pages = {145--151},
pmid = {12662723},
title = {{On the momentum term in gradient descent learning algorithms.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12662723},
volume = {12},
year = {1999}
}
@article{Hsu2010,
author = {Hsu, Chih-wei and Chang, Chih-chung and Lin, Chih-jen},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hsu, Chang, Lin - 2010 - A Practical Guide to Support Vector Classification.pdf:pdf},
number = {1},
pages = {1--16},
title = {{A Practical Guide to Support Vector Classification}},
volume = {1},
year = {2010}
}
@article{Krapac,
author = {Krapac, Josip},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krapac - Unknown - Improving web image search results using query-relative classifiers Fr ´.pdf:pdf},
title = {{Improving web image search results using query-relative classifiers Fr ´}}
}
@article{Andrews,
author = {Andrews, Stuart and Tsochantaridis, Ioannis and Hofmann, Thomas},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Andrews, Tsochantaridis, Hofmann - Unknown - Support Vector Machines for Multiple-Instance Learning.pdf:pdf},
title = {{Support Vector Machines for Multiple-Instance Learning}}
}
@article{Jegou2010,
author = {Jegou, Herve and Douze, Matthijs and Schmid, Cordelia and Perez, Patrick},
doi = {10.1109/CVPR.2010.5540039},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jegou et al. - 2010 - Aggregating local descriptors into a compact image representation.pdf:pdf},
isbn = {978-1-4244-6984-0},
journal = {Proceedings of Computer Vision and Pattern Recognition},
month = {jun},
pages = {3304--3311},
publisher = {Ieee},
title = {{Aggregating local descriptors into a compact image representation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540039},
year = {2010}
}
@article{Arandjelovic2012,
author = {Arandjelovic, Relja and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arandjelovic, Zisserman - 2012 - Three things everyone should know to improve object retrieval.pdf:pdf},
journal = {Computer Vision and Pattern Recognition},
title = {{Three things everyone should know to improve object retrieval}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=6248018},
year = {2012}
}
@article{Arandjelovi,
author = {Arandjelovi, Relja and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arandjelovi, Zisserman - Unknown - All about VLAD c.pdf:pdf},
title = {{All about VLAD c}}
}
@inproceedings{Delhumeau2013,
address = {New York, New York, USA},
author = {Delhumeau, Jonathan and Gosselin, Philippe-Henri and J{\'{e}}gou, Herv{\'{e}} and P{\'{e}}rez, Patrick},
booktitle = {Proceedings of ACM International Conference on Multimedia},
doi = {10.1145/2502081.2502171},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Delhumeau et al. - 2013 - Revisiting the VLAD image representation.pdf:pdf},
isbn = {9781450324045},
keywords = {image search,multimedia retrieval,vlad},
pages = {653--656},
publisher = {ACM Press},
title = {{Revisiting the VLAD image representation}},
url = {http://dl.acm.org/citation.cfm?doid=2502081.2502171},
volume = {21},
year = {2013}
}
@inproceedings{Perronnin2010a,
author = {Perronnin, Florent and Liu, Yan and S{\'{a}}nchez, Jorge and Poirier, Herv{\'{e}}},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540009},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perronnin et al. - 2010 - Large-scale image retrieval with compressed Fisher vectors.pdf:pdf},
isbn = {978-1-4244-6984-0},
month = {jun},
pages = {3384--3391},
publisher = {Ieee},
title = {{Large-scale Image Retrieval with Compressed Fisher Vectors}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540009},
year = {2010}
}
@article{Jorge2011,
author = {Jorge, S and Perronnin, Florent and Akata, Zeynep},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jorge, Perronnin, Akata - 2011 - Fisher Vectors for Fine-Grained Visual Categorization.pdf:pdf},
title = {{Fisher Vectors for Fine-Grained Visual Categorization}},
year = {2011}
}
@article{Marchesotti2013,
author = {Marchesotti, Luca},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marchesotti - 2013 - Learning beautiful ( and ugly ) attributes.pdf:pdf},
title = {{Learning beautiful ( and ugly ) attributes}},
year = {2013}
}
@article{Simonyana,
author = {Simonyan, Karen},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Simonyan - Unknown - Deep Fisher Networks for Large-Scale Image Classification.pdf:pdf},
number = {iii},
pages = {1--9},
title = {{Deep Fisher Networks for Large-Scale Image Classification}}
}
@article{Chairs,
author = {Chairs, Program},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chairs - Unknown - CVPR 2014.pdf:pdf},
title = {{CVPR 2014}}
}
@article{Fan2011a,
author = {Fan, Bin and Wu, Fuchao},
doi = {10.1109/ICCV.2011.6126294},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fan, Wu - 2011 - Local Intensity Order Pattern for feature description.pdf:pdf},
isbn = {978-1-4577-1102-2},
journal = {2011 International Conference on Computer Vision},
month = {nov},
pages = {603--610},
publisher = {Ieee},
title = {{Local Intensity Order Pattern for feature description}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126294},
year = {2011}
}
@article{Grail2013,
author = {Grail, Holy},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Grail - 2013 - Knowledge Transfer for Video Understanding.pdf:pdf},
title = {{Knowledge Transfer for Video Understanding}},
year = {2013}
}
@article{Simonyan2012,
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/Documents/Mendeley Local/simonyan12.pdf:pdf},
journal = {Proceedings of European Conference on Computer Vision},
pages = {1--14},
title = {{Descriptor learning using convex optimisation}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-33718-5{\_}18},
year = {2012}
}
@article{Analysis2013,
author = {Analysis, Feature},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Analysis - 2013 - V ideo E diting , R etrieval and T agg i ng on the Go for Collaborative ReMixing and Sharing.pdf:pdf},
number = {October},
pages = {1--5},
title = {{V ideo E diting , R etrieval and T agg i ng on the Go for Collaborative ReMixing and Sharing}},
year = {2013}
}
@inproceedings{Goh,
author = {Goh, Hanlin and Thome, Nicolas and Cord, Matthieu and Lim, Joo-Hwee},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Goh, Thome, Cord - Unknown - Top-Down Regularization of Deep Belief Networks.pdf:pdf},
title = {{Top-Down Regularization of Deep Belief Networks}},
year = {2013}
}
@article{Krapaca,
author = {Krapac, Josip},
title = {{Improving web image search results using query-relative classifiers Fr ´}}
}
@article{Torresani2010,
author = {Torresani, Lorenzo and Szummer, Martin and Fitzgibbon, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Torresani, Szummer, Fitzgibbon - 2010 - Classemes.pdf:pdf},
pages = {776--789},
title = {{Classemes}},
year = {2010}
}
@article{U,
author = {U, {\`{I}} and {\'{A}}, {\~{N}} {\'{Y}} and {\'{Y}}, {\~{N}} and U{\"{i}}, Y and Uh, {\^{I}} and U, {\`{O}}},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/U et al. - Unknown - No Title.pdf:pdf},
title = {{No Title}}
}
@article{Chang2013,
author = {Chang, Chih-chung and Lin, Chih-jen},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang, Lin - 2013 - LIBSVM A Library for Support Vector Machines.pdf:pdf},
keywords = {classification,libsvm,optimization,regression,support vector ma-},
pages = {1--39},
title = {{LIBSVM : A Library for Support Vector Machines}},
year = {2013}
}
@inproceedings{Shalev-shwartz2007b,
author = {Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
booktitle = {International Conference of Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shalev-shwartz, Srebro - 2007 - Pegasos Primal Estimated sub-GrAdient SOlver for SVM.pdf:pdf},
title = {{Pegasos : Primal Estimated sub-GrAdient SOlver for SVM}},
year = {2007}
}
@article{Iii,
author = {Iii, Calculus},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iii - Unknown - Calculus III for CS NOTE ON THE CHAIN RULE WITH JACOBIAN MATRIX AND NEWTON ' S INTERATION.pdf:pdf},
number = {x},
pages = {1--9},
title = {{Calculus III for CS NOTE ON THE CHAIN RULE WITH JACOBIAN MATRIX AND NEWTON ' S INTERATION}}
}
@article{Sanchez2013,
author = {S{\'{a}}nchez, Jorge and Perronnin, Florent and Mensink, Thomas and Verbeek, Jakob},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/S{\'{a}}nchez et al. - 2013 - Image Classification with the Fisher Vector Theory and Practice.pdf:pdf},
number = {May},
title = {{Image Classification with the Fisher Vector : Theory and Practice}},
year = {2013}
}
@article{Cech2010,
author = {Cech, Jan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cech - 2010 - by Cosegmentation.pdf:pdf},
number = {9},
pages = {1568--1581},
title = {{by Cosegmentation}},
volume = {32},
year = {2010}
}
@article{Shechtman2007,
author = {Shechtman, Eli and Irani, Michal},
doi = {10.1109/CVPR.2007.383198},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shechtman, Irani - 2007 - Matching Local Self-Similarities across Images and Videos.pdf:pdf},
isbn = {1-4244-1179-3},
journal = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {1--8},
publisher = {Ieee},
title = {{Matching Local Self-Similarities across Images and Videos}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4270223},
year = {2007}
}
@misc{,
title = {{Neural Networks Tutorial}},
url = {https://docs.google.com/viewer?a=v{\&}pid=sites{\&}srcid=ZGVmYXVsdGRvbWFpbnxsc3ZyMTN8Z3g6MTE4Yzc5OTdiMzZiZGY5MQ}
}
@misc{TheMendeleySupportTeam2011,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/The Mendeley Support Team - 2011 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2011}
}
@inproceedings{Malisiewicz,
author = {Malisiewicz, Tomasz and Efros, Alexei A.},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malisiewicz, Efros - Unknown - Beyond Categories The Visual Memex Model for Reasoning About Object Relationships.pdf:pdf},
title = {{Beyond Categories : The Visual Memex Model for Reasoning About Object Relationships}},
year = {2009}
}
@article{Gehler2009,
author = {Gehler, Peter and Nowozin, Sebastian},
doi = {10.1109/ICCV.2009.5459169},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gehler, Nowozin - 2009 - On feature combination for multiclass object classification.pdf:pdf},
isbn = {978-1-4244-4420-5},
journal = {2009 IEEE 12th International Conference on Computer Vision},
month = {sep},
pages = {221--228},
publisher = {Ieee},
title = {{On feature combination for multiclass object classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459169},
year = {2009}
}
@article{Methods2008,
author = {Methods, Optimization},
doi = {10.1080/1055678xxxxxxxxxxxxx},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Methods - 2008 - Learning with Infinitely Many Kernels via Semi-Infinite Programming ¨.pdf:pdf},
keywords = {68t01,90c06,90c34,ams subject classification,continuous optimization,data mining,infinite programming,machine learning,semi-infinite optimization,support,vector machines},
number = {00},
pages = {1--23},
title = {{Learning with Infinitely Many Kernels via Semi-Infinite Programming ¨}},
volume = {00},
year = {2008}
}
@article{Ng;,
author = {Ng;, Andrew Y. and Jordan, Michael I.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Jordan - Unknown - On discriminative vs. generative classifiers a comparison of logistic regression and naive bayes.pdf:pdf},
title = {{On discriminative vs. generative classifiers: a comparison of logistic regression and naive bayes}}
}
@article{Coates2011,
author = {Coates, Adam and Ng, Andrew Y and Mall, Serra},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Coates, Ng, Mall - 2011 - The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization.pdf:pdf},
keywords = {vq},
mendeley-tags = {vq},
title = {{The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization}},
year = {2011}
}
@inproceedings{Chatfield2011,
author = {Chatfield, K. and Lempitsky, V. and Vedaldi, A. and Zisserman, A.},
booktitle = {British Machine Vision Conference},
doi = {10.5244/C.25.76},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chatfield et al. - 2011 - The devil is in the details an evaluation of recent feature encoding methods.pdf:pdf},
isbn = {1-901725-43-X},
number = {1},
publisher = {British Machine Vision Association},
title = {{The devil is in the details: an evaluation of recent feature encoding methods}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper76/index.html},
year = {2011}
}
@article{Patrick2012,
author = {Patrick, P},
isbn = {2011277906},
title = {{Nearest neighbor search for arbitrary kernels with explicit embeddings}},
year = {2012}
}
@article{Romberg,
author = {Romberg, Stefan and August, Moritz and Ries, Christian X and Lienhart, Rainer},
title = {{Robust Feature Bundling}}
}
@article{Fr,
author = {Fr, Yu Su},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fr - Unknown - Improving Image Classification using Semantic Attributes.pdf:pdf},
keywords = {bag-of-words model,image classification,semantic attribute,visual words disambiguation},
title = {{Improving Image Classification using Semantic Attributes}}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Pedro Felzenszwalb's Home Page.html:html},
title = {{Pedro Felzenszwalb's Home Page}},
url = {http://www.cs.brown.edu/{~}pff/},
urldate = {2012-12-07}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Nuit Blanche Learning Compressed Sensing.html:html},
title = {{Nuit Blanche: Learning Compressed Sensing}},
url = {http://nuit-blanche.blogspot.fr/p/teaching-compressed-sensing.html},
urldate = {2012-12-06}
}
@article{Wijnhoven2010,
author = {Wijnhoven, R.G.J. and de With, P.H.N.},
doi = {10.1109/ICPR.2010.112},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wijnhoven, de With - 2010 - Fast Training of Object Detection Using Stochastic Gradient Descent.pdf:pdf},
isbn = {978-1-4244-7542-1},
journal = {2010 20th International Conference on Pattern Recognition},
month = {aug},
pages = {424--427},
publisher = {Ieee},
title = {{Fast Training of Object Detection Using Stochastic Gradient Descent}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5597822},
year = {2010}
}
@article{Shapiro2007,
author = {Shapiro, Alexander and Philpott, Andy},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shapiro, Philpott - 2007 - A Tutorial on Stochastic Programming.pdf:pdf},
title = {{A Tutorial on Stochastic Programming}},
year = {2007}
}
@article{Labs,
author = {Labs, N E C and Way, Independence and Nj, Princeton},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Labs, Way, Nj - Unknown - Stochastic Learning.pdf:pdf},
title = {{Stochastic Learning}}
}
@article{Perronnin2010,
author = {Perronnin, Florent and S{\'{a}}nchez, J and Mensink, Thomas},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perronnin, S{\'{a}}nchez, Mensink - 2010 - Improving the fisher kernel for large-scale image classification.pdf:pdf},
journal = {European Conference on Computer Vision},
title = {{Improving the fisher kernel for large-scale image classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-642-15561-1{\_}11},
year = {2010}
}
@article{Boix,
author = {Boix, Xavier and Roig, Gemma and Gool, Luc Van},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boix, Roig, Gool - Unknown - Nested Sparse Quantization for Efficient Feature Coding.pdf:pdf},
title = {{Nested Sparse Quantization for Efficient Feature Coding}}
}
@article{Korena,
author = {Koren, Tomer and Srebro, Nathan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koren, Srebro - Unknown - Beating SGD Learning SVMs in Sublinear Time.pdf:pdf},
pages = {1--16},
title = {{Beating SGD : Learning SVMs in Sublinear Time}}
}
@article{Krapacb,
author = {Krapac, Josip and Verbeek, Jakob},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krapac, Verbeek - Unknown - Modeling Spatial Layout with Fisher Vectors for Image Categorization.pdf:pdf},
title = {{Modeling Spatial Layout with Fisher Vectors for Image Categorization}}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - No Title.pdf:pdf},
title = {{No Title}}
}
@article{Shapiro2007a,
author = {Shapiro, Alexander and Philpott, Andy},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shapiro, Philpott - 2007 - A Tutorial on Stochastic Programming.pdf:pdf},
title = {{A Tutorial on Stochastic Programming}},
year = {2007}
}
@misc{Bottou,
author = {Bottou, Leon},
title = {{No Title}},
url = {http://leon.bottou.org/projects/sgd}
}
@article{Snoek2005,
address = {New York, New York, USA},
author = {Snoek, Cees G. M. and Worring, Marcel and Smeulders, Arnold W. M.},
doi = {10.1145/1101149.1101236},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Worring, Smeulders - 2005 - Early versus late fusion in semantic video analysis.pdf:pdf},
isbn = {1595930442},
journal = {Proceedings of the 13th annual ACM international conference on Multimedia - MULTIMEDIA '05},
keywords = {early fusion,late fusion,multimedia understanding,seman-,tic concept detection},
pages = {399},
publisher = {ACM Press},
title = {{Early versus late fusion in semantic video analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1101149.1101236},
year = {2005}
}
@article{Rosasco2004,
abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
author = {Rosasco, Lorenzo and {De Vito}, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
doi = {10.1162/089976604773135104},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rosasco et al. - 2004 - Are loss functions all the same.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Learning,Learning: physiology,Linear Models,Models, Neurological,Statistics as Topic},
month = {may},
number = {5},
pages = {1063--76},
pmid = {15070510},
title = {{Are loss functions all the same?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15070510},
volume = {16},
year = {2004}
}
@article{Rennie,
author = {Rennie, Jason D M},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rennie - Unknown - Loss Functions for Preference Levels Regression with Discrete Ordered Labels.pdf:pdf},
title = {{Loss Functions for Preference Levels : Regression with Discrete Ordered Labels}}
}
@article{Allwein2000,
author = {Allwein, Erin L},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Allwein - 2000 - Reducing Multiclass to Binary A Unifying Approach for Margin Classifiers.pdf:pdf},
pages = {113--141},
title = {{Reducing Multiclass to Binary : A Unifying Approach for Margin Classifiers}},
volume = {1},
year = {2000}
}
@article{Chen2011,
author = {Chen, David and Tsai, Sam and Chandrasekhar, Vijay and Takacs, Gabriel and Chen, Huizhong and Vedantham, Ramakrishna and Grzeszczuk, Radek and Girod, Bernd},
doi = {10.1109/ACSSC.2011.6190128},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chen et al. - 2011 - Residual Enhanced Visual Vectors for on-device image matching.pdf:pdf},
isbn = {978-1-4673-0323-1},
journal = {2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Computers (ASILOMAR)},
month = {nov},
pages = {850--854},
publisher = {Ieee},
title = {{Residual Enhanced Visual Vectors for on-device image matching}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6190128},
year = {2011}
}
@article{Cartis2011,
author = {Cartis, Coralia and Gould, Nicholas I M and Toint, Philippe L},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cartis, Gould, Toint - 2011 - Optimal Newton-type methods for nonconvex smooth optimization problems.pdf:pdf},
title = {{Optimal Newton-type methods for nonconvex smooth optimization problems}},
year = {2011}
}
@article{Clausi2002,
author = {Clausi, D.a.},
doi = {10.1016/S0031-3203(01)00138-8},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Clausi - 2002 - K-means Iterative Fisher (KIF) unsupervised clustering algorithm applied to image texture segmentation.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {binary hierarchical clustering,co-occurrence probabilities,computer vision,discriminant,fisher linear,gabor {\"{y}}lters,image segmentation,k-means,texture segmentation},
month = {sep},
number = {9},
pages = {1959--1972},
title = {{K-means Iterative Fisher (KIF) unsupervised clustering algorithm applied to image texture segmentation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320301001388},
volume = {35},
year = {2002}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2001 - Learning with Kernels.pdf:pdf},
title = {{Learning with Kernels}},
year = {2001}
}
@article{Nguyen,
author = {Nguyen, Hien V and Patel, Vishal M and Nasrabadi, Nasser M and Chellappa, Rama},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen et al. - Unknown - Sparse Embedding A Framework For Sparsity Promoting Dimensionality Reduction.pdf:pdf},
title = {{Sparse Embedding : A Framework For Sparsity Promoting Dimensionality Reduction}}
}
@article{Perronnin2006,
author = {Perronnin, Florent and Dance, Christopher and Csurka, Gabriela and Bressan, Marco},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perronnin et al. - 2006 - Categorization.pdf:pdf},
pages = {464--475},
title = {{Categorization}},
year = {2006}
}
@article{Wang2010a,
author = {Wang, Jinjun and Yang, Jianchao and Yu, Kai and Lv, Fengjun and Huang, Thomas and Gong, Yihong},
doi = {10.1109/CVPR.2010.5540018},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2010 - Locality-constrained Linear Coding for image classification.pdf:pdf},
isbn = {978-1-4244-6984-0},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {3360--3367},
publisher = {Ieee},
title = {{Locality-constrained Linear Coding for image classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5540018},
year = {2010}
}
@inproceedings{Yang2009,
author = {Yang, Jianchao and Yu, Kai and Gong, Yihong and Huang, Thomas},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206757},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang - 2009 - Linear spatial pyramid matching using sparse coding for image classification.pdf:pdf},
isbn = {978-1-4244-3992-8},
month = {jun},
pages = {1794--1801},
publisher = {Ieee},
title = {{Linear spatial pyramid matching using sparse coding for image classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5206757},
year = {2009}
}
@article{Jegou2011,
author = {J{\'{e}}gou, Her{\'{e}} and Perronnin, Florent and Douze, Matthijs and Jorge, S{\'{a}}nchez and Patrick, P{\'{e}}rez and Schmid, Cordelia},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perronnin et al. - 2011 - Aggregating local image descriptors into compact codes.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
pages = {1--12},
title = {{Aggregating Local Image Descriptors into Compact Codes}},
year = {2011}
}
@article{Jaakkola,
author = {Jaakkola, Tommi S and Haussler, David},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaakkola, Haussler - Unknown - Exploiting generative models in discriminative classi ers(2).pdf:pdf},
title = {{Exploiting generative models in discriminative classi ers}}
}
@inproceedings{Perronnin,
author = {Perronnin, Florent and Dance, Christopher and Maupertuis, De},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Perronnin, Dance, Maupertuis - Unknown - Fisher Kernels on Visual Vocabularies for Image Categorization.pdf:pdf},
title = {{Fisher Kernels on Visual Vocabularies for Image Categorization}},
year = {2007}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Exploiting generative models in discriminative classifiers.pdf:pdf},
title = {{Exploiting generative models in discriminative classifiers}}
}
@inproceedings{Yang2010a,
author = {Yang, Jianchao and Yu, Kai and Huang, Thomas},
booktitle = {Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5539958},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Yu, Huang - 2010 - Supervised translation-invariant sparse coding.pdf:pdf},
isbn = {978-1-4244-6984-0},
month = {jun},
pages = {3517--3524},
publisher = {Ieee},
title = {{Supervised translation-invariant sparse coding}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539958},
year = {2010}
}
@article{Bradley2008,
author = {Bradley, David M and Bagnell, J Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bradley, Bagnell - 2008 - Differential Sparse Coding Differentiable Sparse Coding.pdf:pdf},
title = {{Differential Sparse Coding Differentiable Sparse Coding}},
year = {2008}
}
@article{Lazar2010,
author = {Lazar, Alina},
doi = {10.1109/ICMLA.2010.137},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazar - 2010 - A Comparison of Linear Support Vector Machine Algorithms on Large Non-Sparse Datasets.pdf:pdf},
isbn = {978-1-4244-9211-4},
journal = {2010 Ninth International Conference on Machine Learning and Applications},
month = {dec},
pages = {879--882},
publisher = {Ieee},
title = {{A Comparison of Linear Support Vector Machine Algorithms on Large Non-Sparse Datasets}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5708960},
year = {2010}
}
@article{Agarwal2004,
abstract = {We study the problem of detecting objects in still, gray-scale images. Our primary focus is the development of a learning-based approach to the problem that makes use of a sparse, part-based representation. A vocabulary of distinctive object parts is automatically constructed from a set of sample images of the object class of interest; images are then represented using parts from this vocabulary, together with spatial relations observed among the parts. Based on this representation, a learning algorithm is used to automatically learn to detect instances of the object class in new images. The approach can be applied to any object with distinguishable parts in a relatively fixed spatial configuration; it is evaluated here on difficult sets of real-world images containing side views of cars, and is seen to successfully detect objects in varying conditions amidst background clutter and mild occlusion. In evaluating object detection approaches, several important methodological issues arise that have not been satisfactorily addressed in previous work. A secondary focus of this paper is to highlight these issues and to develop rigorous evaluation standards for the object detection problem. A critical evaluation of our approach under the proposed standards is presented.},
author = {Agarwal, Shivani and Awan, Aatif and Roth, Dan},
doi = {10.1109/TPAMI.2004.108},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal, Awan, Roth - 2004 - Learning to detect objects in images via a sparse, part-based representation.pdf:pdf},
isbn = {0162-8828 VO - 26},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Evaluation/methodology,Image representation,Machine learning,Must read,Object detection,currently reading},
mendeley-tags = {Must read,currently reading},
month = {nov},
number = {11},
pages = {1475--1490},
pmid = {15521495},
title = {{Learning to detect objects in images via a sparse, part-based representation}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=1335452},
volume = {26},
year = {2004}
}
@inproceedings{Rigamonti2010,
author = {Rigamonti, Roberto and Brown, Matthew and Lepetit, Vincent},
booktitle = {CVPR},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rigamonti, Brown, Lepetit - 2010 - Is Sparsity Really Relevant for Image Classification.pdf:pdf},
keywords = {Image classification,currently reading,natural images,sparsity},
mendeley-tags = {currently reading},
title = {{Is Sparsity Really Relevant for Image Classification?}},
year = {2010}
}
@article{Figueiredo2003,
author = {Figueiredo, A T and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Figueiredo - 2003 - Adaptive Sparseness for Supervised Learning.pdf:pdf},
journal = {Analysis},
keywords = {currently reading},
mendeley-tags = {currently reading},
number = {9},
pages = {1150--1159},
title = {{Adaptive Sparseness for Supervised Learning ´}},
volume = {25},
year = {2003}
}
@article{Topics,
annote = {1) Frame
2) Gibbs distribution
3) **Compressed sensing** - tutorials at http://dsp.rice.edu/cs
        
4) Boosting
5) Support Vector Machine
6) Expectation maximization
7) Random trees
8) Image indexing - Lowe, Herve, LSH, others?
9) Vocabulary construction (look at references in Herve{\&}{\#}039;s papers)
10) Contextual dissimilarity measure / Hubs in space
11) RANSAC
12) Transformations - affine / projective / homographies / homogeneous coordinates.
13) Expectation maximization
14) Machine learning
15) Bayesian estimation
16) Particle filters / Kalman filters
17) Self-organizing map, Kohonen
18) Hough transform
19) Xavier Lepennec / Geodesic distance
20) Lie Algebra / Riemmanian geometry
21) FLANN, KD-TREE
22) Iterative soft thresholding},
author = {Topics, Interesting},
keywords = {must read},
mendeley-tags = {must read},
title = {{Topics of interest}}
}
@misc{MatthewBrown,
annote = {undefined},
author = {{Matthew Brown} and {Gang Hua} and {Simon Winder}},
booktitle = {Pattern Analysis and Machine Intelligence},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matthew Brown, Gang Hua, Simon Winder - Unknown - Discriminative Learning of Local Image Descriptors.html:html},
keywords = {currently reading,patrick},
mendeley-tags = {currently reading,patrick},
title = {{Discriminative Learning of Local Image Descriptors}},
url = {http://cvlab.epfl.ch/{~}brown/papers/pami2010.pdf},
urldate = {2011-02-11}
}
@article{Wipf2010,
author = {Wipf, David and Nagarajan, Srikantan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wipf, Nagarajan - 2010 - Iterative Reweighted l1 and l2 Methods for Finding Sparse Solutions.pdf:pdf},
journal = {IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING},
keywords = {sparse representations},
mendeley-tags = {sparse representations},
number = {2},
pages = {317--329},
title = {{Iterative Reweighted l1 and l2 Methods for Finding Sparse Solutions}},
volume = {4},
year = {2010}
}
@article{Chandrasekhar2011,
author = {Chandrasekhar, Vijay and Takacs, Gabriel and Chen, David M. and Tsai, Sam S. and Reznik, Yuriy and Grzeszczuk, Radek and Girod, Bernd},
doi = {10.1007/s11263-011-0453-z},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chandrasekhar et al. - 2011 - Compressed Histogram of Gradients A Low-Bitrate Descriptor.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {chog,content-based image retrieval,feature descriptor,histogram-of-gradients,low bitrate,mobile visual,search},
month = {may},
number = {3},
pages = {384--399},
title = {{Compressed Histogram of Gradients: A Low-Bitrate Descriptor}},
url = {http://www.springerlink.com/index/10.1007/s11263-011-0453-z},
volume = {96},
year = {2011}
}
@article{Sums1930,
author = {Sums, Exponential and Langer, B Y R E},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sums, Langer - 1930 - m !.pdf:pdf},
number = {1},
pages = {213--239},
title = {m *!},
year = {1930}
}
@article{Uo,
author = {{\`{U}}{\"{o}}, {\AA} {\"{U}} {\`{E}} Ð {\`{O}} {\O} {\O} {\`{U}} {\O} and {\~{A}}{\'{y}}, {\'{O}} Ð {\'{O}} and {\`{O}}{\'{y}}, {\`{I}} {\`{U}} {\`{O}} {\`{O}} {\"{O}} {\~{N}} and {\`{O}}{\'{o}}ð{\'{o}}, {\'{O}}{\^{u}}{\`{u}}ð {\`{I}}},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\`{U}}{\"{o}} et al. - Unknown - {\"{E}}{\^{i}}{\aa} {\`{o}} {\~{a}} {\"{o}}{\`{o}} ð {\aa} {\o} {\'{o}} ×.pdf:pdf},
title = {{{\"{E}}{\^{i}}{\aa} {\`{o}} {\~{a}} {\"{o}}{\`{o}} ð {\aa} {\o} {\'{o}} ×}}
}
@techreport{Ng,
author = {Ng, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - Unknown - CS229 Lecture notes. Part V Support Vector Machines.pdf:pdf},
institution = {Stanford University},
number = {x},
pages = {1--25},
title = {{CS229 Lecture notes. Part V: Support Vector Machines}}
}
@article{Krapac2011,
author = {Krapac, Josip and Verbeek, Jakob and Jurie, Frederic},
doi = {10.1109/ICCV.2011.6126406},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Krapac, Verbeek, Jurie - 2011 - Modeling spatial layout with fisher vectors for image categorization.pdf:pdf},
isbn = {978-1-4577-1102-2},
journal = {2011 International Conference on Computer Vision},
month = {nov},
pages = {1487--1494},
publisher = {Ieee},
title = {{Modeling spatial layout with fisher vectors for image categorization}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126406},
year = {2011}
}
@article{Huang2010,
author = {Huang, Junzhou and Zhang, Tong},
doi = {10.1214/09-AOS778},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang, Zhang - 2010 - The benefit of group sparsity.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {aug},
number = {4},
pages = {1978--2004},
title = {{The benefit of group sparsity}},
url = {http://projecteuclid.org/euclid.aos/1278861240},
volume = {38},
year = {2010}
}
@article{Gong,
author = {Gong, Yunchao and Lazebnik, Svetlana and Science, Computer and Hill, U N C Chapel},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gong et al. - Unknown - Iterative Quantization A Procrustean Approach to Learning Binary Codes.pdf:pdf},
title = {{Iterative Quantization : A Procrustean Approach to Learning Binary Codes}}
}
@book{Lindeberg2010,
author = {Lindeberg, Tony},
booktitle = {Journal of Mathematical Imaging and Vision},
doi = {10.1007/s10851-010-0242-2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindeberg - 2010 - Generalized Gaussian Scale-Space Axiomatics Comprising Linear Scale-Space, Affine Scale-Space and Spatio-Temporal Sca.pdf:pdf},
isbn = {1085101002},
issn = {0924-9907},
keywords = {affine,causality,computer,diffusion,gaussian derivative,gaussian kernel,image processing,multi-scale representation,non-enhancement,of local extrema,receptive field,scale invariance,scale-space,scale-space axioms,spatial,spatio-,temporal,time-recursive,vision},
month = {dec},
number = {1},
pages = {36--81},
title = {{Generalized Gaussian Scale-Space Axiomatics Comprising Linear Scale-Space, Affine Scale-Space and Spatio-Temporal Scale-Space}},
url = {http://www.springerlink.com/index/10.1007/s10851-010-0242-2},
volume = {40},
year = {2010}
}
@article{Mairal2012,
abstract = {Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience, and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
doi = {10.1109/TPAMI.2011.156},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal, Bach, Ponce - 2012 - Task-driven dictionary learning(2).pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {apr},
number = {4},
pages = {791--804},
pmid = {21808090},
title = {{Task-driven dictionary learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21808090},
volume = {34},
year = {2012}
}
@article{Vbc1996,
author = {Vbc, Tutorial and Bart, M and Romeny, Haar},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vbc, Bart, Romeny - 1996 - Introduction to Scale-Space Theory Multiscale Geometric Image Analysis Fourth International Conference on Vi.pdf:pdf},
number = {September},
title = {{Introduction to Scale-Space Theory : Multiscale Geometric Image Analysis Fourth International Conference on Visualization in Biomedical Computing}},
year = {1996}
}
@article{Mairal2012a,
abstract = {Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience, and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
doi = {10.1109/TPAMI.2011.156},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal, Bach, Ponce - 2012 - Task-driven dictionary learning.pdf:pdf},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {apr},
number = {4},
pages = {791--804},
pmid = {21808090},
title = {{Task-driven dictionary learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21808090},
volume = {34},
year = {2012}
}
@article{Kittler1998,
author = {Kittler, Josef and Society, Ieee Computer and Hatef, Mohamad and Duin, Robert P W and Matas, Jiri},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kittler et al. - 1998 - On Combining Classifiers.pdf:pdf},
number = {3},
pages = {226--239},
title = {{On Combining Classifiers}},
volume = {20},
year = {1998}
}
@article{Forss,
author = {Forss, Per-erik and Lowe, David G},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Forss, Lowe - Unknown - Shape Descriptors for Maximally Stable Extremal Regions.pdf:pdf},
title = {{Shape Descriptors for Maximally Stable Extremal Regions}}
}
@article{Forssa,
author = {Forss, Per-erik},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Forss - Unknown - Maximally Stable Colour Regions for Recognition and Matching.pdf:pdf},
title = {{Maximally Stable Colour Regions for Recognition and Matching}}
}
@article{Matas2004,
author = {Matas, J and Chum, O and Urban, M and Pajdla, T},
doi = {10.1016/j.imavis.2004.02.006},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Matas et al. - 2004 - Robust wide-baseline stereo from maximally stable extremal regions.pdf:pdf},
issn = {02628856},
journal = {Image and Vision Computing},
month = {sep},
number = {10},
pages = {761--767},
title = {{Robust wide-baseline stereo from maximally stable extremal regions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0262885604000435},
volume = {22},
year = {2004}
}
@article{Lehmann2011,
annote = {BMVC 2011, Best Impact Paper},
author = {Lehmann, Alain D and Gehler, Peter V},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lehmann, Gehler - 2011 - Branch {\&} Rank Non-Linear Object Detection.pdf:pdf},
pages = {1--11},
title = {{Branch {\&} Rank : Non-Linear Object Detection}},
year = {2011}
}
@article{Mensink2012,
author = {Mensink, Thomas and Verbeek, Jakob and Perronnin, Florent and Csurka, Gabriela},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mensink et al. - 2012 - Metric Learning for Large Scale Image Classification Generalizing to New Classes at Near-Zero Cost.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence},
title = {{Metric Learning for Large Scale Image Classification : Generalizing to New Classes at Near-Zero Cost}},
year = {2012}
}
@article{Hofmann1999,
address = {New York, New York, USA},
author = {Hofmann, Thomas},
doi = {10.1145/312624.312649},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hofmann - 1999 - Probabilistic latent semantic indexing.pdf:pdf},
isbn = {1581130961},
journal = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval - SIGIR '99},
pages = {50--57},
publisher = {ACM Press},
title = {{Probabilistic latent semantic indexing}},
url = {http://portal.acm.org/citation.cfm?doid=312624.312649},
year = {1999}
}
@article{Philbin,
author = {Philbin, James and Sivic, Josef and Isard, Michael and Zisserman, Andrew and Group, Visual Geometry and Valley, Silicon},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Philbin et al. - Unknown - Total Recall Automatic Query Expansion with a Generative Feature Model for Object Retrieval.pdf:pdf},
title = {{Total Recall: Automatic Query Expansion with a Generative Feature Model for Object Retrieval}}
}
@article{Url2007,
author = {Url, Stable and Society, Royal Statistical},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Url, Society - 2007 - Maximum Likelihood from Incomplete Data via the EM Algorithm.pdf:pdf},
number = {1},
pages = {1--38},
title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
volume = {39},
year = {2007}
}
@article{Blei2003,
author = {Blei, David M and Ng, Andrew Y and Jordan, Michael I},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blei, Ng, Jordan - 2003 - Latent Dirichlet Allocation.pdf:pdf},
pages = {993--1022},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}
@article{Philbin2010,
author = {Philbin, James and Sivic, Josef and Zisserman, Andrew},
doi = {10.1007/s11263-010-0363-5},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Philbin, Sivic, Zisserman - 2010 - Geometric Latent Dirichlet Allocation on a Matching Graph for Large-scale Image Datasets.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {generative models,large-scale retrieval,object discovery,topic},
month = {jun},
number = {2},
pages = {138--153},
title = {{Geometric Latent Dirichlet Allocation on a Matching Graph for Large-scale Image Datasets}},
url = {http://www.springerlink.com/index/10.1007/s11263-010-0363-5},
volume = {95},
year = {2010}
}
@article{Barata,
archivePrefix = {arXiv},
arxivId = {arXiv:1110.6882v1},
author = {Barata, J C A and Hussein, M S},
eprint = {arXiv:1110.6882v1},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barata, Hussein - Unknown - The Moore-Penrose Pseudoinverse. A Tutorial Review of the Theory.pdf:pdf},
number = {3},
pages = {1--23},
title = {{The Moore-Penrose Pseudoinverse. A Tutorial Review of the Theory}}
}
@article{Lee,
author = {Lee, Honglak and Ng, Andrew Y},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Ng - Unknown - Efficient sparse coding algorithms.pdf:pdf},
title = {{Efficient sparse coding algorithms}}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - olshausen-field-emergence-simple-cells.pdf.pdf:pdf},
title = {olshausen-field-emergence-simple-cells.pdf}
}
@article{Olshausen2005,
abstract = {A wide variety of papers have reviewed what is known about the function of primary visual cortex. In this review, rather than stating what is known, we attempt to estimate how much is still unknown about V1 function. In particular, we identify five problems with the current view of V1 that stem largely from experimental and theoretical biases, in addition to the contributions of nonlinearities in the cortex that are not well understood. Our purpose is to open the door to new theories, a number of which we describe, along with some proposals for testing them.},
author = {Olshausen, Bruno a and Field, David J},
doi = {10.1162/0899766054026639},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olshausen, Field - 2005 - How close are we to understanding v1.pdf:pdf},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Animals,Bias (Epidemiology),Humans,Models, Neurological,Neurons,Neurons: classification,Neurons: physiology,Nonlinear Dynamics,Visual Cortex,Visual Cortex: cytology,Visual Cortex: physiology,Visual Pathways,Visual Pathways: physiology},
month = {aug},
number = {8},
pages = {1665--99},
pmid = {15969914},
title = {{How close are we to understanding v1?}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15969914},
volume = {17},
year = {2005}
}
@article{Ng2007,
author = {Ng, Jeffrey and Bharath, Anil a and Zhaoping, Li},
doi = {10.1155/2007/97961},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Bharath, Zhaoping - 2007 - A Survey of Architecture and Function of the Primary Visual Cortex (V1).pdf:pdf},
issn = {1687-6180},
journal = {EURASIP Journal on Advances in Signal Processing},
number = {1},
pages = {097961},
title = {{A Survey of Architecture and Function of the Primary Visual Cortex (V1)}},
url = {http://asp.eurasipjournals.com/content/2007/1/097961},
volume = {2007},
year = {2007}
}
@inproceedings{Gregor2010,
author = {Gregor, Karol and LeCun, Yann},
booktitle = {International Conference on Machine Learning},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregor, LeCun - 2010 - Learning Fast Approximations of Sparse Coding.pdf:pdf},
keywords = {deep learning,feature extraction,sparse coding},
title = {{Learning Fast Approximations of Sparse Coding}},
year = {2010}
}
@misc{Wolfson,
author = {Wolfson, Haim J. and Rigoutsos, Isidore},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolfson, Rigoutsos - Unknown - Geometric Hashing An Overview.pdf:pdf},
title = {{Geometric Hashing: An Overview}}
}
@article{Vedaldi2012,
abstract = {Large scale nonlinear support vector machines (SVMs) can be approximated by linear ones using a suitable feature map. The linear SVMs are in general much faster to learn and evaluate (test) than the original nonlinear SVMs. This work introduces explicit feature maps for the additive class of kernels, such as the intersection, Hellinger's, and $\chi$2 kernels, commonly used in computer vision, and enables their use in large scale problems. In particular, we: 1) provide explicit feature maps for all additive homogeneous kernels along with closed form expression for all common kernels; 2) derive corresponding approximate finite-dimensional feature maps based on a spectral analysis; and 3) quantify the error of the approximation, showing that the error is independent of the data dimension and decays exponentially fast with the approximation order for selected kernels such as $\chi$2. We demonstrate that the approximations have indistinguishable performance from the full kernels yet greatly reduce the train/test times of SVMs. We also compare with two other approximation methods: Nystrom's approximation of Perronnin et al., which is data dependent, and the explicit map of Maji and Berg for the intersection kernel, which, as in the case of our approximations, is data independent. The approximations are evaluated on a number of standard data sets, including Caltech-101, Daimler-Chrysler pedestrians, and INRIA pedestrians.},
author = {Vedaldi, Andrea and Zisserman, Andrew},
doi = {10.1109/TPAMI.2011.153},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vedaldi, Zisserman - 2012 - Efficient additive kernels via explicit feature maps.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {mar},
number = {3},
pages = {480--92},
pmid = {21808094},
title = {{Efficient additive kernels via explicit feature maps.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21808094},
volume = {34},
year = {2012}
}
@article{Vedaldi,
author = {Vedaldi, Andrea and Soatto, Stefano},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vedaldi, Soatto - Unknown - Features for Recognition Viewpoint Invariance for Non-Planar Scenes ∗.pdf:pdf},
isbn = {4962003100},
title = {{Features for Recognition : Viewpoint Invariance for Non-Planar Scenes ∗}}
}
@article{Vedaldia,
author = {Vedaldi, Andrea and Ling, Haibin and Soatto, Stefano},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vedaldi, Ling, Soatto - Unknown - Knowing a Good Feature When You See It Ground Truth and Methodology to Evaluate Local Features for Re.pdf:pdf},
pages = {27--49},
title = {{Knowing a Good Feature When You See It : Ground Truth and Methodology to Evaluate Local Features for Recognition}}
}
@inproceedings{Parkhi,
author = {Parkhi, O. M. and Vedaldi, A. and Zisserman, A.},
booktitle = {International Workshop on Image Analysis for Multimedia Interactive Services},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Parkhi, Zisserman - Unknown - ON-THE-FLY SPECIFIC PERSON RETRIEVAL Andrea Vedaldi Off-line and On-line Processing.pdf:pdf},
title = {{ON-THE-FLY SPECIFIC PERSON RETRIEVAL}},
year = {2012}
}
@article{Holub2008,
author = {Holub, Alex and Moreels, Pierre and Perona, Pietro},
doi = {10.1109/AFGR.2008.4813463},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holub, Moreels, Perona - 2008 - Unsupervised clustering for google searches of celebrity images.pdf:pdf},
isbn = {978-1-4244-2153-4},
journal = {2008 8th IEEE International Conference on Automatic Face {\&} Gesture Recognition},
keywords = {also benefit,and do not utilize,associated with the images,drastically,has the potential to,images,increase the precision of,mation for celebrity searches,such queries and could,using visual infor-,visual information within the},
month = {sep},
pages = {1--8},
publisher = {Ieee},
title = {{Unsupervised clustering for google searches of celebrity images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4813463},
year = {2008}
}
@article{Rublee2011,
author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
doi = {10.1109/ICCV.2011.6126544},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rublee et al. - 2011 - ORB An efficient alternative to SIFT or SURF.pdf:pdf},
isbn = {978-1-4577-1102-2},
journal = {2011 International Conference on Computer Vision},
month = {nov},
pages = {2564--2571},
publisher = {Ieee},
title = {{ORB: An efficient alternative to SIFT or SURF}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6126544},
year = {2011}
}
@article{Hundelshausen,
author = {Hundelshausen, Felix Von},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hundelshausen - Unknown - D-Nets Beyond Patch-Based Image Descriptors.pdf:pdf},
title = {{D-Nets : Beyond Patch-Based Image Descriptors}}
}
@article{Nguyen2010,
author = {Nguyen, Minh Hoai and de la Torre, Fernando},
doi = {10.1016/j.patcog.2009.09.003},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nguyen, de la Torre - 2010 - Optimal feature selection for support vector machines.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {feature extraction,feature selection,support vector machine},
month = {mar},
number = {3},
pages = {584--591},
title = {{Optimal feature selection for support vector machines}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0031320309003409},
volume = {43},
year = {2010}
}
@article{Tompkin,
author = {Tompkin, James and Kim, Kwang In and Kautz, Jan and Informatik, M P I},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tompkin et al. - Unknown - Videoscapes Exploring Sparse , Unstructured Video Collections.pdf:pdf},
keywords = {spatio-temporal exploration,video collections},
title = {{Videoscapes : Exploring Sparse , Unstructured Video Collections}}
}
@article{Eccv,
author = {Eccv, Anonymous},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eccv - Unknown - Explicit embeddings for nearest neighbor search with arbitrary kernels.pdf:pdf},
title = {{Explicit embeddings for nearest neighbor search with arbitrary kernels}}
}
@article{Yilmaz,
author = {Yilmaz, Emine and Aslam, Javed A and Ave, Huntington},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yilmaz, Aslam, Ave - Unknown - Inferred AP Estimating Average Precision with Incomplete Judgments.pdf:pdf},
title = {{Inferred AP : Estimating Average Precision with Incomplete Judgments}}
}
@article{Li,
author = {Li, Li-jia and Su, Hao and Xing, Eric P and Fei-fei, Li},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - Unknown - Object Bank A High-Level Image Representation for Scene Classification {\&} Semantic Feature Sparsification.pdf:pdf},
pages = {1--9},
title = {{Object Bank : A High-Level Image Representation for Scene Classification {\&} Semantic Feature Sparsification}}
}
@article{Xiao2010,
author = {Xiao, Jianxiong and Hays, James and Ehinger, Krista a. and Oliva, Aude and Torralba, Antonio},
doi = {10.1109/CVPR.2010.5539970},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xiao et al. - 2010 - SUN database Large-scale scene recognition from abbey to zoo.pdf:pdf},
isbn = {978-1-4244-6984-0},
journal = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
month = {jun},
pages = {3485--3492},
publisher = {Ieee},
title = {{SUN database: Large-scale scene recognition from abbey to zoo}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5539970},
year = {2010}
}
@article{Schroff2011,
abstract = {The objective of this work is to automatically generate a large number of images for a specified object class. A multimodal approach employing both text, metadata, and visual features is used to gather many high-quality images from the Web. Candidate images are obtained by a text-based Web search querying on the object identifier (e.g., the word penguin). The Webpages and the images they contain are downloaded. The task is then to remove irrelevant images and rerank the remainder. First, the images are reranked based on the text surrounding the image and metadata features. A number of methods are compared for this reranking. Second, the top-ranked images are used as (noisy) training data and an SVM visual classifier is learned to improve the ranking further. We investigate the sensitivity of the cross-validation procedure to this noisy training data. The principal novelty of the overall method is in combining text/metadata and visual features in order to achieve a completely automatic ranking of the images. Examples are given for a selection of animals, vehicles, and other classes, totaling 18 classes. The results are assessed by precision/recall curves on ground-truth annotated data and by comparison to previous approaches, including those of Berg and Forsyth and Fergus et al.},
author = {Schroff, Florian and Criminisi, Antonio and Zisserman, Andrew},
doi = {10.1109/TPAMI.2010.133},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schroff, Criminisi, Zisserman - 2011 - Harvesting image databases from the Web.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Databases, Factual,Image Enhancement,Image Enhancement: methods,Internet,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = {apr},
number = {4},
pages = {754--66},
pmid = {21330688},
title = {{Harvesting image databases from the Web.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21330688},
volume = {33},
year = {2011}
}
@article{Maji2008,
author = {Maji, Subhransu and Berkeley, U C and Berg, Alexander C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maji, Berkeley, Berg - 2008 - To Appear in IEEE Computer Vision and Pattern Recognition 2008 , Anchorage Classification using Intersecti.pdf:pdf},
title = {{To Appear in IEEE Computer Vision and Pattern Recognition 2008 , Anchorage Classification using Intersection Kernel Support Vector Machines is Efficient ∗}},
year = {2008}
}
@article{Bruna,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.3023v3},
author = {Bruna, Joan and Polytechnique, Ecole and Cedex, Palaiseau},
eprint = {arXiv:1011.3023v3},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bruna, Polytechnique, Cedex - Unknown - Classification with Scattering Operators arXiv 1011 . 3023v3 cs . CV 17 Nov 2010.pdf:pdf},
pages = {1--18},
title = {{Classification with Scattering Operators arXiv : 1011 . 3023v3 [ cs . CV ] 17 Nov 2010}}
}
@article{Gemert2008,
author = {Gemert, J C Van and Veenman, C J and Geusebroek, J M},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gemert, Veenman, Geusebroek - 2008 - Visual Word Ambiguity.pdf:pdf},
pages = {1--13},
title = {{Visual Word Ambiguity}},
year = {2008}
}
@article{Tuytelaars2007,
author = {Tuytelaars, Tinne and Mikolajczyk, Krystian},
doi = {10.1561/0600000017},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tuytelaars, Mikolajczyk - 2007 - Local Invariant Feature Detectors A Survey.pdf:pdf},
issn = {1572-2740},
journal = {Foundations and Trends{\textregistered} in Computer Graphics and Vision},
number = {3},
pages = {177--280},
title = {{Local Invariant Feature Detectors: A Survey}},
url = {http://www.nowpublishers.com/product.aspx?product=CGV{\&}doi=0600000017},
volume = {3},
year = {2007}
}
@article{Snoek2007,
author = {Snoek, Cees G. M. and Worring, Marcel},
doi = {10.1561/1500000014},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Snoek, Worring - 2007 - Concept-Based Video Retrieval.pdf:pdf},
issn = {1554-0669},
journal = {Foundations and Trends{\textregistered} in Information Retrieval},
number = {4},
pages = {215--322},
title = {{Concept-Based Video Retrieval}},
url = {http://www.nowpublishers.com/product.aspx?product=INR{\&}doi=1500000014},
volume = {2},
year = {2007}
}
@inproceedings{Dalal,
author = {Dalal, Navneet and Triggs, Bill},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dalal, Triggs, Europe - Unknown - Histograms of Oriented Gradients for Human Detection.pdf:pdf},
pages = {886--893},
title = {{Histograms of Oriented Gradients for Human Detection}},
url = {http://lear.inrialpes.fr/pubs/2005/DT05/},
year = {2005}
}
@article{Felzenszwalb2010,
abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI--SVM in terms of latent variables. A latent SVM is semiconvex, and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.},
author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David and Ramanan, Deva},
doi = {10.1109/TPAMI.2009.167},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Felzenszwalb et al. - 2010 - Object detection with discriminatively trained part-based models.pdf:pdf},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Discriminant Analysis,Image Enhancement,Image Enhancement: methods,Image Interpretation,Imaging,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Three-Dimensional,Three-Dimensional: methods},
month = {sep},
number = {9},
pages = {1627--45},
pmid = {20634557},
title = {{Object detection with discriminatively trained part-based models}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20634557},
volume = {32},
year = {2010}
}
@article{Araujo,
author = {Araujo, A F De and Weinzaepfel, P and Perez, P and Diot, C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Araujo et al. - Unknown - TECHNICOLOR TECHNICAL REPORT LIMITATIONS OF ‘ BAG-OF-WORDS ' -BASED IMAGE CLASSIFICATION Ecole Normale Superio.pdf:pdf},
pages = {1--5},
title = {{TECHNICOLOR TECHNICAL REPORT LIMITATIONS OF ‘ BAG-OF-WORDS ' -BASED IMAGE CLASSIFICATION Ecole Normale Superiore Cachan-Bretagne}}
}
@article{Araujoa,
author = {Araujo, A F De and Weinzaepfel, P and Perez, P and Diot, C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Araujo et al. - Unknown - ‘ OBJECT BANK ' -BASED SCENE CLASSIFICATION Ecole Normale Superieure Cachan-Bretagne.pdf:pdf},
title = {{‘ OBJECT BANK ' -BASED SCENE CLASSIFICATION Ecole Normale Superieure Cachan-Bretagne}}
}
@article{Zhou2008,
address = {New York, New York, USA},
author = {Zhou, Xi and Zhuang, Xiaodan and Yan, Shuicheng and Chang, Shih-Fu and Hasegawa-Johnson, Mark and Huang, Thomas S.},
doi = {10.1145/1459359.1459391},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2008 - SIFT-Bag kernel for video event analysis.pdf:pdf},
isbn = {9781605583037},
journal = {Proceeding of the 16th ACM international conference on Multimedia - MM '08},
pages = {229},
publisher = {ACM Press},
title = {{SIFT-Bag kernel for video event analysis}},
url = {http://portal.acm.org/citation.cfm?doid=1459359.1459391},
year = {2008}
}
@article{Dai2007,
author = {Dai, Guang and Yeung, Dit-yan and Bay, Water and Kong, Hong},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dai et al. - 2007 - Kernel Selection for Semi-Supervised Kernel Machines.pdf:pdf},
title = {{Kernel Selection for Semi-Supervised Kernel Machines}},
year = {2007}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - rabiner.pdf.pdf:pdf},
title = {rabiner.pdf}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - The Self-Organizing Map (Kohonen).pdf.pdf:pdf},
title = {{The Self-Organizing Map (Kohonen).pdf}}
}
@article{Viola2001,
author = {Viola, Paul},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola - 2001 - Rapid Object Detection using a Boosted Cascade of Simple Features.pdf:pdf},
isbn = {0769512720},
number = {C},
title = {{Rapid Object Detection using a Boosted Cascade of Simple Features}},
volume = {00},
year = {2001}
}
@article{Lindeberg1998,
author = {Lindeberg, Tony},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lindeberg - 1998 - Feature Detection with Automatic Scale Selection.pdf:pdf},
keywords = {blob detection,computer vision,corner detection,feature detection,fre-,gaussian derivative,multi-scale representation,normalized derivative,quency estimation,scale,scale selection,scale-space},
number = {2},
pages = {79--116},
title = {{Feature Detection with Automatic Scale Selection}},
volume = {30},
year = {1998}
}
@article{Bilen2011,
annote = {Best paper, BMVC 2011},
author = {Bilen, Hakan and Namboodiri, Vinay and Gool, Luc Van},
doi = {10.5244/C.25.17},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bilen, Namboodiri, Gool - 2011 - Object and Action Classification with Latent Variables.pdf:pdf},
isbn = {1-901725-43-X},
journal = {Procedings of the British Machine Vision Conference 2011},
pages = {17.1--17.11},
publisher = {British Machine Vision Association},
title = {{Object and Action Classification with Latent Variables}},
url = {http://www.bmva.org/bmvc/2011/proceedings/paper17/index.html},
year = {2011}
}
@article{Yang,
author = {Yang, Liu and Jurie, Frederic},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang, Jurie - Unknown - Unifying Discriminative Visual Codebook Generation with Classifier Training for Object Category Recognition.pdf:pdf},
journal = {Framework},
title = {{Unifying Discriminative Visual Codebook Generation with Classifier Training for Object Category Recognition}},
volume = {1}
}
@article{Engineering,
author = {Engineering, Electrical and Dong, Guseong and Gu, Yuseong},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Engineering, Dong, Gu - Unknown - LEARNING A DISCRIMINATIVE VISUAL CODEBOOK USING HOMONYM SCHEME SeungRyul Baek , Chang D . Yoo and Sung.pdf:pdf},
journal = {Learning},
pages = {11--14},
title = {{LEARNING A DISCRIMINATIVE VISUAL CODEBOOK USING HOMONYM SCHEME SeungRyul Baek , Chang D . Yoo and Sungrack Yun}}
}
@article{Zhang2009,
address = {New York, New York, USA},
author = {Zhang, Wei and Surve, Akshat and Fern, Xiaoli and Dietterich, Thomas},
doi = {10.1145/1553374.1553533},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang et al. - 2009 - Learning non-redundant codebooks for classifying complex objects.pdf:pdf},
isbn = {9781605585161},
journal = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
pages = {1--8},
publisher = {ACM Press},
title = {{Learning non-redundant codebooks for classifying complex objects}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553533},
year = {2009}
}
@article{Burges1997,
author = {Burges, Christopher J C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burges - 1997 - A Tutorial on Support Vector Machines for Pattern Recognition.pdf:pdf},
journal = {Data Mining and Knowledge Discovery},
keywords = {121-167,1998,appeared in,data mining and knowledge,discovery 2,pattern recognition,statistical learning theory,support vector machines,vc dimension},
pages = {1--43},
title = {{A Tutorial on Support Vector Machines for Pattern Recognition}},
volume = {43},
year = {1997}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - TRECVID 2012 Guidelines.html:html},
title = {{TRECVID 2012 Guidelines}},
url = {http://www-nlpir.nist.gov/projects/tv2012/tv2012.html{\#}data},
urldate = {2012-04-13}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - trec video retrieval evaluation papers and slides.html:html},
title = {trec video retrieval evaluation papers and slides},
url = {http://www-nlpir.nist.gov/projects/tvpubs/tv.pubs.org.html},
urldate = {2012-04-13}
}
@article{Inoue2011,
author = {Inoue, Nakamasa},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Inoue - 2011 - TokyoTech Canon at TRECVID 2011.pdf:pdf},
journal = {Event (London)},
title = {{TokyoTech + Canon at TRECVID 2011}},
year = {2011}
}
@article{Li2011,
author = {Li, X and Mazloom, M and Koelma, D C},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li, Mazloom, Koelma - 2011 - The MediaMill TRECVID 2011 Semantic Video Search Engine.pdf:pdf},
journal = {Foundations},
title = {{The MediaMill TRECVID 2011 Semantic Video Search Engine}},
year = {2011}
}
@article{Essid2011,
author = {Essid, Slim},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Essid - 2011 - Nonnegative matrix factorization for unsupervised audiovisual document structuring.pdf:pdf},
journal = {Audio},
keywords = {bag of features,content structuring,indexing,machine learning,matrices,unsupervised classification,videos},
title = {{Nonnegative matrix factorization for unsupervised audiovisual document structuring}},
year = {2011}
}
@article{Apostoloff,
author = {Apostoloff, Nicholas and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Apostoloff, Zisserman - Unknown - Who are you – real-time person identification.pdf:pdf},
title = {{Who are you ? – real-time person identification}}
}
@article{Brown2011,
abstract = {In this paper, we explore methods for learning local image descriptors from training data. We describe a set of building blocks for constructing descriptors which can be combined together and jointly optimized so as to minimize the error of a nearest-neighbor classifier. We consider both linear and nonlinear transforms with dimensionality reduction, and make use of discriminant learning techniques such as Linear Discriminant Analysis (LDA) and Powell minimization to solve for the parameters. Using these techniques, we obtain descriptors that exceed state-of-the-art performance with low dimensionality. In addition to new experiments and recommendations for descriptor learning, we are also making available a new and realistic ground truth data set based on multiview stereo data.},
author = {Brown, Matthew and Hua, Gang and Winder, Simon},
doi = {10.1109/TPAMI.2010.54},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brown, Hua, Winder - 2011 - Discriminative learning of local image descriptors.pdf:pdf},
issn = {1939-3539},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Discriminant Analysis,Humans,Image Processing,Linear Models,Nonlinear Dynamics},
month = {jan},
number = {1},
pages = {43--57},
pmid = {21088318},
title = {{Discriminative learning of local image descriptors.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21088318},
volume = {33},
year = {2011}
}
@article{Mikul,
author = {Mikul, Andrej and Perdoch, Michal},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikul, Perdoch - Unknown - Learning a Fine Vocabulary.pdf:pdf},
journal = {Most},
pages = {1--14},
title = {{Learning a Fine Vocabulary}}
}
@article{Planck2006,
author = {Planck, Max and Luxburg, Ulrike Von},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Planck, Luxburg - 2006 - A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering.pdf:pdf},
keywords = {patrick,thin sparse discrete reading group},
mendeley-tags = {patrick,thin sparse discrete reading group},
number = {August},
title = {{A Tutorial on Spectral Clustering A Tutorial on Spectral Clustering}},
year = {2006}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 1996 - Dimensionality reduction.pdf:pdf},
keywords = {patrick,thin sparse discrete reading group},
mendeley-tags = {patrick,thin sparse discrete reading group},
title = {{Dimensionality reduction}},
year = {1996}
}
@article{Song,
author = {Song, Xiaohu and Muselet, Damien and Tr, Alain},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Song, Muselet, Tr - Unknown - Affine Transforms between Image Space And Color Space For Invariant Local Descriptors.pdf:pdf},
journal = {Analysis},
keywords = {lionel},
mendeley-tags = {lionel},
number = {X},
pages = {1--18},
title = {{Affine Transforms between Image Space And Color Space For Invariant Local Descriptors}},
volume = {X}
}
@inproceedings{Nister,
author = {Nist{\'{e}}r, David and Stew{\'{e}}nius, Henrik},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nist, Stew - Unknown - Scalable Recognition with a Vocabulary Tree.pdf:pdf},
pages = {2161--2168},
title = {{Scalable Recognition with a Vocabulary Tree}},
year = {2006}
}
@article{Elkan,
author = {Elkan, Charles},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elkan - Unknown - Using the Triangle Inequality to Accelerate -Means.pdf:pdf},
journal = {Machine Learning},
title = {{Using the Triangle Inequality to Accelerate -Means}}
}
@article{Ferrari,
author = {Ferrari, Vittorio and Tuytelaars, Tinne and Gool, Luc Van},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferrari, Tuytelaars, Gool - Unknown - Simultaneous Object Recognition and Segmentation by Image Exploration ⋆.pdf:pdf},
journal = {Compute},
title = {{Simultaneous Object Recognition and Segmentation by Image Exploration ⋆}}
}
@article{Comaniciu2002,
author = {Comaniciu, Dorin and Meer, Peter and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Comaniciu, Meer, Member - 2002 - Mean Shift A Robust Approach Toward Feature Space Analysis.pdf:pdf},
journal = {Analysis},
number = {5},
pages = {603--619},
title = {{Mean Shift : A Robust Approach Toward Feature Space Analysis}},
volume = {24},
year = {2002}
}
@article{Tola2010,
author = {Tola, Engin and Lepetit, Vincent and Fua, Pascal and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tola et al. - 2010 - DAISY An Efficient Dense Descriptor Applied to Wide-Baseline Stereo.pdf:pdf},
journal = {Analysis},
number = {5},
pages = {815--830},
title = {{DAISY : An Efficient Dense Descriptor Applied to Wide-Baseline Stereo}},
volume = {32},
year = {2010}
}
@inproceedings{Lin2010,
abstract = {The so-called bag-of-features (BoF) representation for im- ages is by now well-established in the context of large scale image and video retrieval. The BoF framework typically ranks database image ac- cording to a metric on the global histograms of the query and database images, respectively. Ranking based on global histograms has the advan- tage of being scalable with respect to the number of database images, but at the cost of reduced retrieval precision when the object of interest is small. Additionally, computationally intensive post-processing (such as RANSAC) is typically required to locate the object of interest in the retrieved images. To address these shortcomings, we propose a general- ization of the global BoF framework to support scalable local matching. Specifically, we propose an efficient and accurate algorithm to accom- plish local histogram matching and object localization simultaneously. The generalization is to represent each database image as a family of histograms that depend functionally on a bounding rectangle. Integral with the image retrieval process, we identify bounding rectangles whose histograms optimize query relevance, and rank the images accordingly. Through this localization scheme, we impose a weak spatial consistency constraint with low computational overhead. We validate our approach on two public image retrieval benchmarks: the University of Kentucky data set and the Oxford Building data set. Experiments show that our approach significantly improves on BoF-based retrieval, without requir- ing computationally expensive post-processing.},
author = {Lin, Zhe and Brandt, Jonathan},
booktitle = {Computer Vision ECCV 2010},
editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
isbn = {9783642155666},
pages = {294--308},
publisher = {Springer},
series = {Lecture Notes in Computer Science},
title = {{A Local Bag-of-Features Model for Large-Scale Object Retrieval}},
url = {http://www.springerlink.com/index/WP6Q7742N2G512R1.pdf},
volume = {6316},
year = {2010}
}
@inproceedings{MichalPerd'och2009,
abstract = {State of the art methods for image and object retrieval exploit both appearance (via visual words) and local geometry (spatial extent, relative pose). In large scale problems, memory becomes a limiting factor - local geometry is stored for each feature detected in each image and requires storage larger than the inverted file and term frequency and inverted document frequency weights together. We propose a novel method for learning discretized local geometry representation based on minimization of average reprojection error in the space of ellipses. The representation requires only 24 bits per feature without drop in performance. Additionally, we show that if the gravity vector assumption is used consistently from the feature description to spatial verification, it improves retrieval performance and decreases the memory footprint. The proposed method outperforms state of the art retrieval algorithms in a standard image retrieval benchmark.},
author = {{Michal Perd'och}, Ondrej Chum and Jiri Matas},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michal Perd'och - 2009 - Efficient Representation of Local Geometry for Large Scale Object Retrieval.pdf:pdf},
pages = {9--16},
title = {{Efficient Representation of Local Geometry for Large Scale Object Retrieval}},
year = {2009}
}
@article{Cao2010,
author = {Cao, Yang},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao - 2010 - Spatial-Bag-of-Features.pdf:pdf},
isbn = {9781424469833},
journal = {Image (Rochester, N.Y.)},
title = {{Spatial-Bag-of-Features}},
year = {2010}
}
@inproceedings{Sivic2003,
author = {Sivic, Josef and Zisserman, Andrew},
booktitle = {International Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sivic, Zisserman - 2003 - Video Google A text retrieval approach to object matching in videos.pdf:pdf},
isbn = {0769519504},
title = {{Video Google: A text retrieval approach to object matching in videos}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1238663},
year = {2003}
}
@article{Tirilly,
author = {Tirilly, Pierre and Claveau, Vincent and Gros, Patrick},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tirilly, Claveau, Gros - Unknown - Language Modeling for Bag-of-Visual Words Image Categorization.pdf:pdf},
journal = {Image Processing},
title = {{Language Modeling for Bag-of-Visual Words Image Categorization}}
}
@article{Tommasi1920,
author = {Tommasi, Tatiana},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tommasi - 1920 - Multiclass Transfer Learning from Unconstrained Priors.pdf:pdf},
journal = {Learning},
title = {{Multiclass Transfer Learning from Unconstrained Priors}},
year = {1920}
}
@article{Wanga,
author = {Wang, Xiaoyu and Yang, Ming and Cour, Timothee and Zhu, Shenghuo and Yu, Kai and Han, Tony X},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - Unknown - Contextual Weighting for Vocabulary Tree based Image Retrieval.pdf:pdf},
journal = {Image (Rochester, N.Y.)},
title = {{Contextual Weighting for Vocabulary Tree based Image Retrieval}}
}
@article{Lazebnik,
author = {Lazebnik, Svetlana and Raginsky, Maxim},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazebnik, Raginsky - Unknown - Supervised Learning of Quantizer Codebooks by Information Loss Minimization.pdf:pdf},
journal = {North},
keywords = {Alexey},
mendeley-tags = {Alexey},
pages = {1--15},
title = {{Supervised Learning of Quantizer Codebooks by Information Loss Minimization}}
}
@inproceedings{Lazebnika,
author = {Lazebnik, Svetlana and Schmid, Cordelia},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lazebnik, Schmid - Unknown - Beyond Bags of Features Spatial Pyramid Matching for Recognizing Natural Scene Categories.pdf:pdf},
keywords = {Alexey},
mendeley-tags = {Alexey},
title = {{Beyond Bags of Features : Spatial Pyramid Matching for Recognizing Natural Scene Categories}},
year = {2006}
}
@article{Hara2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1101.3354v1},
author = {Hara, Stephen O and Draper, Bruce A},
eprint = {arXiv:1101.3354v1},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hara, Draper - 2010 - Introduction to the bag of features paradigm for image classification and retrieval.pdf:pdf},
journal = {Image (Rochester, N.Y.)},
keywords = {Alexey},
mendeley-tags = {Alexey},
number = {July},
pages = {1--25},
title = {{Introduction to the bag of features paradigm for image classification and retrieval}},
year = {2010}
}
@article{Wu2010,
abstract = {CENTRIST (CENsus TRansform hISTogram), a new visual descriptor for recognizing topological places or scene categories, is introduced in this paper. We show that place and scene recognition, especially for indoor environments, require its visual descriptor to possess properties that are different from other vision domains (e.g. object recognition). CENTRIST satisfies these properties and suits the place and scene recognition task. It is a holistic representation and has strong generalizability for category recognition. CENTRIST mainly encodes the structural properties within an image and suppresses detailed textural information. Our experiments demonstrate that CENTRIST outperforms the current state-of-the-art in several place and scene recognition datasets, compared with other descriptors such as SIFT and Gist. Besides, it is easy to implement and evaluates extremely fast.},
author = {Wu, Jianxin and Rehg, Jim M},
doi = {10.1109/TPAMI.2010.224},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu, Rehg - 2010 - CENTRIST A Visual Descriptor for Scene Categorization.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
month = {dec},
pages = {1--14},
pmid = {21173449},
title = {{CENTRIST: A Visual Descriptor for Scene Categorization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21173449},
year = {2010}
}
@inproceedings{Philbina,
author = {Philbin, James and Chum, Ondrej and Isard, Michael and Sivic, Josef and Zisserman, Andrew},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Philbin et al. - Unknown - Object retrieval with large vocabularies and fast spatial matching.pdf:pdf},
title = {{Object retrieval with large vocabularies and fast spatial matching}},
year = {2007}
}
@inproceedings{Jegou2008,
author = {{Herv{\'{e}} J{\'{e}}gou} and Douze, Matthijs and Schmid, Cordelia},
booktitle = {European Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herv{\'{e}} J{\'{e}}gou, Douze, Schmid - 2008 - Hamming Embedding and Weak Geometry Consistency for Large Scale Image Search.pdf:pdf},
title = {{Hamming Embedding and Weak Geometry Consistency for Large Scale Image Search}},
url = {https://lear.inrialpes.fr/{~}jegou/data.php{\#}holidays},
year = {2008}
}
@article{Saul,
author = {Saul, Lawrence K and Ave, Park and Park, Florham and Roweis, Sam T},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Saul et al. - Unknown - An Introduction to Locally Linear Embedding.pdf:pdf},
title = {{An Introduction to Locally Linear Embedding}}
}
@misc{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - manifold{\_}ways.pdf.pdf:pdf},
title = {manifold{\_}ways.pdf}
}
@article{Roweis2000,
author = {Roweis, Sam T and Saul, Lawrence K},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Roweis, Saul - 2000 - Nonlinear Dimensionality Reduction by Locally Linear Embedding.pdf:pdf},
journal = {Science},
number = {December},
pages = {2323--2326},
title = {{Nonlinear Dimensionality Reduction by Locally Linear Embedding}},
volume = {290},
year = {2000}
}
@article{Ferecatu2009,
author = {Ferecatu, Marin and Geman, Donald and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferecatu, Geman, Member - 2009 - A Statistical Framework for Image Category Search from a Mental Picture.pdf:pdf},
journal = {Analysis},
keywords = {been explored as well,for large databases,from the database until,more direct solutions have,patrick,point,sampled pages,some simply display randomly,such as,suitable starting,the user identifies a,this rapidly becomes impractical},
mendeley-tags = {patrick},
number = {6},
pages = {1087--1101},
title = {{A Statistical Framework for Image Category Search from a Mental Picture}},
volume = {31},
year = {2009}
}
@article{Kirubarajan2004,
author = {Kirubarajan, Thiagalingam and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kirubarajan, Member - 2004 - Probabilistic Data Association Techniques for Target Tracking in Clutter.pdf:pdf},
journal = {Update},
keywords = {association,data,estimation,kalman filter,kf,probabilistic data association filter,target tracking},
number = {3},
title = {{Probabilistic Data Association Techniques for Target Tracking in Clutter}},
volume = {92},
year = {2004}
}
@article{Takacs,
author = {Takacs, Gabriel and Chandrasekhar, Vijay and Tsai, Sam and Chen, David and Grzeszczuk, Radek and Girod, Bernd},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Takacs et al. - Unknown - Unified Real-Time Tracking and Recognition with Rotation-Invariant Fast Features.pdf:pdf},
journal = {Work},
keywords = {patrick},
mendeley-tags = {patrick},
title = {{Unified Real-Time Tracking and Recognition with Rotation-Invariant Fast Features}}
}
@article{Heath,
author = {Heath, Kyle and Gelfand, Natasha and Ovsjanikov, Maks and Aanjaneya, Mridul and Guibas, Leonidas J and Alto, Palo},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heath et al. - Unknown - Image Webs Computing and Exploiting Connectivity in Image Collections.pdf:pdf},
journal = {Construction},
title = {{Image Webs : Computing and Exploiting Connectivity in Image Collections}}
}
@article{Maneewongvatana1999,
author = {Maneewongvatana, Songrit and Mount, David M},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Maneewongvatana, Mount - 1999 - It ' s okay to be skinny , if your friends are fat ∗.pdf:pdf},
journal = {ReCALL},
number = {October},
pages = {1--8},
title = {{It ' s okay to be skinny , if your friends are fat ∗}},
year = {1999}
}
@article{Philbinb,
author = {Philbin, James},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Philbin - Unknown - Near Duplicate Image Detection min-Hash and tf-idf Weighting.pdf:pdf},
title = {{Near Duplicate Image Detection : min-Hash and tf-idf Weighting}}
}
@article{Langtangen2007,
author = {Langtangen, Hans Petter},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Langtangen - 2007 - A Case Study in High-Performance Mixed-Language Programming.pdf:pdf},
pages = {36--49},
title = {{A Case Study in High-Performance Mixed-Language Programming}},
year = {2007}
}
@article{Zhang2010,
author = {Zhang, Yimeng and Chen, Tsuhan},
doi = {10.5244/C.24.47},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Chen - 2010 - Weakly Supervised Object Recognition and Localization with Invariant High Order Features.pdf:pdf},
journal = {Order A Journal On The Theory Of Ordered Sets And Its Applications},
pages = {1--11},
title = {{Weakly Supervised Object Recognition and Localization with Invariant High Order Features}},
year = {2010}
}
@incollection{Moore1991,
author = {Moore, Andrew W},
booktitle = {An intoductory tutorial on KD-trees},
chapter = {6},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Moore - 1991 - An intoductory tutorial on kd-trees Chapter 6 6 . 1 Nearest Neighbour Speci cation.pdf:pdf},
title = {{Efficient Memory Based Learning for Robot Control}},
year = {1991}
}
@article{Zhanga,
author = {Zhang, Yimeng},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - Unknown - Image Retrieval with Geometry-Preserving Visual Phrases.pdf:pdf},
journal = {I Can},
title = {{Image Retrieval with Geometry-Preserving Visual Phrases}}
}
@article{Shotton,
author = {Shotton, Jamie and Sharp, Toby},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shotton, Sharp - Unknown - Real-Time Human Pose Recognition in Parts from Single Depth Images.pdf:pdf},
title = {{Real-Time Human Pose Recognition in Parts from Single Depth Images}}
}
@article{Leea,
author = {Lee, Daniel D and Laboratories, Bell and Hill, Murray and {\'{Y}}, H Sebastian Seung},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - Unknown - Algorithms for Non-negative Matrix Factorization.pdf:pdf},
journal = {Language},
number = {1},
title = {{Algorithms for Non-negative Matrix Factorization}}
}
@inproceedings{Mairal2008,
author = {Mairal, Julien and Bach, Francis and Zisserman, Andrew and Sapiro, Guillermo},
booktitle = {Neural Information Processing Systems},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal et al. - 2008 - Supervised Dictionary Learning.pdf:pdf},
title = {{Supervised Dictionary Learning}},
year = {2008}
}
@article{Olshausen1998,
author = {Olshausen, Bruno A and Field, David J and Davis, U C and Ct, Newton},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Olshausen et al. - 1998 - Sparse coding with an overcomplete basis set A strategy employed by V1.pdf:pdf},
journal = {Neuroscience},
pages = {1--32},
title = {{Sparse coding with an overcomplete basis set : A strategy employed by V1 ?}},
year = {1998}
}
@article{Halko,
archivePrefix = {arXiv},
arxivId = {arXiv:0909.4061v2},
author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
eprint = {arXiv:0909.4061v2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Halko, Martinsson, Tropp - Unknown - Finding structure with randomness probabilistic algorithms for constructing approximate matrix deco.pdf:pdf},
journal = {Computing},
keywords = {dimension reduction,eigenvalue decomposition,interpolative decomposition,johnson,lindenstrauss lemma,matrix approximation,parallel algorithm,pass-efficient algorithm,principal component analysis,random matrix,randomized algorithm,rank-revealing qr factoriza-,singular value decomposition,streaming algorithm,tion},
pages = {1--74},
title = {{Finding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions}}
}
@article{,
archivePrefix = {arXiv},
arxivId = {arXiv:1009.5358v1},
eprint = {arXiv:1009.5358v1},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2010 - Task-Driven Dictionary Learning.pdf:pdf},
journal = {Bach},
number = {September},
title = {{Task-Driven Dictionary Learning}},
year = {2010}
}
@article{Blumensath2009,
author = {Blumensath, Thomas and Davies, Mike E},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blumensath, Davies - 2009 - Stagewise Weak Gradient Pursuits.pdf:pdf},
journal = {Update},
pages = {1--14},
title = {{Stagewise Weak Gradient Pursuits}},
year = {2009}
}
@article{Sezer,
author = {Sezer, Osman G and Harmanci, Oztan and Guleryuz, Onur G},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sezer, Harmanci, Guleryuz - Unknown - SPARSE ORTHONORMAL TRANSFORMS FOR IMAGE COMPRESSION Center for Signal and Image Processing , Georg.pdf:pdf},
journal = {Simulation},
number = {1},
pages = {1--4},
title = {{SPARSE ORTHONORMAL TRANSFORMS FOR IMAGE COMPRESSION Center for Signal and Image Processing , Georgia Institute of Technology , Atlanta , GA 30308 , USA DoCoMo Comm . Laboratories USA Inc ., Palo Alto , CA 94304 , USA}}
}
@article{Rubinstein2010,
author = {Rubinstein, Ron and Member, Student and Zibulevsky, Michael and Elad, Michael and Member, Senior},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubinstein et al. - 2010 - Double Sparsity Learning Sparse Dictionaries for Sparse Signal Approximation.pdf:pdf},
journal = {IEEE Transactions on Signal Processing},
number = {3},
pages = {1553--1564},
title = {{Double Sparsity : Learning Sparse Dictionaries for Sparse Signal Approximation}},
volume = {58},
year = {2010}
}
@article{Skretting2010,
author = {Skretting, Karl and Engan, Kjersti},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skretting, Engan - 2010 - Recursive Least Squares Dictionary Learning Algorithm.pdf:pdf},
journal = {Learning},
number = {4},
pages = {2121--2130},
title = {{Recursive Least Squares Dictionary Learning Algorithm}},
volume = {58},
year = {2010}
}
@article{Skretting2010a,
author = {Skretting, Karl and Engan, Kjersti},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Skretting, Engan - 2010 - Recursive Least Squares Dictionary Learning Algorithm(2).pdf:pdf},
title = {{Recursive Least Squares Dictionary Learning Algorithm}},
year = {2010}
}
@article{Jenatton2010,
author = {Jenatton, Rodolphe and Fr, Inria and Bach, Francis},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jenatton, Fr, Bach - 2010 - Proximal Methods for Sparse Hierarchical Dictionary Learning.pdf:pdf},
journal = {Bach},
keywords = {Christopher},
mendeley-tags = {Christopher},
title = {{Proximal Methods for Sparse Hierarchical Dictionary Learning}},
year = {2010}
}
@article{Gharavi-alkhansari1998,
author = {Gharavi-alkhansari, Mohammad and Huang, Thomas S},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gharavi-alkhansari, Huang - 1998 - A fast orthogonal matching.pdf:pdf},
journal = {International Journal},
pages = {1389--1392},
title = {{A fast orthogonal matching}},
year = {1998}
}
@article{Bach,
author = {Bach, Francis and Project-team, Inria Willow},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bach, Project-team - Unknown - Convex Optimization with Sparsity-Inducing Norms.pdf:pdf},
keywords = {Christopher},
mendeley-tags = {Christopher},
pages = {1--35},
title = {{Convex Optimization with Sparsity-Inducing Norms}}
}
@article{Hoyer2002,
author = {Hoyer, Patrik O},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoyer - 2002 - NON-NEGATIVE SPARSE CODING.pdf:pdf},
title = {{NON-NEGATIVE SPARSE CODING}},
year = {2002}
}
@article{Zhou2011,
author = {Zhou, Mingyaun and Yang, Hongxia and Sapiro, Guillermo and Dunson, David and Carin, Lawrence},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhou et al. - 2011 - Dependent Hierarchical Beta Process for Image Interpolation and Denoising.pdf:pdf},
title = {{Dependent Hierarchical Beta Process for Image Interpolation and Denoising}},
volume = {15},
year = {2011}
}
@article{Rubinstein,
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rubinstein, Zibulevsky, Elad - Unknown - Technion - Computer Science Department - Technical Report CS-2008-08 . revised - 2008 Efficient.pdf:pdf},
pages = {1--15},
title = {{Technion - Computer Science Department - Technical Report CS-2008-08 . revised - 2008 Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technion - Computer Science Department - Technical Report CS-2008-08 . revised - }}
}
@article{Blumensath,
author = {Blumensath, Thomas and Davies, Mike E},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blumensath, Davies - Unknown - Gradient Pursuits Gradient Pursuits.pdf:pdf},
title = {{Gradient Pursuits Gradient Pursuits}}
}
@article{Leeb,
author = {Lee, Honglak and Ng, Andrew Y},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Ng - Unknown - Efficient sparse coding algorithms(2).pdf:pdf},
journal = {Science},
title = {{Efficient sparse coding algorithms}}
}
@inproceedings{Mairal2008a,
author = {Mairal, Julien and Bach, Francis and Sapiro, Guillermo and Zisserman, Andrew},
booktitle = {Computer Vision and Pattern Recognition},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal et al. - 2008 - Discriminative Learned Dictionaries for Local Image Analysis.pdf:pdf},
title = {{Discriminative Learned Dictionaries for Local Image Analysis}},
year = {2008}
}
@article{Zhao2009,
archivePrefix = {arXiv},
arxivId = {arXiv:0909.0411v1},
author = {Zhao, Peng and Rocha, Guilherme and Yu, Bin},
doi = {10.1214/07-AOS584},
eprint = {arXiv:0909.0411v1},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhao, Rocha, Yu - 2009 - The composite absolute penalties family for grouped and hierarchical variable selection.pdf:pdf},
journal = {Annals of Statistics},
keywords = {Linear regression, penalized regression, variable},
number = {6},
pages = {3468--3497},
title = {{The composite absolute penalties family for grouped and hierarchical variable selection}},
volume = {37},
year = {2009}
}
@article{Aharon2006a,
author = {Aharon, Michal and Elad, Michael and Bruckstein, Alfred M},
doi = {10.1016/j.laa.2005.06.035},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Aharon, Elad, Bruckstein - 2006 - On the uniqueness of overcomplete dictionaries , and a practical way to retrieve them.pdf:pdf},
journal = {Linear Algebra and its Applications},
keywords = {atom,basis pursuit,codebook,decomposition,dictionary,k-means,k-svd,matching pursuit,sparse representation,training,uniqueness,vector quantization},
pages = {48--67},
title = {{On the uniqueness of overcomplete dictionaries , and a practical way to retrieve them}},
volume = {416},
year = {2006}
}
@article{Lei2007,
author = {Lei, Jing and Meinshausen, Nicolai and Purdy, David and Vu, Vince},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei et al. - 2007 - THE COMPOSITE ABSOLUTE PENALTIES FAMILY FOR By Peng Zhao and Guilherme Rocha and Bin Yu † University of California a.pdf:pdf},
journal = {October},
keywords = {and phrases,coef-,linear regression,penalized regression,variable selection},
title = {{THE COMPOSITE ABSOLUTE PENALTIES FAMILY FOR By Peng Zhao and Guilherme Rocha and Bin Yu † University of California at Berkeley 1 . Introduction . Information technology advances are bringing the possibility of new and exciting discoveries in various scien}},
year = {2007}
}
@article{Jenatton2011,
archivePrefix = {arXiv},
arxivId = {arXiv:1009.2139v3},
author = {Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and Bach, Francis},
eprint = {arXiv:1009.2139v3},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jenatton et al. - 2011 - Proximal Methods for Hierarchical Sparse Coding.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Christopher,convex optimization,dictionary learning,matrix factorization,proximal methods,sparse coding,struc-,tured sparsity},
mendeley-tags = {Christopher},
pages = {2297--2334},
title = {{Proximal Methods for Hierarchical Sparse Coding}},
volume = {12},
year = {2011}
}
@article{Kolmogorov2004,
author = {Kolmogorov, Vladimir and Zabih, Ramin},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kolmogorov, Zabih - 2004 - What Energy Functions Can Be Minimized via Graph Cuts.pdf:pdf},
journal = {Analysis},
number = {2},
pages = {147--159},
title = {{What Energy Functions Can Be Minimized via Graph Cuts ?}},
volume = {26},
year = {2004}
}
@article{Hø,
author = {H{\o}, {\`{I}} {\AA} and {\~{A}}, {\^{A}} {\^{A}} and {\"{E}}, {\`{A}} {\AA} {\^{E}} {\^{A}} {\"{E}} {\~{A}} and {\"{U}}, {\^{A}} and {\^{E}}, {\AA} and V{\^{e}}, {\~{N}} {\c{C}} {\`{A}} and B, V {\^{A}} and H{\`{i}}, {\c{C}} {\`{A}} {\^{E}} and {\"{E}}, {\~{A}} and {\c{C}}, {\^{A}} {\~{N}} and {\`{A}}, {\'{I}} and {\^{O}}, {\`{A}} V {\~{A}} {\^{A}} {\^{E}} and T, {\^{E}} {\c{C}} {\`{E}}},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/H{\o} et al. - Unknown - No Title.pdf:pdf},
title = {{No Title}}
}
@article{Jaakkolaa,
author = {Jaakkola, Tommi S and Haussler, David},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jaakkola, Haussler - Unknown - Exploiting generative models in discriminative classi ers.pdf:pdf},
title = {{Exploiting generative models in discriminative classi ers}}
}
@article{Wu2007,
author = {Wu, Xindong and Kumar, Vipin and {Ross Quinlan}, J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi-Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
doi = {10.1007/s10115-007-0114-2},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2007 - Top 10 algorithms in data mining.pdf:pdf},
issn = {0219-1377},
journal = {Knowledge and Information Systems},
keywords = {currently reading,herve},
mendeley-tags = {currently reading,herve},
month = {dec},
number = {1},
pages = {1--37},
title = {{Top 10 algorithms in data mining}},
url = {http://www.springerlink.com/index/10.1007/s10115-007-0114-2},
volume = {14},
year = {2007}
}
@article{Smirnov1988,
author = {Smirnov, A V},
doi = {10.1007/BF00047288},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Smirnov - 1988 - An introduction to tensor calculus, relativity, and cosmology.pdf:pdf},
issn = {0167-8019},
journal = {Acta Applicandae Mathematicae},
month = {feb},
number = {2},
pages = {193--195},
title = {{An introduction to tensor calculus, relativity, and cosmology}},
url = {http://www.springerlink.com/index/10.1007/BF00047288},
volume = {11},
year = {1988}
}
@article{Elad2010,
author = {Elad, Michael and Figueiredo, Mario a T},
doi = {10.1109/JPROC.2009.2037655},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Elad, Figueiredo - 2010 - On the Role of Sparse and Redundant Representations in Image Processing.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
month = {jun},
number = {6},
pages = {972--982},
title = {{On the Role of Sparse and Redundant Representations in Image Processing}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5420029},
volume = {98},
year = {2010}
}
@inproceedings{,
booktitle = {ICCV},
keywords = {currently reading,must read,patrick},
mendeley-tags = {currently reading,must read,patrick},
title = {{2 Articles - ICCV 2011 From Patrick}}
}
@article{Donoho2010,
author = {Donoho, David L and Tanner, Jared},
doi = {10.1109/JPROC.2010.2045630},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho, Tanner - 2010 - Precise Undersampling Theorems.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {1 minimization,bandlimited measurements,compressed sensing,currently reading,must read,random measurements,random polytopes,superresolu-,tion,undersampling,universality of},
mendeley-tags = {currently reading,must read},
month = {jun},
number = {6},
pages = {913--924},
title = {{Precise Undersampling Theorems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5458001},
volume = {98},
year = {2010}
}
@article{Garrett2005,
author = {Garrett, Paul},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garrett - 2005 - Normed and Banach Spaces.pdf:pdf},
pages = {1--8},
title = {{Normed and Banach Spaces}},
year = {2005}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Ch.5 - Banach Spaces.pdf:pdf},
journal = {Convergence},
pages = {90--123},
title = {{Ch.5 - Banach Spaces}}
}
@article{Rauhut2006,
author = {Rauhut, Holger and Schnass, Karin and Vandergheynst, Pierre},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rauhut, Schnass, Vandergheynst - 2006 - Compressed Sensing and Redundant Dictionaries.pdf:pdf},
journal = {Signal Processing},
keywords = {basis pursuit,compressed sensing,matrix,orthogonal matching,random,redundant dictionary,restricted isometry constants,sparse approximation,thresholding},
pages = {1--19},
title = {{Compressed Sensing and Redundant Dictionaries}},
year = {2006}
}
@article{Donoho1998,
author = {Donoho, D L and Vetterli, M and Devore, R A and Daubechies, I},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho et al. - 1998 - Data Compression and Harmonic Analysis.pdf:pdf},
journal = {Transform},
keywords = {-entropy,and phrases,besov spaces,block,cess,coding,eve transform,fourier trans-,gaussian pro-,karhunen-lo,n-widths,non-gaussian process,rate-distortion,scalar quantization,second-order statistics,sobolev spaces,subband coding,transform coding},
pages = {1--62},
title = {{Data Compression and Harmonic Analysis}},
year = {1998}
}
@article{Donoho2006,
author = {Donoho, David L},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donoho - 2006 - Compressed Sensing.pdf:pdf},
journal = {IEEE Transactions on Information Theory},
keywords = {currently reading,must read},
mendeley-tags = {currently reading,must read},
number = {4},
pages = {1289--1306},
title = {{Compressed Sensing}},
volume = {52},
year = {2006}
}
@article{RuiCastroJarvisHaupt,
author = {{Rui Castro , Jarvis Haupt}, Robert Nowak},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rui Castro , Jarvis Haupt - Unknown - COMPRESSED SENSING VS . ACTIVE LEARNING.pdf:pdf},
journal = {Computer},
keywords = {anthony},
mendeley-tags = {anthony},
title = {{COMPRESSED SENSING VS . ACTIVE LEARNING}}
}
@article{Candes2004,
author = {Candes, Emmanuel and Tao, Terence},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Candes, Tao - 2004 - Decoding by Linear Programming.pdf:pdf},
keywords = {1 minimization,basis pursuit,decoding of,determined systems,duality in optimization,gaussian random matrices,gramming,linear codes,linear pro-,principal angles,random,restricted orthonormality,singular,sparse solutions to under-,values of random matrices},
pages = {1--22},
title = {{Decoding by Linear Programming}},
year = {2004}
}
@article{Wright2009a,
author = {Wright, John and Ma, Yi and Mairal, Julien and Sapiro, Guillermo and Huang, Thomas and Yan, Shuicheng},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wright et al. - 2009 - Sparse Representation For Computer Vision and Pattern Recognition.pdf:pdf},
number = {March},
pages = {1--10},
title = {{Sparse Representation For Computer Vision and Pattern Recognition}},
year = {2009}
}
@article{Ren,
author = {Ren, Ehsan Elhamifar},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ren - Unknown - Sparse Subspace Clustering.pdf:pdf},
journal = {Imaging},
keywords = {currently reading},
mendeley-tags = {currently reading},
title = {{Sparse Subspace Clustering}}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CLUSTERING DISJOINT SUBSPACES VIA SPARSE REPRESENTATION Ehsan Elhamifar Ren ´ e Vidal Center for Imaging Science , J.pdf:pdf},
journal = {Image (Rochester, N.Y.)},
keywords = {currently reading},
mendeley-tags = {currently reading},
pages = {1--4},
title = {{CLUSTERING DISJOINT SUBSPACES VIA SPARSE REPRESENTATION Ehsan Elhamifar Ren ´ e Vidal Center for Imaging Science , Johns Hopkins University , Baltimore MD 21218 , USA}}
}
@article{Bosch2007,
author = {Bosch, Anna and Zisserman, Andrew and Munoz, Xavier},
doi = {10.1109/ICCV.2007.4409066},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosch, Zisserman, Munoz - 2007 - Image Classification using Random Forests and Ferns.pdf:pdf},
isbn = {978-1-4244-1630-1},
journal = {2007 IEEE 11th International Conference on Computer Vision},
pages = {1--8},
publisher = {Ieee},
title = {{Image Classification using Random Forests and Ferns}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4409066},
year = {2007}
}
@article{Bosch,
author = {Bosch, Anna and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bosch, Zisserman - Unknown - Representing shape with a spatial pyramid kernel.pdf:pdf},
journal = {Image Processing},
keywords = {object and video,shape features,spatial pyramid kernel},
title = {{Representing shape with a spatial pyramid kernel}}
}
@article{Varma2007,
author = {Varma, Manik and Ray, Debajyoti},
doi = {10.1109/ICCV.2007.4408875},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Varma, Ray - 2007 - Learning The Discriminative Power-Invariance Trade-Off.pdf:pdf},
isbn = {978-1-4244-1630-1},
journal = {2007 IEEE 11th International Conference on Computer Vision},
pages = {1--8},
publisher = {Ieee},
title = {{Learning The Discriminative Power-Invariance Trade-Off}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4408875},
year = {2007}
}
@article{Weinberger,
author = {Weinberger, Kilian Q. and Saul, Lawrence K.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Weinberger, Blitzer, Saul - Unknown - Distance Metric Learning for Large Margin Nearest Neighbor Classification.pdf:pdf},
journal = {The Journal of Machine Learning Research},
pages = {207--244},
title = {{Distance Metric Learning for Large Margin Nearest Neighbor Classification}},
year = {2009}
}
@article{,
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - NONLINEAR OPTIMIZATION WITH CONVEX CONSTRAINTS The Goldstein-Levitin-Polyak algorithm.pdf:pdf},
keywords = {background},
mendeley-tags = {background},
title = {{NONLINEAR OPTIMIZATION WITH CONVEX CONSTRAINTS The Goldstein-Levitin-Polyak algorithm}}
}
@article{Vxfk2009,
author = {Vxfk, Phdvxuhv and Iuhtxhqf, D V and Ihdwxuh, R I and Ryhu, Rffxuuhqfh and Duh, Dssurdfkhv and Rq, Edvhg and Qhljkeru, Qhduhvw},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vxfk et al. - 2009 - xpdq {\$}fwlrq 5hfrjqlwlrq xvlqj 6sduvh 5hsuhvhqwdwlrq.pdf:pdf},
title = {+xpdq {\$}fwlrq 5hfrjqlwlrq xvlqj 6sduvh 5hsuhvhqwdwlrq},
year = {2009}
}
@article{Candes2008,
author = {Cand{\`{e}}s, E. and Wakin, M.},
journal = {IEEE Signal Processing Magazine},
keywords = {currently reading,must read},
mendeley-tags = {currently reading,must read},
number = {2},
pages = {21--30},
title = {{An introduction to compressive sampling}},
volume = {25},
year = {2008}
}
@article{Calderbank,
author = {Calderbank, Robert and Jafarpour, Sina and Schapire, Robert},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Calderbank, Jafarpour, Schapire - Unknown - Compressed Learning Universal Sparse Dimensionality Reduction and Learning in the Measureme.pdf:pdf},
keywords = {currently reading,must read},
mendeley-tags = {currently reading,must read},
title = {{Compressed Learning : Universal Sparse Dimensionality Reduction and Learning in the Measurement Domain}}
}
@article{Fritzke1995,
author = {Fritzke, Bernd},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fritzke - 1995 - A Growing Neural Gas Network Learns Topologies.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
keywords = {Franck},
mendeley-tags = {Franck},
title = {{A Growing Neural Gas Network Learns Topologies}},
volume = {7},
year = {1995}
}
@inproceedings{Wu2009,
author = {Wu, Zhong and Ke, Qifa and Isard, Michael and Sun, Jian},
booktitle = {CVPR},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wu et al. - 2009 - Bundling Features for Large Scale Partial-Duplicate Web Image Search.pdf:pdf},
keywords = {herve,must read},
mendeley-tags = {herve,must read},
title = {{Bundling Features for Large Scale Partial-Duplicate Web Image Search}},
year = {2009}
}
@article{Candes2006,
author = {Cand{\`{e}}s, Emamnuel J.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cand{\`{e}}s - 2006 - Compressive sampling.pdf:pdf},
journal = {Proceedings of the International Congress of Mathematicians},
keywords = {1 -minimization,compressive sampling,currently reading,error cor-,linear programming,must read,signal recovery,sparsity,systems of linear equations,underdertermined,uniform uncertainty principle},
mendeley-tags = {currently reading,must read},
title = {{Compressive sampling}},
year = {2006}
}
@article{Candes2008a,
author = {Cand{\`{e}}s, Emmanuel J. and Wakin, Michael B.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cand{\`{e}}s, Wakin - 2008 - “ People Hearing Without Listening ” An Introduction To Compressive Sampling.pdf:pdf},
journal = {Signal Processing Magazine},
keywords = {currently reading,must read},
mendeley-tags = {currently reading,must read},
title = {{“ People Hearing Without Listening :” An Introduction To Compressive Sampling}},
year = {2008}
}
@article{Cohen2006,
author = {Cohen, Albert and Dahmen, Wolfgang and Devore, Ronald},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Dahmen, Devore - 2006 - Compressed sensing and best k-term.pdf:pdf},
journal = {Contract},
pages = {1--22},
title = {{Compressed sensing and best k-term}},
year = {2006}
}
@article{Lim2010,
author = {Lim, Lek-heng and Comon, Pierre},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lim, Comon - 2010 - Multiarray Signal Processing Tensor decomposition meets compressed sensing.pdf:pdf},
journal = {Comptes-Rendus de l'Acad{\'{e}}mie des Sciences, M{\'{e}}canique},
keywords = {best rank-r approximations,blind channel identification,blind source separation,coherence,decompositions,k-rank,multiarrays,polyadic tensor,spark,sparse representations,tensor rank,tensors},
number = {338},
pages = {311--320},
title = {{Multiarray Signal Processing : Tensor decomposition meets compressed sensing}},
volume = {6},
year = {2010}
}
@article{Figueiredo2003a,
author = {Figueiredo, M{\'{a}}rio A.T.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Figueiredo - 2003 - Adaptive Sparseness for Supervised Learning.pdf:pdf},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {currently reading,must read},
mendeley-tags = {currently reading,must read},
number = {9},
pages = {1150--1159},
title = {{Adaptive Sparseness for Supervised Learning}},
volume = {25},
year = {2003}
}
@misc{MarioA.T.Figueiredo2003,
author = {{M{\'{a}}rio A. T. Figueiredo}},
pages = {1150--1159},
title = {{Adaptive Sparseness for Supervised Learning}},
year = {2003}
}
@article{Efron2003,
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron et al. - 2003 - Least Angle Regression.pdf:pdf},
keywords = {Must read,currently reading},
mendeley-tags = {Must read,currently reading},
pages = {1--44},
title = {{Least Angle Regression}},
year = {2003}
}
@article{Usage,
author = {Usage, Basic},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Usage - Unknown - The NumPy Array A Structure for Efficient Numerical Computation.pdf:pdf},
pages = {22--30},
title = {{The NumPy Array: A Structure for Efficient Numerical Computation}}
}
@article{Iccv2011,
author = {Iccv, Anonymous and Id, Paper},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Iccv, Id - 2011 - Unsupervised Metric Learning for Face Identification in TV Video.pdf:pdf},
journal = {Review Literature And Arts Of The Americas},
title = {{Unsupervised Metric Learning for Face Identification in TV Video}},
year = {2011}
}
@inproceedings{Shi2004,
author = {Shi, J. and Tomasi, C.},
booktitle = {CVPR},
keywords = {must read},
mendeley-tags = {must read},
title = {{Good features to track}},
year = {2004}
}
@inproceedings{Guillaumin2009,
abstract = {Face identification is the problem of determining whether two face images depict the same person or not. This is difficult due to variations in scale, pose, lighting, background, expression, hairstyle, and glasses. In this paper we present two methods for learning robust distance measures: (a) a logistic discriminant approach which learns the metric from a set of labelled image pairs (LDML) and (b) a nearest neighbour approach which computes the probability for two images to belong to the same class (MkNN). We evaluate our approaches on the Labeled Faces in the Wild data set, a large and very challenging data set of faces from Yahoo!News. The evaluation protocol for this data set defines a restricted setting, where a fixed set of positive and negative image pairs is given, as well as an unrestricted one, where faces are labelled by their identity. We are the first to present results for the unrestricted setting, and show that our methods benefit from this richer training data, much more so than the current state-of-the-art method. Our results of 79.3{\%} and 87.5{\%} correct for the restricted and unrestricted setting respectively, significantly improve over the current state-of-the-art result of 78.5{\%}. Confidence scores obtained for face identification can be used for many applications e.g. clustering or recognition from a single training example. We show that our learned metrics also improve performance for these tasks.},
author = {Guillaumin, Matthieu and Verbeek, Jakob and Schmid, Cordelia},
booktitle = {International Conference on Computer Vision},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Guillaumin, Verbeek, Schmid - 2009 - Is that you Metric learning approaches for face identification.pdf:pdf},
keywords = {must read,patrick},
mendeley-tags = {must read,patrick},
title = {{Is that you? Metric learning approaches for face identification}},
url = {http://hal.inria.fr/inria-00439290/en},
year = {2009}
}
@article{LEAR2011,
author = {LEAR},
journal = {ICCV},
keywords = {must read,patrick},
mendeley-tags = {must read,patrick},
title = {{Unsupervised Metric Learning for Face Identification in TV Video}},
year = {2011}
}
@article{Wang2011,
author = {Wang, Heng and Kl, Alexander and Schmid, Cordelia and Liu, Cheng-lin},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang et al. - 2011 - Action Recognition by Dense Trajectories.pdf:pdf},
keywords = {must read,patrick},
mendeley-tags = {must read,patrick},
title = {{Action Recognition by Dense Trajectories}},
year = {2011}
}
@article{Wipf2010a,
author = {Wipf, David and Nagarajan, Srikantan},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wipf, Nagarajan - 2010 - Iterative Reweighted l1 and l2 Methods for Finding Sparse Solutions.pdf:pdf},
journal = {IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING},
keywords = {sparse representations},
mendeley-tags = {sparse representations},
number = {2},
pages = {317--329},
title = {{Iterative Reweighted l1 and l2 Methods for Finding Sparse Solutions}},
volume = {4},
year = {2010}
}
@article{Eriksson,
author = {Eriksson, Anders and Hengel, Anton Van Den},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eriksson, Hengel - Unknown - Efficient Computation of Robust Low-Rank Matrix Approximations in the Presence of Missing Data using the L.pdf:pdf},
keywords = {currently reading},
mendeley-tags = {currently reading},
number = {2},
title = {{Efficient Computation of Robust Low-Rank Matrix Approximations in the Presence of Missing Data using the L 1 Norm}}
}
@article{Statement2010,
author = {Statement, Problem},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Statement - 2010 - MAP detection of image patches.pdf:pdf},
number = {4},
pages = {1--3},
title = {{MAP detection of image patches}},
year = {2010}
}
@article{Kendall1989,
author = {Kendall, Wilfrid S},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall - 1989 - The Diffusion of Euclidean Shape.pdf:pdf},
number = {1977},
title = {{The Diffusion of Euclidean Shape}},
year = {1989}
}
@article{Torr2002,
author = {Torr, P H S},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Torr - 2002 - Bayesian Model Estimation and Selection for Epipolar Geometry.pdf:pdf},
journal = {International Journal of Computer Vision},
keywords = {currently reading,must read},
mendeley-tags = {currently reading,must read},
number = {1987},
title = {{Bayesian Model Estimation and Selection for Epipolar Geometry}},
year = {2002}
}
@inproceedings{Rigamonti2010a,
author = {Rigamonti, Roberto and Brown, Matthew and Lepetit, Vincent},
booktitle = {CVPR},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rigamonti, Brown, Lepetit - 2010 - Is Sparsity Really Relevant for Image Classification.pdf:pdf},
keywords = {Image classification,currently reading,natural images,sparsity},
mendeley-tags = {currently reading},
title = {{Is Sparsity Really Relevant for Image Classification?}},
year = {2010}
}
@inproceedings{VijayChandrasekhar2009,
annote = {undefined},
author = {{Vijay Chandrasekhar} and {Gabriel Takacs} and {David Chen} and {Sam Tsai} and {Radek Grzeszczuk} and ‡ and {Bernd Girod} and †},
booktitle = {CVPR},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vijay Chandrasekhar et al. - 2009 - CHoG Compressed Histogram of Gradients.html:html},
title = {{CHoG: Compressed Histogram of Gradients}},
url = {http://www.stanford.edu/{~}bgirod/pdfs/Chandrasekhar{\_}CVPR2009.pdf},
year = {2009}
}
@book{Chandrasekhar2009,
author = {Chandrasekhar, V. and Takacs, G. and Chen, D. and Tsai, S. and Grzeszczuk, R. and Girod, B.},
booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2009.5206733},
isbn = {978-1-4244-3992-8},
month = {jun},
pages = {2504--2511},
publisher = {IEEE},
title = {{CHoG: Compressed histogram of gradients A low bit-rate feature descriptor}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/CVPRW.2009.5206733},
year = {2009}
}
@inproceedings{Devore2006,
author = {Devore, Ronald A},
booktitle = {Proceedings of the International Congress of Mathematicians},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Devore - 2006 - Optimal computation.pdf:pdf},
keywords = {compressed sensing},
mendeley-tags = {compressed sensing},
title = {{Optimal computation}},
year = {2006}
}
@inproceedings{EmamnuelJ.Candes2006,
address = {Madrid, Spain},
annote = {undefined},
author = {{Emamnuel J. Cand{\`{e}}s}},
booktitle = {Proceedings of the International Congress of Mathematicians},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Emamnuel J. Cand{\`{e}}s - 2006 - Compressive sampling.html:html},
keywords = {compressed sensing},
mendeley-tags = {compressed sensing},
title = {{Compressive sampling}},
url = {http://www-stat.stanford.edu/{~}candes/papers/CompressiveSampling.pdf},
year = {2006}
}
@inproceedings{Calonder2008,
address = {Berlin, Heidelberg},
author = {Calonder, Michael and Lepetit, Vincent and Fua, Pascal},
booktitle = {ECCV 2008},
doi = {10.1007/978-3-540-88682-2},
editor = {Forsyth, David and Torr, Philip and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Calonder, Lepetit, Fua - 2008 - Keypoint Signatures for Fast Learning and Recognition.pdf:pdf},
isbn = {978-3-540-88681-5},
issn = {0302-9743},
month = {oct},
pages = {58--71},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{Keypoint Signatures for Fast Learning and Recognition}},
url = {http://portal.acm.org/citation.cfm?id=1478392.1478401},
volume = {5302},
year = {2008}
}
@misc{,
annote = {Constrain data points in embedded space to be specified from the same weights / linear combination of neighbors as in the original space.

        
?? New constrain for dictionary training scheme ??},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - locally linear embedding (lle) homepage.html:html},
keywords = {patrick},
mendeley-tags = {patrick},
title = {locally linear embedding (lle) homepage},
url = {http://www.cs.nyu.edu/{~}roweis/lle/publications.html},
urldate = {2011-02-14}
}
@misc{Everingham2006,
abstract = {We investigate the problem of automatically labelling appearances of characters in TV or film material. This is tremendously challenging due to the huge variation in imaged appearance of each character and the weakness and ambiguity of available annotation. However, we demonstrate that high precision can be achieved by combining multiple sources of information, both visual and textual. The principal novelties that we introduce are: (i) automatic generation of time stamped character annotation by aligning subtitles and transcripts; (ii) strengthening the supervisory information by identifying when characters are speaking; (iii) using complementary cues of face matching and clothing matching to propose common annotations for face tracks. Results are presented on episodes of the TV series “Buffy the Vampire Slayer”. 1},
annote = {9 landmark points extracted from face - can use SGD},
author = {Everingham, Mark and Sivic, Josef and Zisserman, Andrew},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Everingham, Sivic, Zisserman - 2006 - Hello! My name is... Buffy – Automatic naming of characters in TV video.pdf:pdf},
keywords = {patrick},
mendeley-tags = {patrick},
publisher = {In BMVC},
title = {{Hello! My name is... Buffy – Automatic naming of characters in TV video}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.104.1500},
year = {2006}
}
@misc{Viola2001a,
abstract = {This paper describes a visual object detection framework that is capable of processing images extremely rapidly while achieving high detection rates. There are three key contributions. The first is the introduction of a new image representation called the "Integral Image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features and yields extremely efficient classifiers [6]. The third contribution is a method for combining classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. A set of experiments in the domain of face detection are presented. The system yields face detection performace comparable to the best previous systems [18, 13, 16, 12, 1]. Implemented on a conventional desktop, face detection proceeds at 15 frames per second. 1.},
author = {Viola, Paul and Jones, Michael},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2001 - Robust Real-time Object Detection(2).pdf:pdf},
keywords = {patrick},
mendeley-tags = {patrick},
publisher = {International Journal of Computer Vision},
title = {{Robust Real-time Object Detection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.23.2751},
year = {2001}
}
@article{Bay2008,
author = {Bay, Herbert and Ess, Andreas and Tuytelaars, Tinne and Gool, Luc Van},
doi = {10.1016/j.cviu.2007.09.014},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bay et al. - 2008 - SURF Speeded Up Robust Features.pdf:pdf},
issn = {10773142},
journal = {Computer Vision and Image Understanding (CVIU)},
keywords = {camera calibration,feature description,interest points,local features,object recognition,patrick},
mendeley-tags = {patrick},
month = {jun},
number = {3},
pages = {346--359},
title = {{SURF: Speeded Up Robust Features}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1077314207001555},
volume = {110},
year = {2008}
}
@inproceedings{MichaelCalonder2010,
annote = {undefined},
author = {{Michael Calonder} and {Vincent Lepetit} and {Christoph Strecha} and {Pascal Fua}},
booktitle = {ECCV},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Michael Calonder et al. - 2010 - BRIEF Binary Robust Independent Elementary Features.html:html},
keywords = {patrick},
mendeley-tags = {patrick},
pages = {778--792},
title = {{BRIEF: Binary Robust Independent Elementary Features}},
url = {http://cvlab.epfl.ch/{~}calonder/CalonderLSF10.pdf},
year = {2010}
}
@inproceedings{Zhang2010a,
abstract = {Automatically assigning relevant text keywords to images is an important problem. Many algorithms have been proposed in the past decade and achieved good performance. Efforts have focused upon model representations of keywords, but properties of features have not been well investigated. In most cases, a group of features is preselected, yet important feature properties are not well used to select features. In this paper, we introduce a regularization based feature selection algorithm to leverage both the sparsity and clustering properties of features, and incorporate it into the image annotation task. A novel approach is also proposed to iteratively obtain similar and dissimilar pairs from both the keyword similarity and the relevance feedback. Thus keyword similarity is modeled in the annotation framework. Numerous experiments are designed to compare the performance between features, feature combinations and regularization based feature selection methods applied on the image annotation task, which gives insight into the properties of features in the image annotation task. The experimental results demonstrate that the group sparsity based method is more accurate and stable than others.},
author = {Zhang, Shaoting and Huang, Junzhou and Huang, Yuchi and Yu, Yang and Li, Hongsheng and Metaxas, Dimitris N.},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540036},
isbn = {978-1-4244-6984-0},
keywords = {cv sparsity},
mendeley-tags = {cv sparsity},
month = {jun},
pages = {3312--3319},
publisher = {IEEE},
title = {{Automatic image annotation using group sparsity}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=5540036},
year = {2010}
}
@inproceedings{Oleskiw2010,
abstract = {This paper presents a sparse representation of 2D planar shape through the composition of warping functions, termed formlets, localized in scale and space. Each formlet subjects the 2D space in which the shape is embedded to a localized isotropic radial deformation. By constraining these localized warping transformations to be diffeomorphisms, the topology of shape is preserved, and the set of simple closed curves is closed under any sequence of these warpings. A generative model based on a composition of formlets applied to an embryonic shape, e.g., an ellipse, has the advantage of synthesizing only those shapes that could correspond to the boundaries of physical objects. To compute the set of formlets that represent a given boundary, we demonstrate a greedy coarse-to-fine formlet pursuit algorithm that serves as a non-commutative generalization of matching pursuit for sparse approximations. We evaluate our method by pursuing partially occluded shapes, comparing performance against a contour-based sparse shape coding framework.},
author = {Oleskiw, Timothy D. and Elder, James H. and Peyre, Gabriel},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540179},
isbn = {978-1-4244-6984-0},
keywords = {cv sparsity},
mendeley-tags = {cv sparsity},
month = {jun},
pages = {459--466},
publisher = {IEEE},
title = {{On growth and formlets: Sparse multi-scale coding of planar shape}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=5540179},
year = {2010}
}
@inproceedings{Masnadi-shirazi,
abstract = {The design of robust classifiers, which can contend with the noisy and outlier ridden datasets typical of computer vision, is studied. It is argued that such robustness requires loss functions that penalize both large positive and negative margins. The probability elicitation view of classifier design is adopted, and a set of necessary conditions for the design of such losses is identified. These conditions are used to derive a novel robust Bayes-consistent loss, denoted Tangent loss, and an associated boosting algorithm, denoted TangentBoost. Experiments with data from the computer vision problems of scene classification, object tracking, and multiple instance learning show that TangentBoost consistently outperforms previous boosting algorithms. 1.},
author = {Masnadi-shirazi, Hamed and Mahadevan, Vijay and Vasconcelos, Nuno},
booktitle = {CVPR 2010},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Masnadi-shirazi, Mahadevan, Vasconcelos - Unknown - On the design of robust classifiers for computer vision.pdf:pdf},
keywords = {cv sparsity},
mendeley-tags = {cv sparsity},
title = {{On the design of robust classifiers for computer vision}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.172.6416}
}
@book{Rodriguez2010,
abstract = {Thousands of hours of video are recorded every second across the world. Due to the fact that searching for a particular event of interest within hours of video is time consuming, most captured videos are never examined, and are only used in a post-factum manner. In this work, we introduce activity-specific video summaries, which provide an effective means of browsing and indexing video based on a set of events of interest. Our method automatically generates a compact video representation of a long sequence, which features only activities of interest while preserving the general dynamics of the original video. Given a long input video sequence, we compute optical flow and represent the corresponding vector field in the Clifford Fourier domain. Dynamic regions within the flow field are identified within the phase spectrum volume of the flow field. We then compute the likelihood that certain activities of relevance occur within the the video by correlating it with spatio-temporal maximum average correlation height filters. Finally, the input sequence is condensed via a temporal shift optimization, resulting in a short video clip which simultaneously displays multiple instances of each relevant activity.},
author = {Rodriguez, Mikel},
booktitle = {2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2010.5540030},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rodriguez - 2010 - CRAM Compact representation of actions in movies.pdf:pdf},
isbn = {978-1-4244-6984-0},
keywords = {video search},
mendeley-tags = {video search},
month = {jun},
pages = {3328--3335},
publisher = {IEEE},
title = {{CRAM: Compact representation of actions in movies}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=5540030},
year = {2010}
}
@inproceedings{Erikssona,
abstract = {The calculation of a low-rank approximation of a matrix is a fundamental operation in many computer vision applications. The workhorse of this class of problems has long been the Singular Value Decomposition. However, in the presence of missing data and outliers this method is not applicable, and unfortunately, this is often the case in practice. In this paper we present a method for calculating the low-rank factorization of a matrix which minimizes the L1 norm in the presence of missing data. Our approach represents a generalization the Wiberg algorithm of one of the more convincing methods for factorization under the L2 norm. By utilizing the differentiability of linear programs, we can extend the underlying ideas behind this approach to include this class of L1 problems as well. We show that the proposed algorithm can be efficiently implemented using existing optimization software. We also provide preliminary experiments on synthetic as well as real world data with very convincing results. 1.},
author = {Eriksson, Anders and Hengel, Anton Van Den},
booktitle = {CVPR 2010},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eriksson, Hengel - Unknown - Efficient Computation of Robust Low-Rank Matrix Approximations in the Presence of Missing Data using the L1.pdf:pdf},
keywords = {best paper},
mendeley-tags = {best paper},
title = {{Efficient Computation of Robust Low-Rank Matrix Approximations in the Presence of Missing Data using the L1 Norm}}
}
@article{Rabiner1989,
author = {Rabiner, Lawrence R. and Rabiner, Lawrence R},
journal = {[[Proceedings of the IEEE]]},
keywords = {background},
mendeley-tags = {background},
number = {2},
pages = {257 -- 286},
title = {{A tutorial on hidden Markov models and selected applications in speech recognition}},
url = {http://en.wikipedia.org/wiki/List{\_}of{\_}important{\_}publications{\_}in{\_}computer{\_}science},
volume = {77},
year = {1989}
}
@article{Ram2010,
abstract = {In this paper we propose a new wavelet transform applicable to functions defined on graphs, high dimensional data and networks. The proposed method generalizes the Haar-like transform proposed in $\backslash$cite{\{}gavish2010mwot{\}}, and it is similarly defined via a hierarchical tree, which is assumed to capture the geometry and structure of the input data. It is applied to the data using a multiscale filtering and decimation scheme, which can employ different wavelet filters. We propose a tree construction method which results in efficient representation of the input function in the transform domain. We show that the proposed transform is more efficient than both the 1D and 2D separable wavelet transforms in representing images. We also explore the application of the proposed transform to image denoising, and show that combined with a subimage averaging scheme, it achieves denoising results which are similar to the ones obtained with the K-SVD algorithm.},
archivePrefix = {arXiv},
arxivId = {1011.4615},
author = {Ram, Idan and Elad, Michael and Cohen, Israel},
eprint = {1011.4615},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ram, Elad, Cohen - 2010 - Generalized Tree-Based Wavelet Transform.pdf:pdf},
keywords = {Computer Vision and Pattern Recognition},
month = {nov},
pages = {10},
title = {{Generalized Tree-Based Wavelet Transform}},
url = {http://arxiv.org/abs/1011.4615},
year = {2010}
}
@article{Candes2009,
abstract = {This paper is about a curious phenomenon. Suppose we have a data matrix, which is the superposition of a low-rank component and a sparse component. Can we recover each component individually? We prove that under some suitable assumptions, it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit; among all feasible decompositions, simply minimize a weighted combination of the nuclear norm and of the L1 norm. This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily corrupted. This extends to the situation where a fraction of the entries are missing as well. We discuss an algorithm for solving this optimization problem, and present applications in the area of video surveillance, where our methodology allows for the detection of objects in a cluttered background, and in the area of face recognition, where it offers a principled way of removing shadows and specularities in images of faces.},
archivePrefix = {arXiv},
arxivId = {0912.3599},
author = {Candes, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
eprint = {0912.3599},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Candes et al. - 2009 - Robust Principal Component Analysis.pdf:pdf},
keywords = {Information Theory,cedric herzet,must read},
mendeley-tags = {cedric herzet},
month = {dec},
title = {{Robust Principal Component Analysis?}},
url = {http://arxiv.org/abs/0912.3599},
year = {2009}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Tim Davis University of Florida Sparse Matrix Collection sparse matrices from a wide range of applications.html:html},
keywords = {jean-ronan},
mendeley-tags = {jean-ronan},
title = {{Tim Davis: University of Florida Sparse Matrix Collection : sparse matrices from a wide range of applications}},
url = {http://www.cise.ufl.edu/research/sparse/matrices/},
urldate = {2011-01-26}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - University of florida sparse matrix library.html:html},
keywords = {jean-ronan},
mendeley-tags = {jean-ronan},
title = {{University of florida sparse matrix library}},
url = {http://www.cise.ufl.edu/{~}davis/techreports/matrices.pdf},
urldate = {2011-01-26}
}
@inproceedings{JagannadanVaradarajan2010,
address = {Aberystwyth},
annote = {undefined},
author = {{Jagannadan Varadarajan} and {R{\'{e}}mi Emonet} and {Jean-Marc Odobez}},
booktitle = { Proc. British Machine and Vision Conference},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jagannadan Varadarajan, R{\'{e}}mi Emonet, Jean-Marc Odobez - 2010 - Probabilistic Latent Sequential Motifs Discovering temporal activity pat.html:html},
keywords = {must read,patrick},
mendeley-tags = {must read,patrick},
title = {{Probabilistic Latent Sequential Motifs: Discovering temporal activity patterns in video scenes}},
url = {http://www.idiap.ch/{~}odobez/publications/VaradarajanEmonetOdobez-BMVC-2010.pdf},
year = {2010}
}
@article{J.J.Koenderink1979,
author = {{J. J. Koenderink} and {A. J. Doorn}},
journal = {BIOLOGICAL CYBERNETICS},
keywords = {background},
mendeley-tags = {background},
number = {4},
pages = {211--216},
title = {{The internal representation of solid shape with respect to vision}},
url = {https://springerlink3.metapress.com/content/r6r7586802h37826/resource-secured/?target=fulltext.pdf{\&}sid=u24nrw45pglcnc5520vv4t3n{\&}sh=www.springerlink.com},
volume = {32},
year = {1979}
}
@book{Ripley2008,
abstract = {Pattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.},
author = {Ripley, Brian D.},
isbn = {0521717701},
keywords = {background},
mendeley-tags = {background},
pages = {416},
publisher = {Cambridge University Press},
title = {{Pattern Recognition and Neural Networks}},
url = {http://books.google.com/books?id=m12UR8QmLqoC{\&}pgis=1},
year = {2008}
}
@article{Gold1998,
abstract = {A fundamental open problem in computer vision—determining pose and correspondence between two sets of points in space—is solved with a novel, fast, robust and easily implementable algorithm. The technique works on noisy 2D or 3D point sets that may be of unequal sizes and may differ by non-rigid transformations. Using a combination of optimization techniques such as deterministic annealing and the softassign, which have recently emerged out of the recurrent neural network/statistical physics framework, analog objective functions describing the problems are minimized. Over thirty thousand experiments, on randomly generated points sets with varying amounts of noise and missing and spurious points, and on hand-written character sets demonstrate the robustness of the algorithm.},
author = {Gold, Steven and Rangarajan, Anand and Lu, Chien-Ping and Pappu, Suguna and Mjolsness, Eric},
doi = {10.1016/S0031-3203(98)80010-1},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {affine transformation,background,correspondence,deterministic annealing,neural networks,optimization,point-matching,pose estimation,softassign},
mendeley-tags = {background},
month = {aug},
number = {8},
pages = {1019--1031},
title = {{New algorithms for 2D and 3D point matching: pose estimation and correspondence}},
url = {http://dx.doi.org/10.1016/S0031-3203(98)80010-1},
volume = {31},
year = {1998}
}
@article{Jonker1987,
abstract = {We develop a shortest augmenting path algorithm for the linear assignment problem. It contains new initialization routines and a special implementation of Dijkstra's shortest path method. For both dense and sparse problems computational experiments show this algorithm to be uniformly faster than the best algorithms from the literature. A Pascal implementation is presented.},
author = {Jonker, R. and Volgenant, A.},
doi = {10.1007/BF02278710},
issn = {0010-485X},
journal = {Computing},
keywords = {Computer Science,background},
mendeley-tags = {background},
month = {dec},
number = {4},
pages = {325--340},
publisher = {Springer Wien},
title = {{A shortest augmenting path algorithm for dense and sparse linear assignment problems}},
url = {http://www.springerlink.com/content/7003m6n54004m004/},
volume = {38},
year = {1987}
}
@article{Welch1995,
abstract = {In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to advances in digital computing, the Kalman filter has been the subject of extensive research and application, particularly in the area of autonomous or assisted navigation. The Kalman filter is a set of mathematical equations that provides an efficient computational (recursive) means to estimate the state of a process, in a way that minimizes the mean of the squared error. The filter is very powerful in several aspects: it supports estimations of past, present, and even future states, and it can do so even when the precise nature of the modeled system is unknown. The purpose of this paper is to provide a practical introduction to the discrete Kalman filter. This introduction includes a description and some discussion of the basic discrete Kalman filter, a derivation, description and some discussion of the extended Kalman filter, and a relatively simple (tangible) example with real numbers {\&} results.},
author = {Welch, Greg and Bishop, Gary},
chapter = {8},
doi = {10.1.1.117.6808},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Welch, Bishop - 1995 - An Introduction to the Kalman Filter.pdf:pdf},
institution = {University of North Carolina at Chapel Hill},
journal = {University of North Carolina at Chapel Hill Chapel Hill NC},
keywords = {background},
mendeley-tags = {background},
number = {TR 95-041},
pages = {1--16},
publisher = {Citeseer},
title = {{An Introduction to the Kalman Filter}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6578{\&}rep=rep1{\&}type=pdf},
volume = {95},
year = {1995}
}
@inproceedings{Viola2001b,
author = {Viola, Paul and Jones, Michael},
booktitle = {SECOND INTERNATIONAL WORKSHOP ON STATISTICAL AND COMPUTATIONAL THEORIES OF VISION MODELING, LEARNING, COMPUTING, AND SAMPLING},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Viola, Jones - 2001 - Robust Real-time Object Detection.pdf:pdf},
keywords = {must read,patrick},
mendeley-tags = {must read,patrick},
title = {{Robust Real-time Object Detection}},
year = {2001}
}
@article{Radovanovic2010,
author = {Radovanovi{\'{c}}, Milo{\v{s}} and Nanopoulos, Alexandros and Ivanovi{\'{c}}, Mirjana},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Radovanovi{\'{c}}, Nanopoulos, Ivanovi{\'{c}} - 2010 - Hubs in Space Popular Nearest Neighbors in High-Dimensional Data.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {classification,currently reading,curse of dimensionality,herve,must read,nearest neighbors,semi-supervised learning},
mendeley-tags = {currently reading,herve,must read},
pages = {2487--2531},
title = {{Hubs in Space : Popular Nearest Neighbors in High-Dimensional Data}},
volume = {11},
year = {2010}
}
@article{Pengwei2001,
abstract = {Reversible integer mapping is essential for lossless source coding by transformation. A general matrix factorization theory for reversible integer mapping of invertible linear transforms is developed. Concepts of the integer factor and the elementary reversible matrix (ERM) for integer mapping are introduced, and two forms of ERM-triangular ERM (TERM) and single-row ERM (SERM)-are studied. We prove that there exist some approaches to factorize a matrix into TERMs or SERMs if the transform is invertible and in a finite-dimensional space. The advantages of the integer implementations of an invertible linear transform are (i) mapping integers to integers, (ii) perfect reconstruction, and (iii) in-place calculation. We find that besides a possible permutation matrix, the TERM factorization of an N-by-N nonsingular matrix has at most three TERMs, and its SERM factorization has at most N+1 SERMs. The elementary structure of ERM transforms is the ladder structure. An executable factorization algorithm is also presented. Then, the computational complexity is compared, and some optimization approaches are proposed. The error bounds of the integer implementations are estimated as well. Finally, three ERM factorization examples of DFT, DCT, and DWT are given},
author = {Pengwei, Hao and Qingyun, Shi},
doi = {10.1109/78.950787},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pengwei, Qingyun - 2001 - Matrix factorizations for reversible integer mapping.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {JPEG2000,Jonathan Taquet,lifting},
mendeley-tags = {JPEG2000,Jonathan Taquet,lifting},
number = {10},
pages = {2314--2324},
title = {{Matrix factorizations for reversible integer mapping}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=950787},
volume = {49},
year = {2001}
}
@article{HAO2004,
abstract = {Customizable triangular factorizations of matrices find their applications in computer graphics and lossless transform coding. In this paper, we prove that any N × N nonsingular matrix A can be factorized into 3 triangular matrices, A=PLUS, where P is a permutation matrix, L is a unit lower triangular matrix, U is an upper triangular matrix of which the diagonal entries are customizable and can be given by all means as long as its determinant is equal to that of A up to a possible sign adjustment, and S is a unit lower triangular matrix of which all but N−1 off-diagonal elements are set zeros and the positions of those N−1 elements are also flexibly customizable, such as a single-row, a single-column, a bidiagonal matrix or other specially patterned matrices. A pseudo-permutation matrix, which is a simple unit upper triangular matrix with off-diagonal elements being 0, 1 or −1, can take the role of the permutation matrix P as well. In some cases, P may be the identity matrix. Besides PLUS, a customizable factorization also has other alternatives, LUSP, PSUL or SULP for lower S, and PULS, ULSP, PSLU, SLUP for upper S.},
author = {HAO, P},
doi = {10.1016/j.laa.2003.12.023},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/HAO - 2004 - Customizable triangular factorizations of matrices.pdf:pdf},
issn = {00243795},
journal = {Linear Algebra and its Applications},
keywords = {JPEG2000,Jonathan Taquet,lifting},
mendeley-tags = {JPEG2000,Jonathan Taquet,lifting},
month = {may},
pages = {135--154},
title = {{Customizable triangular factorizations of matrices}},
url = {http://dx.doi.org/10.1016/j.laa.2003.12.023},
volume = {382},
year = {2004}
}
@article{Pennec2006,
author = {Pennec, Xavier},
doi = {10.1007/s10851-006-6228-4},
issn = {0924-9907},
journal = {Journal of Mathematical Imaging and Vision},
keywords = {Frechet mean,Riemannian manifolds,computing on manifolds,covariance,geometry,patrick,statistics},
mendeley-tags = {patrick},
month = {jul},
number = {1},
pages = {127--154},
title = {{Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measurements}},
url = {http://portal.acm.org/citation.cfm?id=1166859.1166868},
volume = {25},
year = {2006}
}
@book{Pennec1996,
author = {Pennec, X. and Ayache, N.},
booktitle = {Proceedings CVPR IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.1996.517116},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pennec, Ayache - 1996 - Randomness and geometric features in computer vision.pdf:pdf},
isbn = {0-8186-7258-7},
keywords = {patrick},
mendeley-tags = {patrick},
month = {jun},
pages = {484--491},
publisher = {IEEE Comput. Soc. Press},
title = {{Randomness and geometric features in computer vision}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/CVPR.1996.517116},
year = {1996}
}
@article{Belongie2002,
abstract = {We present a novel approach to measuring similarity between shapes and exploit it for object recognition. In our framework, the measurement of similarity is preceded by: (1) solving for correspondences between points on the two shapes; (2) using the correspondences to estimate an aligning transform. In order to solve the correspondence problem, we attach a descriptor, the shape context, to each point. The shape context at a reference point captures the distribution of the remaining points relative to it, thus offering a globally discriminative characterization. Corresponding points on two similar shapes will have similar shape contexts, enabling us to solve for correspondences as an optimal assignment problem. Given the point correspondences, we estimate the transformation that best aligns the two shapes; regularized thin-plate splines provide a flexible class of transformation maps for this purpose. The dissimilarity between the two shapes is computed as a sum of matching errors between corresponding points, together with a term measuring the magnitude of the aligning transform. We treat recognition in a nearest-neighbor classification framework as the problem of finding the stored prototype shape that is maximally similar to that in the image. Results are presented for silhouettes, trademarks, handwritten digits, and the COIL data set},
author = {Belongie, S. and Malik, J. and Puzicha, J.},
doi = {10.1109/34.993558},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Belongie, Malik, Puzicha - 2002 - Shape matching and object recognition using shape contexts.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {must read,patrick},
mendeley-tags = {must read,patrick},
month = {apr},
number = {4},
pages = {509--522},
title = {{Shape matching and object recognition using shape contexts}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=993558},
volume = {24},
year = {2002}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CVPR10 - Open Source Vision Software.html:html},
keywords = {intro and training,open source vision software,organizers},
title = {{CVPR10 - Open Source Vision Software}},
url = {http://www.vlfeat.org/cvpr10/Open{\_}Source{\_}Vision{\_}Software,{\_}Intro{\_}and{\_}Training},
urldate = {2010-12-22}
}
@article{Cotter1999,
abstract = {The problem of signal representation in terms of basis vectors from a large, over-complete, spanning dictionary has been the focus of much research. Achieving a succinct, or `sparse', representation is known as the problem of best basis representation. Methods are considered which seek to solve this problem by sequentially building up a basis set for the signal. Three distinct algorithm types have appeared in the literature which are here termed basic matching pursuit (BMP), order recursive matching pursuit (ORMP) and modified matching pursuit (MMP). The algorithms are first described and then their computation is closely examined. Modifications are made to each of the procedures which improve their computational efficiency. The complexity of each algorithm is considered in two contexts; one where the dictionary is variable (time-dependent) and the other where the dictionary is fixed (time-independent). Experimental results are presented which demonstrate that the ORMP method is the best procedure in terms of its ability to give the most compact signal representation, followed by MMP and then BMP which gives the poorest results. Finally, weighing the performance of each algorithm, its computational complexity and the type of dictionary available, recommendations are made as to which algorithm should be used for a given problem},
author = {Cotter, S. F. and Rao, B. D. and Kreutz-Delgado, K. and Adler, J.},
doi = {10.1049/ip-vis:19990445},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - forward.pdf.pdf:pdf},
issn = {1350245X},
journal = {IEE Proceedings - Vision, Image, and Signal Processing},
number = {5},
pages = {235},
title = {{Forward sequential algorithms for best basis selection}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?tp={\&}arnumber=826992},
volume = {146},
year = {1999}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - VLFeat - Applications - sift{\_}mosaic.m.html:html},
keywords = {Must read},
mendeley-tags = {Must read},
title = {{VLFeat - Applications - sift{\_}mosaic.m}},
url = {http://www.vlfeat.org/applications/sift-mosaic-code.html},
urldate = {2010-12-15}
}
@inproceedings{JianboShi;Tomasi,
abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not correspond to points in the world. These methods are based on a new tracking algorithm that extends previous Newton-Raphson style search methods to work under affine image transformations. We test performance with several simulations and experiments},
author = {{Jianbo Shi;   Tomasi}, C.;},
booktitle = {Proceedings of IEEE Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.1994.323794},
isbn = {0-8186-5825-8},
keywords = {Must read},
mendeley-tags = {Must read},
pages = {593--600},
publisher = {IEEE Comput. Soc. Press},
title = {{Good features to track}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=323794}
}
@inproceedings{Douze2010,
abstract = {This paper introduces a very compact yet discriminative video description, which allows example-based search in a large number of frames corresponding to thousands of hours of video. Our description extracts one descriptor per indexed video frame by aggregating a set of local descriptors. These frame descriptors are encoded using a time-aware hierarchical indexing structure. A modified temporal Hough voting scheme is used to rank the retrieved database videos and estimate segments in them that match the query. If we use a dense temporal description of the videos, matched video segments are localized with excellent precision. Experimental results on the Trecvid 2008 copy detection task and a set of 38000 videos from YouTube show that our method offers an excellent trade-off between search accuracy, efficiency and memory usage.},
author = {Douze, Matthijs and J{\'{e}}gou, Herv{\'{e}} and Schmid, Cordelia and P{\'{e}}rez, Patrick},
booktitle = {ECCV 2010},
doi = {10.1007/978-3-642-15549-9_38},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Douze et al. - 2010 - Compact Video Description for Copy Detection with Precise Temporal Alignment.pdf:pdf},
keywords = {Must read},
mendeley-tags = {Must read},
pages = {522--535},
publisher = {Springer Berlin / Heidelberg},
title = {{Compact Video Description for Copy Detection with Precise Temporal Alignment}},
url = {http://www.springerlink.com/content/g811634570617831/},
volume = {6311},
year = {2010}
}
@misc{,
annote = {undefined},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - Belongie's Top Ten TPAMI articles.html:html},
title = {{Belongie's Top Ten TPAMI articles}},
url = {http://www.computer.org/portal/web/tpami/belongie},
urldate = {2010-12-08}
}
@article{Hartley1997,
abstract = {The fundamental matrix is a basic tool in the analysis of scenes taken with two uncalibrated cameras, and the eight-point algorithm is a frequently cited method for computing the fundamental matrix from a set of eight or more point matches. It has the advantage of simplicity of implementation. The prevailing view is, however, that it is extremely susceptible to noise and hence virtually useless for most purposes. This paper challenges that view, by showing that by preceding the algorithm with a very simple normalization (translation and scaling) of the coordinates of the matched points, results are obtained comparable with the best iterative algorithms. This improved performance is justified by theory and verified by extensive experiments on real images},
author = {Hartley, R.I.},
doi = {10.1109/34.601246},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hartley - 1997 - In Defense of the Eight-Point Algorithm.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
keywords = {Must read},
mendeley-tags = {Must read},
number = {6},
pages = {580--593},
title = {{In Defense of the Eight-Point Algorithm}},
volume = {19},
year = {1997}
}
@article{Malik2000,
abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging},
author = {Malik, J.},
doi = {10.1109/34.868688},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Malik - 2000 - Normalized cuts and image segmentation.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {888--905},
title = {{Normalized cuts and image segmentation}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=868688},
volume = {22},
year = {2000}
}
@article{Bookstein1989,
abstract = {The decomposition of deformations by principal warps is demonstrated. The method is extended to deal with curving edges between landmarks. This formulation is related to other applications of splines current in computer vision. How they might aid in the extraction of features for analysis, comparison, and diagnosis of biological and medical images in indicated},
author = {Bookstein, F.L.},
doi = {10.1109/34.24792},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bookstein - 1989 - Principal warps thin-plate splines and the decomposition of deformations.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {jun},
number = {6},
pages = {567--585},
title = {{Principal warps: thin-plate splines and the decomposition of deformations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=24792},
volume = {11},
year = {1989}
}
@article{Freeman1991,
abstract = {The authors present an efficient architecture to synthesize filters of arbitrary orientations from linear combinations of basis filters, allowing one to adaptively steer a filter to any orientation, and to determine analytically the filter output as a function of orientation. Steerable filters may be designed in quadrature pairs to allow adaptive control over phase as well as orientation. The authors show how to design and steer the filters and present examples of their use in the analysis of orientation and phase, angularly adaptive filtering, edge detection, and shape from shading. One can also build a self-similar steerable pyramid representation. The same concepts can be generalized to the design of 3-D steerable filters},
author = {Freeman, W.T. and Adelson, E.H.},
doi = {10.1109/34.93808},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Freeman, Adelson - 1991 - The design and use of steerable filters.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {9},
pages = {891--906},
title = {{The design and use of steerable filters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=93808},
volume = {13},
year = {1991}
}
@article{Sawhney,
abstract = {An explosion of on-line image and video data in digital form is already well underway. With the exponential rise in interactive information exploration and dissemination through the World-Wide Web (WWW), the major inhibitors of rapid access to on-line video data are costs and management of capture and storage, lack of real-time delivery, and nonavailability of content-based intelligent search and indexing techniques. The solutions for capture, storage, and delivery may be on the horizon or a little beyond. However, even with rapid delivery, the lack of efficient authoring and querying tools for visual content-based indexing may still inhibit as widespread a use of video information as that of text and traditional tabular data is currently. In order to be able to nonlinearly browse and index into videos through visual content, it is necessary to develop authoring tools that can automatically separate moving objects and significant components of the scene, and represent these in a compact form. Given that video data comes in torrents-almost a megabyte every 30th of a second-it will be highly inefficient to search for objects and scenes in every frame of a video. In this paper, we present techniques to automatically derive compact representations of scenes and objects from the motion information. Image motion is a significant cue in videos for the separation of scenes into their significant components and for the separation of moving objects. Motion analysis is useful in capturing the visual content of videos for indexing and browsing in two different ways. First, separation of the static scene from moving objects can be accomplished by employing dominant 2D/3D motion estimation methods. Alternatively, if the goal is to be able to represent the fixed scene too as a composition of significant structures and objects, then simultaneous multiple motion methods might be more appropriate. In either case, view-based summarized representations of the scene can be created by video compositing/mosaicing based on the estimated motions. We present robust algorithms for both kinds of representations: 1) dominant motion estimation based techniques which exploit a fairly common occurrence in videos that a mostly fixed background (scene) is imaged with or without independently moving objects, and 2) simultaneous multiple motion estimation and representation of motion video using layered representations. Ample examples of the representations achieved by each method are included in the paper},
author = {Sawhney, H.S. and Ayer, S.},
doi = {10.1109/34.531801},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sawhney, Ayer - Unknown - Compact representations of videos through dominant and multiple motion estimation.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {8},
pages = {814--830},
title = {{Compact representations of videos through dominant and multiple motion estimation}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=531801},
volume = {18}
}
@article{Canny1986,
abstract = {This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge.},
author = {Canny, John},
doi = {10.1109/TPAMI.1986.4767851},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Canny - 1986 - A Computational Approach to Edge Detection.pdf:pdf},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
month = {nov},
number = {6},
pages = {679--698},
title = {{A Computational Approach to Edge Detection}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4767851},
volume = {PAMI-8},
year = {1986}
}
@article{Boykov2001,
abstract = {Many tasks in computer vision involve assigning a label (such as disparity) to every pixel. A common constraint is that the labels should vary smoothly almost everywhere while preserving sharp discontinuities that may exist, e.g., at object boundaries. These tasks are naturally stated in terms of energy minimization. The authors consider a wide class of energies with various smoothness constraints. Global minimization of these energy functions is NP-hard even in the simplest discontinuity-preserving case. Therefore, our focus is on efficient approximation algorithms. We present two algorithms based on graph cuts that efficiently find a local minimum with respect to two types of large moves, namely expansion moves and swap moves. These moves can simultaneously change the labels of arbitrarily large sets of pixels. In contrast, many standard algorithms (including simulated annealing) use small moves where only one pixel changes its label at a time. Our expansion algorithm finds a labeling within a known factor of the global minimum, while our swap algorithm handles more general energy functions. Both of these algorithms allow important cases of discontinuity preserving energies. We experimentally demonstrate the effectiveness of our approach for image restoration, stereo and motion. On real data with ground truth, we achieve 98 percent accuracy},
author = {Boykov, Y. and Veksler, O. and Zabih, R.},
doi = {10.1109/34.969114},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boykov, Veksler, Zabih - 2001 - Fast approximate energy minimization via graph cuts.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {11},
pages = {1222--1239},
title = {{Fast approximate energy minimization via graph cuts}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=969114},
volume = {23},
year = {2001}
}
@article{Amit1997,
abstract = {We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classification trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial affine and nonlinear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Different trees correspond to different aspects of shape. They are statistically and weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classified handwritten digits from the NIST database; the error rate was 0.7 percent},
author = {Amit, Y. and Geman, D. and Wilder, K.},
doi = {10.1109/34.632990},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amit, Geman, Wilder - 1997 - Joint Induction of Shape Features and Tree Classifiers.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {11},
pages = {1300--1305},
title = {{Joint Induction of Shape Features and Tree Classifiers}},
volume = {19},
year = {1997}
}
@article{Umeyama1988,
abstract = {An approximate solution to the weighted-graph-matching problem is discussed for both undirected and directed graphs. The weighted-graph-matching problem is that of finding the optimum matching between two weighted graphs, which are graphs with weights at each arc. The proposed method uses an analytic instead of a combinatorial or iterative approach to the optimum matching problem. Using the eigendecompositions of the adjacency matrices (in the case of the undirected-graph-matching problem) or Hermitian matrices derived from the adjacency matrices (in the case of the directed-graph-matching problem), a matching close to the optimum can be found efficiently when the graphs are sufficiently close to each other. Simulation results are given to evaluate the performance of the proposed method},
author = {Umeyama, S.},
doi = {10.1109/34.6778},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Umeyama - 1988 - An Eigendecomposition Approach to Weighted Graph Matching Problems.pdf:pdf},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {5},
pages = {695--703},
title = {{An Eigendecomposition Approach to Weighted Graph Matching Problems}},
volume = {10},
year = {1988}
}
@article{Agarwal2004a,
abstract = {We study the problem of detecting objects in still, gray-scale images. Our primary focus is the development of a learning-based approach to the problem that makes use of a sparse, part-based representation. A vocabulary of distinctive object parts is automatically constructed from a set of sample images of the object class of interest; images are then represented using parts from this vocabulary, together with spatial relations observed among the parts. Based on this representation, a learning algorithm is used to automatically learn to detect instances of the object class in new images. The approach can be applied to any object with distinguishable parts in a relatively fixed spatial configuration; it is evaluated here on difficult sets of real-world images containing side views of cars, and is seen to successfully detect objects in varying conditions amidst background clutter and mild occlusion. In evaluating object detection approaches, several important methodological issues arise that have not been satisfactorily addressed in previous work. A secondary focus of this paper is to highlight these issues and to develop rigorous evaluation standards for the object detection problem. A critical evaluation of our approach under the proposed standards is presented.},
author = {Agarwal, Shivani and Awan, Aatif and Roth, Dan},
doi = {10.1109/TPAMI.2004.108},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Agarwal, Awan, Roth - 2004 - Learning to detect objects in images via a sparse, part-based representation.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Cluster Analysis,Computer Graphics,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Image Enhancement,Image Enhancement: methods,Image Interpretation,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models,Must read,Numerical Analysis,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Signal Processing,Statistical,Subtraction Technique,User-Computer Interface,currently reading},
mendeley-tags = {Must read,currently reading},
month = {nov},
number = {11},
pages = {1475--90},
pmid = {15521495},
title = {{Learning to detect objects in images via a sparse, part-based representation.}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=1335452},
volume = {26},
year = {2004}
}
@article{Kendall1984,
abstract = {The shape-space [IMG]f1.gif" ALT="Formula" BORDER="0"{\textgreater} whose points {\{}sigma{\}} represent the shapes of not totally degenerate k-ads in Rm is introduced as a quotient space carrying the quotient metric. When m = 1, we find that [IMG]f2.gif" ALT="Formula" BORDER="0"{\textgreater}when m [{\&}ge;] 3, the shape-space contains singularities. This paper deals mainly with the case m = 2, when the shape-space [IMG]f3.gif" ALT="Formula" BORDER="0"{\textgreater} can be identified with a version of CPk-2. Of special importance are the shape-measures induced on CPk-2 by any assigned diffuse law of distribution for the k vertices. We determine several such shape-measures, we resolve some of the technical problems associated with the graphic presentation and statistical analysis of empirical shape distributions, and among applications we discuss the relevance of these ideas to testing for the presence of non-accidental multiple alignments in collections of (i) neolithic stone monuments and (ii) quasars. Finally the recently introduced Ambartzumian density is examined from the present point of view, its norming constant is found, and its connexion with random Crofton polygons is established.},
author = {Kendall, David G.},
doi = {10.1112/blms/16.2.81},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kendall - 1984 - Shape Manifolds, Procrustean Metrics, and Complex Projective Spaces.pdf:pdf},
journal = {Bull. London Math. Soc.},
keywords = {Must read},
mendeley-tags = {Must read},
number = {2},
pages = {81--121},
title = {{Shape Manifolds, Procrustean Metrics, and Complex Projective Spaces}},
url = {http://blms.oxfordjournals.org/cgi/content/abstract/16/2/81},
volume = {16},
year = {1984}
}
@article{Das2010,
abstract = {Our goal is to develop statistical models for the shape change of a configuration of "landmark" points (key points of interest) over time and to use these models for filtering and tracking to automatically extract landmarks, synthesis, and change detection. The term "shape activity" was introduced in recent work to denote a particular stochastic model for the dynamics of landmark shapes (dynamics after global translation, scale, and rotation effects are normalized for). In that work, only models for stationary shape sequences were proposed. But most "activities" of a set of landmarks, e.g., running, jumping, or crawling, have large shape changes with respect to initial shape and hence are nonstationary. The key contribution of this work is a novel approach to define a generative model for both 2D and 3D nonstationary landmark shape sequences. Greatly improved performance using the proposed models is demonstrated for sequentially filtering noise-corrupted landmark configurations to compute Minimum Mean Procrustes Square Error (MMPSE) estimates of the true shape and for tracking human activity videos, i.e., for using the filtering to predict the locations of the landmarks (body parts) and using this prediction for faster and more accurate landmarks extraction from the current image.},
author = {Das, Samarjit and Vaswani, Namrata},
doi = {10.1109/TPAMI.2009.94},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Das, Vaswani - 2010 - Nonstationary shape activities dynamic models for landmark shape change and applications.pdf:pdf},
issn = {1939-3539},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Automated,Automated: methods,Computer-Assisted,Computer-Assisted: methods,Humans,Image Processing,Models,Movement,Pattern Recognition,Theoretical,Walking},
month = {apr},
number = {4},
pages = {579--92},
pmid = {20224116},
publisher = {Published by the IEEE Computer Society},
title = {{Nonstationary shape activities: dynamic models for landmark shape change and applications.}},
url = {http://www.computer.org/portal/web/csdl/doi/10.1109/TPAMI.2009.94},
volume = {32},
year = {2010}
}
@misc{Park,
abstract = {. This paper presents a linear solution for reconstructing the 3D trajectory of a moving point from its correspondence in a collection of 2D perspective images, given the 3D spatial pose and time of capture of the cameras that produced each image. Triangulation-based solutions do not apply, as multiple views of the point may not exist at each instant in time. A geometric analysis of the problem is presented and a criterion, called reconstructibility, is defined to precisely characterize the cases when reconstruction is possible, and how accurate it can be. We apply the linear reconstruction algorithm to reconstruct the time evolving 3D structure of several real-world scenes, given a collection of non-coincidental 2D images. Keywords: Multiple view geometry, Non-rigid structure from motion, Trajectory basis, and Reconstructibility 1},
author = {Park, Hyun Soo and Shiratori, Takaaki and Matthews, Iain and Sheikh, Yaser},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - Unknown - 3D Reconstruction of a Moving Point from a Series of 2D Projections.pdf:pdf},
title = {{3D Reconstruction of a Moving Point from a Series of 2D Projections}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.168.5777}
}
@article{Mikolajczyk2005a,
abstract = {In this paper, we compare the performance of descriptors computed for local interest regions, as, for example, extracted by the Harris-Affine detector. Many different descriptors have been proposed in the literature. It is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context, steerable filters, PCA-SIFT, differential invariants, spin images, SIFT, complex filters, moment invariants, and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT-based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.},
author = {Mikolajczyk, Krystian and Schmid, Cordelia},
doi = {10.1109/TPAMI.2005.188},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolajczyk, Schmid - 2005 - Performance evaluation of local descriptors.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Data Interpretation,Image Enhancement,Image Enhancement: methods,Image Interpretation,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models,Pattern Recognition,Software,Software Validation,Statistical},
month = {oct},
number = {10},
pages = {1615--30},
pmid = {16237996},
title = {{Performance evaluation of local descriptors.}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=1498756},
volume = {27},
year = {2005}
}
@article{Wright2009b,
abstract = {We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination, as well as occlusion and disguise. We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem. Based on a sparse representation computed by l{\{}1{\}}-minimization, we propose a general classification algorithm for (image-based) object recognition. This new framework provides new insights into two crucial issues in face recognition: feature extraction and robustness to occlusion. For feature extraction, we show that if sparsity in the recognition problem is properly harnessed, the choice of features is no longer critical. What is critical, however, is whether the number of features is sufficiently large and whether the sparse representation is correctly computed. Unconventional features such as downsampled images and random projections perform just as well as conventional features such as Eigenfaces and Laplacianfaces, as long as the dimension of the feature space surpasses certain threshold, predicted by the theory of sparse representation. This framework can handle errors due to occlusion and corruption uniformly by exploiting the fact that these errors are often sparse with respect to the standard (pixel) basis. The theory of sparse representation helps predict how much occlusion the recognition algorithm can handle and how to choose the training images to maximize robustness to occlusion. We conduct extensive experiments on publicly available databases to verify the efficacy of the proposed algorithm and corroborate the above claims.},
author = {Wright, John and Yang, Allen Y and Ganesh, Arvind and Sastry, S Shankar and Ma, Yi},
doi = {10.1109/TPAMI.2008.79},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Biometry,Biometry: methods,Cluster Analysis,Computer-Assisted,Computer-Assisted: methods,Face,Face: anatomy {\&} histology,Humans,Image Enhancement,Image Enhancement: methods,Image Interpretation,Pattern Recognition,Reproducibility of Results,Sensitivity and Specificity,Subtraction Technique,currently reading,face recognition,sparse representations},
mendeley-tags = {currently reading,face recognition,sparse representations},
month = {mar},
number = {2},
pages = {210--27},
pmid = {19110489},
title = {{Robust face recognition via sparse representation.}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=4483511},
volume = {31},
year = {2009}
}
@misc{Blumensath2007,
abstract = {”This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible.”},
author = {Blumensath, Thomas and Davies, Mike E.},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blumensath, Davies - 2007 - Gradient Pursuits.pdf:pdf},
keywords = {sparse representations},
mendeley-tags = {sparse representations},
title = {{Gradient Pursuits}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.6283},
year = {2007}
}
@misc{Hinterstoisser2008,
abstract = {We show that the simultaneous estimation of keypoint identities and poses is more reliable than the two separate steps undertaken by previous approaches. A simple linear classifier coupled with linear predictors trained during a learning phase appears to be sufficient for this task. The retrieved poses are subpixel accurate due to the linear predictors. We demonstrate the advantages of our approach on real-time 3D object detection and tracking applications. Thanks to the high accuracy, one single keypoint is often enough to precisely estimate the object pose. As a result, we can deal in real-time with objects that are significantly less textured than the ones required by state-of-the-art methods. 1},
author = {Hinterstoisser, Stefan and Benhimane, Selim and Lepetit, Vincent and Fua, Pascal and Navab, Nassir},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hinterstoisser et al. - 2008 - Simultaneous recognition and homography extraction of local patches with a simple linear classifier.pdf:pdf},
publisher = {In BMVC},
title = {{Simultaneous recognition and homography extraction of local patches with a simple linear classifier}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.3234},
year = {2008}
}
@misc{Ozuysal2007,
abstract = {While feature point recognition is a key component of modern approaches to object detection, existing approaches require computationally expensive patch preprocessing to handle perspective distortion. In this paper, we show that formulating the problem in a Naive Bayesian classification framework makes such preprocessing unnecessary and produces an algorithm that is simple, efficient, and robust. Furthermore, it scales well to handle large number of classes. To recognize the patches surrounding keypoints, our classifier uses hundreds of simple binary features and models class posterior probabilities. We make the problem computationally tractable by assuming independence between arbitrary sets of features. Even though this is not strictly true, we demonstrate that our classifier nevertheless performs remarkably well on image datasets containing very significant perspective changes. 1.},
author = {{\"{O}}zuysal, Mustafa and Fua, Pascal and Lepetit, Vincent},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/{\"{O}}zuysal, Fua, Lepetit - 2007 - Fast keypoint recognition in ten lines of code.pdf:pdf},
publisher = {In Proc. IEEE Conference on Computing Vision and Pattern Recognition},
title = {{Fast keypoint recognition in ten lines of code}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.92.6299},
year = {2007}
}
@misc{Adler1996,
abstract = {In this paper, we describe and evaluate three forward sequential basis selection methods: Basic Matching Pursuit (BMP), Order Recursive Matching Pursuit (ORMP) and Modified Matching Pursuit (MMP), and a parallel basis selection method: the FOCal Underdetermined System Solver (FOCUSS) algorithm. Computer simulations show that the ORMP method is superior to the BMP method in terms of its ability to select a compact basis set. However, it is computationally more complex. The MMP algorithm is developed which is of intermediate computational complexity and has performance comparable to the ORMP method. All the sequential selection methods are shown to have difficulty in environments where the basis set contains highly correlated vectors. The drawback can be traced to the sequential nature of these methods suggesting the need for a parallel basis selection method like FOCUSS. Simulations demonstrate that the FOCUSS algorithm does indeed perform well in such correlated environments. However,...},
author = {Adler, J. and Rao, B. D. and Kreutz-delgado, K.},
title = {{Comparison of Basis Selection Methods}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.54.7883},
year = {1996}
}
@article{Cotter1999a,
abstract = {The problem of signal representation in terms of basis vectors from a large, over-complete, spanning dictionary has been the focus of much research. Achieving a succinct, or `sparse', representation is known as the problem of best basis representation. Methods are considered which seek to solve this problem by sequentially building up a basis set for the signal. Three distinct algorithm types have appeared in the literature which are here termed basic matching pursuit (BMP), order recursive matching pursuit (ORMP) and modified matching pursuit (MMP). The algorithms are first described and then their computation is closely examined. Modifications are made to each of the procedures which improve their computational efficiency. The complexity of each algorithm is considered in two contexts; one where the dictionary is variable (time-dependent) and the other where the dictionary is fixed (time-independent). Experimental results are presented which demonstrate that the ORMP method is the best procedure in terms of its ability to give the most compact signal representation, followed by MMP and then BMP which gives the poorest results. Finally, weighing the performance of each algorithm, its computational complexity and the type of dictionary available, recommendations are made as to which algorithm should be used for a given problem},
author = {Cotter, S. F. and Rao, B. D. and Kreutz-Delgado, K. and Adler, J.},
doi = {10.1049/ip-vis:19990445},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - forward.pdf.pdf:pdf},
issn = {1350245X},
journal = {IEE Proceedings - Vision, Image, and Signal Processing},
keywords = {sparse representations},
mendeley-tags = {sparse representations},
number = {5},
pages = {235--244},
title = {{Forward sequential algorithms for best basis selection}},
url = {http://ieeexplore.ieee.org/xpl/freeabs{\_}all.jsp?arnumber=826992},
volume = {146},
year = {1999}
}
@misc{TheMendeleySupportTeam2010,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/The Mendeley Support Team - 2010 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2010}
}
@article{Koenderink1987,
abstract = {It is shown that a convolution with certain reasonable receptive field (RF) profiles yields the exact partial derivatives of the retinal illuminance blurred to a specified degree. Arbitrary concatenations of such RF profiles yield again similar ones of higher order and for a greater degree of blurring.},
annote = {local jet},
author = {Koenderink, J. J. and Doorn, A. J.},
doi = {10.1007/BF00318371},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Koenderink, Doorn - 1987 - Representation of local geometry in the visual system.pdf:pdf},
issn = {0340-1200},
journal = {Biological Cybernetics},
keywords = {Computer Science},
month = {mar},
number = {6},
pages = {367--375},
publisher = {Springer Berlin / Heidelberg},
title = {{Representation of local geometry in the visual system}},
url = {http://www.springerlink.com/content/k16pr3u653n64l65/},
volume = {55},
year = {1987}
}
@book{McClave2001,
author = {McClave, James T and Benson, P George and Sincich, Terry},
isbn = {0130186791},
pages = {0--619},
publisher = {Prentice Hall},
title = {{A First Course in Business Statistics}},
url = {http://www.amazon.com/dp/053696730X},
year = {2001}
}
@misc{TheMendeleySupportTeam2010a,
abstract = {A quick introduction to Mendeley. Learn how Mendeley creates your personal digital library, how to organize and annotate documents, how to collaborate and share with colleagues, and how to generate citations and bibliographies.},
address = {London},
author = {{The Mendeley Support Team}},
booktitle = {Mendeley Desktop},
file = {:C$\backslash$:/Users/zepedaj/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/The Mendeley Support Team - 2010 - Getting Started with Mendeley.pdf:pdf},
keywords = {Mendeley,how-to,user manual},
pages = {1--16},
publisher = {Mendeley Ltd.},
title = {{Getting Started with Mendeley}},
url = {http://www.mendeley.com},
year = {2010}
}
