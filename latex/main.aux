\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Krizhevsky2012}
\citation{Simonyan2014}
\citation{Szegedy2014a}
\citation{He2015}
\citation{Krizhevsky2012}
\citation{Szegedy2014a}
\citation{Krizhevsky2012}
\citation{Lin2013}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {paragraph}{Notation:}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Background}{1}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\hskip -1em.\nobreakspace  {}Recent CNN architectures}{1}{subsection.2.1}}
\@writefile{brf}{\backcite{Krizhevsky2012}{{1}{2.1}{table.1}}}
\citation{He2015}
\citation{He2015}
\citation{He2016}
\citation{Zagoruyko2016}
\citation{feng2015learning}
\citation{scardapane2016group}
\citation{Kulkarnia}
\citation{Liu2015}
\citation{Jaderberg2014a}
\citation{Bach}
\@writefile{brf}{\backcite{Krizhevsky2012}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{Simonyan2014}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{Szegedy2014a}{{2}{2.1}{subsection.2.1}}}
\@writefile{brf}{\backcite{He2015}{{2}{2.1}{subsection.2.1}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Recent architectures in terms of depth $D$ (excluding the soft-max layer), top-1 and top-5 error rates ($1$-crop / $10$-crop) testing, and number of learned parameters.\hlc [pink]{JZ: Need to complete and/or prune this table}}}{2}{table.1}}
\newlabel{tbl:hist_archs}{{1}{2}{Recent architectures in terms of depth $D$ (excluding the soft-max layer), top-1 and top-5 error rates ($1$-crop / $10$-crop) testing, and number of learned parameters.\JZ {Need to complete and/or prune this table}}{table.1}{}}
\@writefile{brf}{\backcite{Szegedy2014a}{{2}{2.1}{table.1}}}
\@writefile{brf}{\backcite{Krizhevsky2012}{{2}{2.1}{table.1}}}
\@writefile{brf}{\backcite{Lin2013}{{2}{2.1}{table.1}}}
\@writefile{brf}{\backcite{He2015}{{2}{2.1}{table.1}}}
\@writefile{brf}{\backcite{He2015}{{2}{2.1}{table.1}}}
\@writefile{brf}{\backcite{He2016}{{2}{2.1}{table.1}}}
\@writefile{brf}{\backcite{Zagoruyko2016}{{2}{2.1}{table.1}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\hskip -1em.\nobreakspace  {}Architecture learning methods}{2}{subsection.2.2}}
\@writefile{brf}{\backcite{feng2015learning}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{scardapane2016group}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Kulkarnia}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Liu2015}{{2}{2.2}{subsection.2.2}}}
\@writefile{brf}{\backcite{Jaderberg2014a}{{2}{2.2}{subsection.2.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\hskip -1em.\nobreakspace  {}Structured sparsity}{2}{subsection.2.3}}
\citation{Jenatton2010a}
\citation{Jenatton2010}
\citation{Goodfellow2013}
\citation{Krizhevsky2012}
\citation{Krizhevsky2012}
\citation{?}
\@writefile{brf}{\backcite{Bach}{{3}{2.3}{subsection.2.3}}}
\@writefile{brf}{\backcite{Jenatton2010a}{{3}{2.3}{equation.2.1}}}
\@writefile{brf}{\backcite{Jenatton2010}{{3}{2.3}{equation.2.1}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}CNN Architecture Selection}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}\hskip -1em.\nobreakspace  {}Learning convolutional architectures}{3}{subsection.3.1}}
\@writefile{brf}{\backcite{Goodfellow2013}{{3}{3.1}{equation.3.4}}}
\newlabel{eq:obj}{{7}{3}{\hskip -1em.~Learning convolutional architectures}{equation.3.7}{}}
\@writefile{brf}{\backcite{Krizhevsky2012}{{3}{3.1}{equation.3.8}}}
\newlabel{eq:reg0}{{9}{3}{\hskip -1em.~Learning convolutional architectures}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}\hskip -1em.\nobreakspace  {}Heuristic selection of the architecture}{3}{subsection.3.2}}
\@writefile{brf}{\backcite{Krizhevsky2012,?}{{3}{3.2}{subsection.3.2}}}
\citation{inception}
\citation{resnet}
\citation{Bottou2012}
\citation{Kulkarnia}
\citation{resent}
\citation{resent}
\@writefile{brf}{\backcite{inception}{{4}{3.2}{subsection.3.2}}}
\@writefile{brf}{\backcite{resnet}{{4}{3.2}{subsection.3.2}}}
\newlabel{eq:complexity}{{10}{4}{\hskip -1em.~Heuristic selection of the architecture}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}\hskip -1em.\nobreakspace  {}Structured sparsity regularization}{4}{subsection.3.3}}
\@writefile{toc}{\contentsline {paragraph}{Kernel-number-reducing regularizer.}{4}{section*.2}}
\newlabel{eq:di_l0}{{11}{4}{Kernel-number-reducing regularizer}{equation.3.11}{}}
\newlabel{eq:drlx}{{12}{4}{Kernel-number-reducing regularizer}{equation.3.12}{}}
\newlabel{eq:reg1}{{13}{4}{Kernel-number-reducing regularizer}{equation.3.13}{}}
\newlabel{eq:dmax}{{14}{4}{Kernel-number-reducing regularizer}{equation.3.14}{}}
\citation{CB2}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of base architectures used in this work. As per \cite  {resent}, ResNet variants of these base architectures use shortcut connections across pairs of layers starting with the second layer. The depth $D$ indicated includes the fully connected layer for the softmax, which is not regularized. \hlc [pink]{JZ:  CS: please add all missing base archs. Need to center m column, $>\{\delimiter "026E30F $centering$\}$}}}{5}{table.2}}
\@writefile{brf}{\backcite{resent}{{5}{2}{table.2}}}
\newlabel{tbl:base_archs}{{2}{5}{Summary of base architectures used in this work. As per \cite {resent}, ResNet variants of these base architectures use shortcut connections across pairs of layers starting with the second layer. The depth $D$ indicated includes the fully connected layer for the softmax, which is not regularized. \JZ { CS: please add all missing base archs. Need to center m column, $>\{\backslash $centering$\}$}}{table.2}{}}
\@writefile{brf}{\backcite{Bottou2012,Kulkarnia}{{5}{3.3}{equation.3.15}}}
\newlabel{eq:dmaxi}{{16}{5}{Kernel-number-reducing regularizer}{equation.3.16}{}}
\@writefile{brf}{\backcite{CB2}{{5}{3.3}{figure.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Illustration of the $d_i$-regularized family of architectures obtained for $\tau =10^{-3}$ by varying $\alpha $ (value indicated in the legend) when using the seven-layer base architecture in Table \ref  {tbl:base_archs} for the constraints $\mathaccentV {hat}05E{d}_i$ (denoted as the dashed envelope). The learned width $d_i$ of each layer is denoted by the vertical length of the area plots. \hlc [pink]{JZ: Need to stretch curve to occupy all the columns... Can add o markers to indicate active constraints. Need to specify all parameters. Why is this shifted right??}\hlc [cyan]{PP: I don't understand the vertical symmetry}.}}{5}{figure.1}}
\newlabel{fig:reg1_family}{{1}{5}{Illustration of the $d_i$-regularized family of architectures obtained for $\tau =10^{-3}$ by varying $\alpha $ (value indicated in the legend) when using the seven-layer base architecture in \tblref {tbl:base_archs} for the constraints $\dmax _i$ (denoted as the dashed envelope). The learned width $d_i$ of each layer is denoted by the vertical length of the area plots. \JZ {Need to stretch curve to occupy all the columns... Can add o markers to indicate active constraints. Need to specify all parameters. Why is this shifted right??}\PP {I don't understand the vertical symmetry}}{figure.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Kernel-size-reducing regularizer.}{5}{section*.3}}
\citation{Bach}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Examples of supports (positions of colored entries) used for groups $\ensuremath  {\mathcal  {G}}_n$ for the case $\mathaccentV {hat}05E{s}=5$ using \textit  {(left)} a \emph  {nested half-ring} strategy with $\mathaccentV {hat}05E{s}$ groups resulting in $s_{ij} \in \{0,1,2,\ldots  ,\mathaccentV {hat}05E{s}_i\}$; and \textit  {(right)} a \emph  {nested ring} strategy with $\delimiter "4264306 \frac  {\mathaccentV {hat}05E{s}}{2} \delimiter "5265307 $ groups resulting in $s_{ij} \in \{0,1,3, \ldots  , \mathaccentV {hat}05E{s}_i\}$ that are zero or odd. For a given kernel $\ensuremath  {\bm  {k}}_{ij}$ with entries $k_{pqr}$, $1 \leqslant p,q \leqslant \mathaccentV {hat}05E{s}_i$, $1 \leqslant r \leqslant \mathaccentV {hat}05E{d}_{i-1}$ , each group $\ensuremath  {\mathcal  {G}}_n$ contains all triplets $(p,q,r)$ corresponding to the shaded area. Note that the shaded supports only depend on the spatial indices $p$ and $q$ -- for a given group, the same support is used for all of the kernel's channels.}}{6}{figure.2}}
\newlabel{fig:groups}{{2}{6}{Examples of supports (positions of colored entries) used for groups $\ms G_n$ for the case $\smax =5$ using \textit {(left)} a \emph {nested half-ring} strategy with $\smax $ groups resulting in $s_{ij} \in \{0,1,2,\ldots ,\smax _i\}$; and \textit {(right)} a \emph {nested ring} strategy with $\lceil \frac {\smax }{2} \rceil $ groups resulting in $s_{ij} \in \{0,1,3, \ldots , \smax _i\}$ that are zero or odd. For a given kernel $\mv k_{ij}$ with entries $k_{pqr}$, $1 \leq p,q \leq \smax _i$, $1 \leq r \leq \dmax _{i-1}$ , each group $\ms G_n$ contains all triplets $(p,q,r)$ corresponding to the shaded area. Note that the shaded supports only depend on the spatial indices $p$ and $q$ -- for a given group, the same support is used for all of the kernel's channels}{figure.2}{}}
\newlabel{eq:srlx}{{21}{6}{Kernel-size-reducing regularizer}{equation.3.21}{}}
\newlabel{eq:reg2}{{22}{6}{Kernel-size-reducing regularizer}{equation.3.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Illustration of the family of $s_{ij}$-regularized architectures obtained for $\tau =??$ by varying $\alpha $ (value indicated in the legend) when using the seven layer base architecture in Table \ref  {tbl:base_archs} for the constraints $\mathaccentV {hat}05E{s}_{ij}$ (denoted as the dashed envelopes). The number of kernels with a fixed size $s_{ij}$ (one fixed $s_{ij}$ per plot) is denoted by the vertical vertical length of the area plots. \hlc [pink]{JZ: Need to stretch curve to occupy all the column... Can add o markers to indicate active constraints. Need to specify all parameters. Add legend. Add correct/more data in file \texttt  {data/hist\_szs\_vs\_layer\_vs\_alpha.dat}, modify for loop in picture code. Can replace the $s_{ij}=0$ plot by a $d_i$ snake plot. Add $\mathaccentV {hat}05E{s}_{ij}$ dashed envelopes}}}{6}{figure.3}}
\newlabel{fig:reg2_family}{{3}{6}{Illustration of the family of $s_{ij}$-regularized architectures obtained for $\tau =??$ by varying $\alpha $ (value indicated in the legend) when using the seven layer base architecture in \tblref {tbl:base_archs} for the constraints $\smax _{ij}$ (denoted as the dashed envelopes). The number of kernels with a fixed size $s_{ij}$ (one fixed $s_{ij}$ per plot) is denoted by the vertical vertical length of the area plots. \JZ {Need to stretch curve to occupy all the column... Can add o markers to indicate active constraints. Need to specify all parameters. Add legend. Add correct/more data in file \texttt {data/hist\_szs\_vs\_layer\_vs\_alpha.dat}, modify for loop in picture code. Can replace the $s_{ij}=0$ plot by a $d_i$ snake plot. Add $\smax _{ij}$ dashed envelopes}}{figure.3}{}}
\@writefile{brf}{\backcite{Bach}{{6}{3.3}{equation.3.24}}}
\citation{eq:assumption}
\citation{He2015}
\citation{Springenberg2015}
\citation{Romero2014}
\citation{Romero2014}
\citation{Romero2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}\hskip -1em.\nobreakspace  {}Variants}{7}{subsection.3.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.4.1}Alternative norms}{7}{subsubsection.3.4.1}}
\@writefile{toc}{\contentsline {paragraph}{Group-normalized $\ell _2$ norm.}{7}{section*.4}}
\@writefile{brf}{\backcite{eq:assumption}{{7}{26}{equation.3.26}}}
\@writefile{toc}{\contentsline {paragraph}{Group-scaled $\ell _2$ norm.}{7}{section*.5}}
\@writefile{brf}{\backcite{}{{7}{3.4.1}{section*.5}}}
\@writefile{brf}{\backcite{}{{7}{3.4.1}{section*.5}}}
\@writefile{brf}{\backcite{}{{7}{3.4.1}{equation.3.29}}}
\@writefile{toc}{\contentsline {paragraph}{Other approaches.}{7}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{Batch-normalization variant.}{7}{section*.7}}
\@writefile{brf}{\backcite{}{{7}{3.4.1}{section*.7}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}\hskip -1em.\nobreakspace  {}Complexity regularizers.}{7}{subsection.3.5}}
\newlabel{eq:reg1 sq}{{32}{7}{\hskip -1em.~Complexity regularizers}{equation.3.32}{}}
\newlabel{eq:reg2 sq}{{33}{7}{\hskip -1em.~Complexity regularizers}{equation.3.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Results}{7}{section.4}}
\bibstyle{ieee}
\bibdata{egbib,jz}
\bibcite{Bach}{1}
\bibcite{Bottou2012}{2}
\bibcite{Goodfellow2013}{3}
\bibcite{He2015}{4}
\bibcite{He2016}{5}
\bibcite{Jenatton2010}{6}
\bibcite{Jenatton2010a}{7}
\bibcite{Krizhevsky2012}{8}
\bibcite{Kulkarnia}{9}
\bibcite{Lin2013}{10}
\bibcite{Liu2015}{11}
\bibcite{Romero2014}{12}
\bibcite{Simonyan2014}{13}
\bibcite{Springenberg2015}{14}
\@writefile{brf}{\backcite{He2015}{{8}{4}{section.4}}}
\@writefile{brf}{\backcite{Springenberg2015}{{8}{4}{section.4}}}
\@writefile{brf}{\backcite{Romero2014}{{8}{4}{section.4}}}
\@writefile{brf}{\backcite{Romero2014}{{8}{4}{section.4}}}
\@writefile{brf}{\backcite{Romero2014}{{8}{4}{section.4}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  \hlc [pink]{JZ: Better order for the legend values. Add more $\tau $ values for $\alpha =10^{-3.8}, \ldots  , 10^{-4.0}$}}}{8}{figure.4}}
\newlabel{fig:cifar10_rsdl}{{4}{8}{\JZ {Better order for the legend values. Add more $\tau $ values for $\alpha =10^{-3.8}, \ldots , 10^{-4.0}$}}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Kernel num. regularization, base arch. \hlc [pink]{JZ: Description, more $\tau $ values...}}}{8}{figure.5}}
\newlabel{fig:cifar10_rsdl}{{5}{8}{Kernel num. regularization, base arch. \JZ {Description, more $\tau $ values...}}{figure.5}{}}
\bibcite{Szegedy2014a}{15}
\bibcite{Zagoruyko2016}{16}
